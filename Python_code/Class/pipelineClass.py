#This class houses the methods which are relevant 
#For fiting gaussians to a galactic spectra, 
#fitting template spectra to an observed spectrum 
#and estimating the galactic physical properties 

#import the relevant modules
import os, sys, numpy as np, random, matplotlib.mlab as mlab, matplotlib.pyplot as plt, math, matplotlib.cm as cm
from matplotlib.backends.backend_pdf import PdfPages
from pylab import *
from matplotlib.colors import LogNorm
import numpy.polynomial.polynomial as poly
import lmfit
from lmfit.models import GaussianModel, ExponentialModel, LorentzianModel, VoigtModel, PolynomialModel

#add my toolkit.py file to the PYTHONPATH
sys.path.append('/Users/owenturner/Documents/PhD/SUPA_Courses/AdvancedDataAnalysis/Homeworks')

#and import the toolkit
import toolkit
from astropy.io import fits

#Import pyraf - python iraf 
import pyraf

####################################################################

class pipelineOps(object):
	#Initialiser creates an instance of the spectrumFit object 
	def __init__(self):
		self.self = self

############################################################################################
#Current functions in this file: 
#computeOffset - from a sky file, object file, bad pixel map and full lcal file 
#                compute the column correction for the 2048x64 pixel chunks 
#
#computeOffsetTwo - Same as above, but uses the output of the stackLcal function 
#                   which are a more conservative set of stacked lcal maps for each detector
#
#computeOffsetTopFour - Use the top four pixels on the raw subtracted frame to compute the offset
#
#computeOffsetSegments  - Same as top method but splitting the resultant into more chunks
#
#subFrames       - Take one fits image with three data extensions and subtract another 
#
#pixelHistogram  - select a chunk of pixels and compute and draw a histogram of their values
#
#stackLcal       - stack the lcal frames from the six different rotation angles. Makes sure 
#                   that the median pixel value will be more reliable 
#
#applyCorrections - takes a sorted list of fits files (piped from dfits) and applies the correction
#	                from the computeOffsetSegments method		
############################################################################################

######################################################################################
#MODULE: computeOffset
#
#PURPOSE:
#Take the object image after calibration, along with a sky frame, bad pixel frame and 
#the lcal frame and homogenise the readout columns, so that after subtraction gives 0
#
#INPUTS:
#
#			objectFile: The object image to be corrected 	
#			skyFile: The sky image taken directly before or after the object frame
#			badPMap: The bad pixel frame generated by the pipeline
#			lcalMap: The calibration frame generated by the pipeline
#
#OUTPUTS: 	newObjData: The corrected 2048x2048 object arrray which can then be saved
#						
#
#USAGE: 	newObjData = computeOffset(objectFile, skyFile, badPMap, lcalMap)
#######################################################################################	

	
	#Access the primary extension, later this will be looped  
	def computeOffset(self, objectFile, skyFile, badPMap, lcalMap): 
		
		#Set up vector to house the corrected extensions
		correctedExtensions=[]
		#Read in the tables of data
		table_o = fits.open(objectFile)
		fitsHeader = table_o[0].header
		header_one = table_o[1].header
		header_two = table_o[2].header 
		header_three = table_o[3].header

		print fitsHeader
		print header_one
		print header_two
		print header_three

		table_s = fits.open(skyFile)
		bad_pixel_table = fits.open(badPMap)

		#Now choose the correct rotation angle 
		lcal_table = fits.open(lcalMap)
		#This is a list of all possible rotation angles 
		angleList = np.array([0, 60, 120, 180, 240, 300])
		#Select the ocs.rot.naangle keyword 
		obsAngle = table_o[0].header["HIERARCH ESO OCS ROT NAANGLE"]
		#Find where the difference between the observed and idealised angle is minimum
		newAngleList = abs( abs(obsAngle) - angleList )
		n = newAngleList.argmin()
		obsAngleNew = angleList[n]


		#Find the extension to which this corresponds
		val = 0
		if obsAngleNew == 0:
			val = 1 
		elif obsAngleNew == 60:
			val = 4
		elif obsAngleNew == 120:
			val = 7
		elif obsAngleNew == 180: 
			val = 10
		elif obsAngleNew == 240:
			val = 13
		elif obsAngleNew == 300:
			val = 16		
		print val	


		#Loop over the fits image extensions, do the same each time
		for count in range(1,4):
			print val
			data_o = table_o[count].data
			data_s = table_s[count].data

			#Create copies of the data arrays so that I can mask the bad pixels 
			#and lcal pixels outside of the loop, instead of wasting time inside 
			manObjData = copy(data_o)
			manSkyData = copy(data_s)

			#Read in the bad pixel and lcal maps
			bad_pixel_data = bad_pixel_table[count].data			
			lcal_data = lcal_table[val].data

			#Find the coordinates of the bad pixels and the slitlets 
			bad_pixel_coords = np.where(bad_pixel_data == 0)
			lcal_pixel_coords = np.where(lcal_data > 0)

			#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
			for i in range(len(bad_pixel_coords[0])):
				#Because of the way np.where works, need to define the x and y coords in this way
				xcoord = bad_pixel_coords[0][i]
				ycoord = bad_pixel_coords[1][i]
				#Now set all positions where there is a dead pixel to np.nan in the object and sky
				manObjData[xcoord][ycoord] = np.nan
				manSkyData[xcoord][ycoord] = np.nan

			#Loop around the slitlet positions
			for i in range(len(lcal_pixel_coords[0])):
				#Do the same, this time for the slitlet positions (substantially more will have a value)
				xcoord = lcal_pixel_coords[0][i]
				ycoord = lcal_pixel_coords[1][i]
				#Set all of these locations to nan 
				manObjData[xcoord][ycoord] = np.nan
				manSkyData[xcoord][ycoord] = np.nan			

			#Debug to see if mask is being applied properly
			#test_array = np.zeros(shape=[2048, 2048])
			#tempName = 'lcal' + str(count) + '.fits'
			#Loop around the bad pixel locations

			#Now we have both the object and sky data these are 2D arrays, 
			#essentially a matrix where each number represents a pixel flux, 
			#And the location of the number in the matrix represents the 
			#Pixel position on the detector, which in turn corresponds to the 
			#objects position on the sky. Need to slice this 2D array into a 
			#1D array of 2D arrays, each of which is 64 pixels wide 
			#so that I can examine these in turn and loop over them 

			#Counters for the slicing 
			x = 0
			y = 64

			#1D arrays to host the data 
			skyArray = []
			objArray = []
			manObjArray = []
			manSkyArray = []
			badPArray = []
			lcalArray = []
			testArray = []

			for i in range(32):
			   
			   #Slice each of the data files into 32 columns of 64 pixels width
			   newObjectArray = data_o[:,x:y]
			   newSkyArray = data_s[:,x:y]
			   newManObjArray = manObjData[:,x:y]
			   newManSkyArray = manSkyData[:,x:y]
			   newPArray = bad_pixel_data[:,x:y]
			   newCalArray = lcal_data[:,x:y]
			   #newTestArray = test_array[:,x:y]
			   #testArray.append(newTestArray)
			   objArray.append(newObjectArray)
			   skyArray.append(newSkyArray)
			   manObjArray.append(newManObjArray)
			   manSkyArray.append(newManSkyArray)
			   badPArray.append(newPArray)
			   lcalArray.append(newCalArray)

			   #Add 64 to the counters each time to create the slices
			   x += 64
			   y += 64

			#Now wanto to read in the lcal and the bad pixel map, 
			#To get a list of the pixel indices which should be averaged
			#First need to slice this in the same way as the other file 
			#Then simply find the bad pixel and the lcal positions, hide 
			#these as nan and compute the median in each 64 pixel column 
			#from everything that isn't nan, then apply the correction to the 
			#data before stitching everything back together as a data file
			#And feeding back into the pipeline at the appropriate location

			#Redefine objArray and sky array, do manipulations to temp
			#objTemp = copy(objArray)
			#skyTemp = copy(skyArray)
			#Start the loop for all the columns in the Array vectors 
			for num in range(len(objArray)):


				#Now all the pixels that shouldn't be included in the median 
				#have value nan. Can then just do np.nanmean(objTemp) which will ignore nan's
				#then repeat the process for the sky, compare the mean's, compute and apply the offset.

				obj_mean = np.nanmedian(manObjArray[num])
				sky_mean = np.nanmedian(manSkyArray[num])
				print sky_mean
				print obj_mean

				#Need to compare the two medians to see how to apply the offset.
				#If the sky is brighter, add the difference to the object image

				if sky_mean > obj_mean:
					objArray[num] += abs(sky_mean - obj_mean)
				elif obj_mean > sky_mean:
					objArray[num] -= abs(obj_mean - sky_mean)	

			#Have now made the correction to all 32 of the 64 pixel width columns.
			#All that is left to do is stitch the objArray arrays back together to 
			#give a single 2048x2048 array.
			newObjData = np.hstack(objArray)
			correctedExtensions.append(newObjData)
			if count == 1:
				print 'Computed First Correction'
			elif count == 2:
				print 'Computed Second Correction'
			elif count == 3: 
				print 'Computed Third Correction'			
			val += 1				
		#Create the object fits file with the three corrected extensions
		fileName = raw_input('Enter a name for the corrected fits file: ') + '.fits'
		#Note that the readout complained about the header not being 
		#in the correct fits format
		hdu = fits.PrimaryHDU(header=fitsHeader)
		hdu.writeto(fileName, clobber=True)
		fits.append(fileName, data=correctedExtensions[0], header=header_one)	
		fits.append(fileName, data=correctedExtensions[1], header=header_two)	
		fits.append(fileName, data=correctedExtensions[2], header=header_three)

	#Second compute offset method, this time with the refined lcal
	#Access the primary extension, later this will be looped  
	def computeOffsetTwo(self, objectFile, skyFile, badPMap, lcal1, lcal2, lcal3): 
		#Set up vector to house the corrected extensions
		correctedExtensions=[]
		#Read in the tables of data
		table_o = fits.open(objectFile)
		fitsHeader = table_o[0].header
		header_one = table_o[1].header
		header_two = table_o[2].header 
		header_three = table_o[3].header

		print fitsHeader
		print header_one
		print header_two
		print header_three

		table_s = fits.open(skyFile)
		bad_pixel_table = fits.open(badPMap)

		#Now read in the refined lcal maps 
		lcal1 = fits.open(lcal1)
		lcal2 = fits.open(lcal2)
		lcal3 = fits.open(lcal3)

		#Save to a dictionary 
		d = {}
		d[1] = lcal1[0].data
		d[2] = lcal2[0].data
		d[3] = lcal3[0].data

		#Loop over the fits image extensions, do the same each time
		for count in range(1,4):
			data_o = table_o[count].data
			data_s = table_s[count].data
			bad_pixel_data = bad_pixel_table[count].data			
			lcal_data = d[count]

			#Loop around the bad pixel locations
			#Now we have both the object and sky data these are 2D arrays, 
			#essentially a matrix where each number represents a pixel flux, 
			#And the location of the number in the matrix represents the 
			#Pixel position on the detector, which in turn corresponds to the 
			#objects position on the sky. Need to slice this 2D array into a 
			#1D array of 2D arrays, each of which is 64 pixels wide 
			#so that I can examine these in turn and loop over them 

			#Counters for the slicing 
			x = 0
			y = 64

			#1D arrays to host the data 
			skyArray = []
			objArray = []
			badPArray = []
			lcalArray = []

			for i in range(32):
			   
			   #Slice each of the data files into 32 columns of 64 pixels width
			   newObjectArray = data_o[:,x:y]
			   newSkyArray = data_s[:,x:y]
			   newPArray = bad_pixel_data[:,x:y]
			   newCalArray = lcal_data[:,x:y]
			   objArray.append(newObjectArray)
			   skyArray.append(newSkyArray)
			   badPArray.append(newPArray)
			   lcalArray.append(newCalArray)

			   #Add 64 to the counters each time to create the slices
			   x += 64
			   y += 64

			#Now wanto to read in the lcal and the bad pixel map, 
			#To get a list of the pixel indices which should be averaged
			#First need to slice this in the same way as the other file 
			#Then simply find the bad pixel and the lcal positions, hide 
			#these as nan and compute the median in each 64 pixel column 
			#from everything that isn't nan, then apply the correction to the 
			#data before stitching everything back together as a data file
			#And feeding back into the pipeline at the appropriate location

			#Redefine objArray and sky array, do manipulations to temp
			objTemp = copy(objArray)
			skyTemp = copy(skyArray)
			#Start the loop for all the columns in the Array vectors 
			for num in range(len(objArray)):


				#Find the coordinates of the bad pixels and the slitlets 
				bad_pixel_coords = np.where(badPArray[num] == 0)
				lcal_pixel_coords = np.where(lcalArray[num] == 0)


				#Loop through the first 2048 x 64 data and sky columns and mask off these coords
				#Then compute the median in both columns. If median sky > median obj, add the difference 
				#between the median values to the obj. If the other way around, decrement

				#Loop around the bad pixel locations
				for i in range(len(bad_pixel_coords[0])):
					#Because of the way np.where works, need to define the x and y coords in this way
					xcoord = bad_pixel_coords[0][i]
					ycoord = bad_pixel_coords[1][i]
					#Now set all positions where there is a dead pixel to np.nan in the object and sky
					objTemp[num][xcoord][ycoord] = np.nan
					skyTemp[num][xcoord][ycoord] = np.nan

				#Loop around the slitlet positions
				for i in range(len(lcal_pixel_coords[0])):
					#Do the same, this time for the slitlet positions (substantially more will have a value)
					xcoord = lcal_pixel_coords[0][i]
					ycoord = lcal_pixel_coords[1][i]
					#Set all of these locations to nan 
					objTemp[num][xcoord][ycoord] = np.nan
					skyTemp[num][xcoord][ycoord] = np.nan


				#Now all the pixels that shouldn't be included in the median 
				#have value nan. Can then just do np.nanmean(objTemp) which will ignore nan's
				#then repeat the process for the sky, compare the mean's, compute and apply the offset.

				obj_mean = np.nanmedian(objTemp[num])
				sky_mean = np.nanmedian(skyTemp[num])
				print sky_mean
				print obj_mean

				#Need to compare the two medians to see how to apply the offset.
				#If the sky is brighter, add the difference to the object image

				if sky_mean > obj_mean:
					objArray[num] += abs(sky_mean - obj_mean)
				elif obj_mean > sky_mean:
					objArray[num] -= abs(obj_mean - sky_mean)	


			#Have now made the correction to all 32 of the 64 pixel width columns.
			#All that is left to do is stitch the objArray arrays back together to 
			#give a single 2048x2048 array.
			newObjData = np.hstack(objArray)
			correctedExtensions.append(newObjData)
			if count == 1:
				print 'Computed First Correction'
			elif count == 2:
				print 'Computed Second Correction'
			elif count == 3: 
				print 'Computed Third Correction'			
		#Create the object fits file with the three corrected extensions
		fileName = raw_input('Enter a name for the corrected fits file: ') + '.fits'
		#Note that the readout complained about the header not being 
		#in the correct fits format
		hdu = fits.PrimaryHDU(header=fitsHeader)
		hdu.writeto(fileName, clobber=True)
		fits.append(fileName, data=correctedExtensions[0], header=header_one)	
		fits.append(fileName, data=correctedExtensions[1], header=header_two)	
		fits.append(fileName, data=correctedExtensions[2], header=header_three)	

	def computeOffsetTopFour(self, rawSubFile, objectFile):

		correctedExtensions = []
		#Read in the tables of data
		table_o = fits.open(objectFile)
		fitsHeader = table_o[0].header
		header_one = table_o[1].header
		header_two = table_o[2].header 
		header_three = table_o[3].header

		print fitsHeader
		print header_one
		print header_two
		print header_three
		table_s = fits.open(rawSubFile)	


		#Loop over the fits image extensions, do the same each time
		for count in range(1,4):
			data_o = table_o[count].data
			data_s = table_s[count].data

			#Counters for the slicing 
			x = 0
			y = 64

			#1D arrays to host the data 
			subArray = []
			objArray = []

			for i in range(32):
			   
			   #Slice each of the data files into 32 columns of 64 pixels width
			   newObjectArray = data_o[:,x:y]
			   newSubArray = data_s[2044:2048,x:y]

			   objArray.append(newObjectArray)
			   subArray.append(newSubArray)

			   #Add 64 to the counters each time to create the slices
			   x += 64
			   y += 64		

			#testData = np.hstack(subArray)
			#fileName = 'testTopFour' + str(count) + '.fits'
			#fits.writeto(fileName, data=testData, clobber=True)   

			for num in range(len(objArray)):
				correctionMedian = np.median(subArray[num])
				objArray[num] -= correctionMedian
				print correctionMedian
				print subArray[num][:,0:1]
			#Have now made the correction to all 32 of the 64 pixel width columns.
			#All that is left to do is stitch the objArray arrays back together to 
			#give a single 2048x2048 array.
			newObjData = np.hstack(objArray)
			correctedExtensions.append(newObjData)
			if count == 1:
				print 'Computed First Correction'
			elif count == 2:
				print 'Computed Second Correction'
			elif count == 3: 
				print 'Computed Third Correction'			
		#Create the object fits file with the three corrected extensions
		fileName = raw_input('Enter a name for the corrected fits file: ') + '.fits'
		#Note that the readout complained about the header not being 
		#in the correct fits format
		hdu = fits.PrimaryHDU(header=fitsHeader)
		hdu.writeto(fileName, clobber=True)
		fits.append(fileName, data=correctedExtensions[0], header=header_one)	
		fits.append(fileName, data=correctedExtensions[1], header=header_two)	
		fits.append(fileName, data=correctedExtensions[2], header=header_three)				
			
	def computeOffsetSegments(self, objectFile, skyFile, badPMap, lcalMap):
		#function should be identical to compute Offset until the initial pixel loop

		#Set up vector to house the corrected extensions
		correctedExtensions=[]

		#Set up vector to house the segments to be vStacked. This is differnet from the 
		#usual compute offset method which just uses each individual readout column

		#Read in the tables of data
		table_o = fits.open(objectFile)
		fitsHeader = table_o[0].header
		header_one = table_o[1].header
		header_two = table_o[2].header 
		header_three = table_o[3].header

		print fitsHeader
		print header_one
		print header_two
		print header_three

		table_s = fits.open(skyFile)
		bad_pixel_table = fits.open(badPMap)

		#Now choose the correct rotation angle 
		lcal_table = fits.open(lcalMap)
		#This is a list of all possible rotation angles 
		angleList = np.array([0, 60, 120, 180, 240, 300])
		#Select the ocs.rot.naangle keyword 
		obsAngle = table_o[0].header["HIERARCH ESO OCS ROT NAANGLE"]
		print obsAngle
		#Find where the difference between the observed and idealised angle is minimum
		newAngleList = abs( abs(obsAngle) - angleList)
		n = newAngleList.argmin()
		obsAngleNew = angleList[n]


		#Find the extension to which this corresponds
		val = 0
		if obsAngleNew == 0:
			val = 1 
		elif obsAngleNew == 60:
			val = 4
		elif obsAngleNew == 120:
			val = 7
		elif obsAngleNew == 180: 
			val = 10
		elif obsAngleNew == 240:
			val = 13
		elif obsAngleNew == 300:
			val = 16		
		print val	


		#Loop over the fits image extensions, do the same each time
		for count in range(1,4):
			vStackArray = []

			print val
			data_o = table_o[count].data
			data_s = table_s[count].data

			#Create copies of the data arrays so that I can mask the bad pixels 
			#and lcal pixels outside of the loop, instead of wasting time inside 
			manObjData = copy(data_o)
			manSkyData = copy(data_s)

			#Read in the bad pixel and lcal maps
			bad_pixel_data = bad_pixel_table[count].data			
			lcal_data = lcal_table[val].data

			#Find the coordinates of the bad pixels and the slitlets 
			bad_pixel_coords = np.where(bad_pixel_data == 0)
			lcal_pixel_coords = np.where(lcal_data > 0)

			#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
			for i in range(len(bad_pixel_coords[0])):
				#Because of the way np.where works, need to define the x and y coords in this way
				xcoord = bad_pixel_coords[0][i]
				ycoord = bad_pixel_coords[1][i]
				#Now set all positions where there is a dead pixel to np.nan in the object and sky
				manObjData[xcoord][ycoord] = np.nan
				manSkyData[xcoord][ycoord] = np.nan

			#Loop around the slitlet positions
			for i in range(len(lcal_pixel_coords[0])):
				#Do the same, this time for the slitlet positions (substantially more will have a value)
				xcoord = lcal_pixel_coords[0][i]
				ycoord = lcal_pixel_coords[1][i]
				#Set all of these locations to nan 
				manObjData[xcoord][ycoord] = np.nan
				manSkyData[xcoord][ycoord] = np.nan			

			#fits.writeto('test.fits', data=manObjData, clobber=True)
			#fits.writeto('test1.fits', data=manSkyData, clobber=True)	
			#Debug to see if mask is being applied properly
			#test_array = np.zeros(shape=[2048, 2048])
			#tempName = 'lcal' + str(count) + '.fits'
			#Loop around the bad pixel locations

			#Now we have both the object and sky data these are 2D arrays, 
			#essentially a matrix where each number represents a pixel flux, 
			#And the location of the number in the matrix represents the 
			#Pixel position on the detector, which in turn corresponds to the 
			#objects position on the sky. Need to slice this 2D array into a 
			#1D array of 2D arrays, each of which is 64 pixels wide 
			#so that I can examine these in turn and loop over them 


			#Counters for the horizontal slicing
			hor1 = 0
			hor2 = 128

			for j in range(16):
				print hor1
				print hor2
				#Counters for the slicing vertical slicing
				x = 0
				y = 64
				#1D arrays to host the data 
				skyArray = []
				objArray = []
				manObjArray = []
				manSkyArray = []
				badPArray = []
				lcalArray = []
				testArray = []

				for i in range(32):
				   
				   #Slice each of the data files into 32 columns of 64 pixels width
				   newObjectArray = data_o[hor1:hor2,x:y]
				   newSkyArray = data_s[hor1:hor2,x:y]
				   newManObjArray = manObjData[hor1:hor2,x:y]
				   newManSkyArray = manSkyData[hor1:hor2,x:y]
				   newPArray = bad_pixel_data[hor1:hor2,x:y]
				   newCalArray = lcal_data[hor1:hor2,x:y]
				   #newTestArray = test_array[:,x:y]
				   #testArray.append(newTestArray)
				   objArray.append(newObjectArray)
				   skyArray.append(newSkyArray)
				   manObjArray.append(newManObjArray)
				   manSkyArray.append(newManSkyArray)
				   badPArray.append(newPArray)
				   lcalArray.append(newCalArray)

				   #Add 64 to the counters each time to create the slices
				   x += 64
				   y += 64

				   #Have sliced each matrix into 2048x64. All that's left to do is slice 
				   #Each of these into 8 chunks of 256x64, 16 chunks of 128x64 and 32 chunks of
				   #64x64. Check the length of the array each time to see if there are enough pixels 
				   #for computing the median
				print objArray[1].shape   
				#Start the loop for all the columns in the Array vectors 
				for num in range(len(objArray)):


					#Now all the pixels that shouldn't be included in the median 
					#have value nan. Can then just do np.nanmean(objTemp) which will ignore nan's
					#then repeat the process for the sky, compare the mean's, compute and apply the offset.

					obj_mean = np.nanmedian(manObjArray[num])
					sky_mean = np.nanmedian(manSkyArray[num])
					#print sky_mean
					#print obj_mean

					#Need to compare the two medians to see how to apply the offset.
					#If the sky is brighter, add the difference to the object image

					if sky_mean > obj_mean:
						objArray[num] += abs(sky_mean - obj_mean)
					elif obj_mean > sky_mean:
						objArray[num] -= abs(obj_mean - sky_mean)	

				#Have now made the correction to all 32 of the 64 pixel width columns.
				#Previously would stack the data here, but the loop goes back to the beginning
				#So need to save to a different object, and then vstack at the end. 
				vStackArray.append(np.hstack(objArray))		
				hor1 += 128
				hor2 += 128	

			#Now just need to vstack all of these arrays and will have a 2048x2048 corrected array
			newObjData = np.vstack(vStackArray)	
			print newObjData.shape

			correctedExtensions.append(newObjData)
			if count == 1:
				print 'Computed First Correction'
			elif count == 2:
				print 'Computed Second Correction'
			elif count == 3: 
				print 'Computed Third Correction'			
			val += 1				
		#Create the object fits file with the three corrected extensions
		fileName = objectFile[0:-5] + '_Corrected'  + '.fits'
		#Note that the readout complained about the header not being 
		#in the correct fits format
		hdu = fits.PrimaryHDU(header=fitsHeader)
		hdu.writeto(fileName, clobber=True)
		fits.append(fileName, data=correctedExtensions[0], header=header_one)	
		fits.append(fileName, data=correctedExtensions[1], header=header_two)	
		fits.append(fileName, data=correctedExtensions[2], header=header_three)




	def subFrames(self, objectFile, skyFile):	

		#Read in the object and sky files 
		objData = fits.open(objectFile)
		skyData = fits.open(skyFile)

		#Find the header and extensions of the new fits file
		header = objData[0].header
		headerOne = objData[1].header
		headerTwo = objData[2].header
		headerThree = objData[3].header

		print header
		ext1 = objData[1].data - skyData[1].data
		ext2 = objData[2].data - skyData[2].data
		ext3 = objData[3].data - skyData[3].data

		#Write out to a different fits file, with name user specified
		nameOfFile = raw_input('Enter the name of the subtracted file: ')
		nameOfFile = nameOfFile + '.fits'
		hdu = fits.PrimaryHDU(header=header)
		hdu.writeto(nameOfFile, clobber=True)
		fits.append(nameOfFile, data=ext1, header=headerOne)	
		fits.append(nameOfFile, data=ext2, header=headerTwo)	
		fits.append(nameOfFile, data=ext3, header=headerThree)

	def pixelHistogram(self, subFile, subCorFile, x1, x2):

		#Create a histogram of pixel values on the 
		#subtracted frame before and after correction	

		#First read in the files 
		subData = fits.open(subFile)
		subCorData = fits.open(subCorFile)

		#At the moment we'll just consider the first extension for our data
		subData = subData[1].data
		subCorData = subCorData[1].data

		#The input numbers define the left and right edges of the pixel section
		subData = subData[:,x1:x2]
		subCorData = subCorData[:,x1:x2]
		
		#This gives an array of arrays, we just care about the numbers, not spatial info
		#Use ravel() to convert these into lists for the histogram
		subData = subData.ravel()
		subCorData = subCorData.ravel()	
		print len(subData)
		print subData
		print np.median(subData)
		print np.median(subCorData)

		#Create the bins array for both histograms 
		bins = np.arange(-15, 15, 1)

		plt.close('all')
		#Plot the histograms 
		n1, bins1, patches1 = plt.hist(subData, bins=bins, histtype='step',\
		 color='green', linewidth=3, label='Before Correction') 

		n2, bins2, patches2 = plt.hist(subCorData, bins=bins, histtype='step',\
		 color='blue', linewidth=3, alpha=0.5, label='After Correction')

		#Now want to fit the gaussians to the histograms using lmfit gaussian models 
		#gaussian model number 1
		mod1 = GaussianModel()

		#Create a new bins vector for the fit 
		fitBins = np.arange(-14.5, 14.5, 1)
		print len(fitBins)

		#Take an initial guess at what the model parameters are 
		#In this case the gaussian model has three parameters, 
		#Which are amplitude, center and sigma
		pars1 = mod1.guess(n1, x=fitBins)
		#Perform the actual fit 
		out1  = mod1.fit(n1, pars1, x=fitBins)
		#Now want to add this curve to our plot 
		plt.plot(fitBins, out1.best_fit, linewidth=2.0, label='b.c. model', color='green')

		#Repeat for the corrected data
		mod2 = GaussianModel()
		pars2 = mod2.guess(n2, x=fitBins)
		out2  = mod2.fit(n2, pars2, x=fitBins)
		plt.plot(fitBins, out2.best_fit, linewidth=2.0, label='a.c. model', color='blue')
		plt.xlabel('Counts per pixel')
		plt.ylabel('Number per bin')
		plt.title('Subtracted Frame, Improvement after Column Correction')
		plt.legend(loc='upper left', fontsize='small')
		plt.savefig(raw_input('Enter the plot name: '), papertype='a4', orientation='landscape')
		plt.show()

		#Print out the fit reports to look at the centre at S.D. of each model
		print out1.fit_report() 
		print out2.fit_report() 

	#def topFourCorrection()	
	def stackLcal(self, lcalFile):
		#Read in the file 
		lcal = fits.open(lcalFile)
		#Want to have it so that only nan values survive. Can do this 
		#By substituting 'False' values for the numbers and true values for nan
		#So that when multiplied only True * True all the way will survive 

		#Loop round all the extensions and change the np.nan values to True 
		#and the pixels with values to False 
		d = {}
		for i in range(1, 19):
			data = lcal[i].data 

			#Define the coordinates where there is a value 
			value_coords = np.where(data > 0)

			#Define where there is np.nan
			nan_coords = np.where(np.isnan(data))
			temp = np.empty(shape=[2048,2048], dtype=float)
			#loop over the pixel values in data and change to either True or False 
			for j in range(len(value_coords[0])):
				xcoord = value_coords[0][j]
				ycoord = value_coords[1][j]
				temp[xcoord][ycoord] = 0

			for j in range(len(nan_coords[0])):
				xcoord = nan_coords[0][j]
				ycoord = nan_coords[1][j]
				temp[xcoord][ycoord] = 1

			d[i] = temp
			print 'done' + str(i)
		print len(d)
		#Check to see if we're creating the right array, which we are	
		#fits.writeto(filename='test.fits', data=d[1], clobber=True)	

		#Now create the individual lcal frames by stacking the extensions together
		#This gives a more conservative estimate of the pixels we should and should 
		#not be using throughout the data analysis stage 

		lcal1 = d[1] * d[4] * d[7] * d[10] * d[13] * d[16]
		lcal2 = d[2] * d[5] * d[8] * d[11] * d[14] * d[17]
		lcal3 = d[3] * d[6] * d[9] * d[12] * d[15] * d[18]

		fits.writeto(filename='lcal1.fits', data=lcal1, clobber=True)
		fits.writeto(filename='lcal2.fits', data=lcal2, clobber=True)
		fits.writeto(filename='lcal3.fits', data=lcal3, clobber=True)

	def applyCorrection(self, fileList, badPMap, lcalMap):
		#Read in the data from the fileList
		data = np.genfromtxt(fileList, dtype='str')
		#Save the names and types as lists 
		names = data[0:,0]
		types = data[0:,1]
		#Loop round all names and apply the computeOffsetSegments method
		for i in range(1, len(names)):
			if types[i] == 'O':
				objFile = names[i]
				if i == 1:
					skyFile = names[i + 1]
				elif types[i - 1] == 'S':
					skyFile = names[i - 1]
				else:
					skyFile = names[i + 1]
				#Now use the method defined within this class 
				self.computeOffsetSegments(objFile, skyFile, badPMap, lcalMap)
				#Which will loop through all and save the corrected object file 
				#as objectFile_Corrected.fits. These are then fed through the pipeline. 	

	def rowMedian(self, subFile, y1, y2, x1, x2):

		#Create a histogram of pixel values on the 
		#subtracted frame before and after correction	

		#First read in the files 
		subData = fits.open(subFile)

		#At the moment we'll just consider the first extension for our data
		subData = subData[1].data

		#The input numbers define the left and right edges of the pixel section
		subData = subData[y1:y2,x1:x2]

		#What we have now is an array of 500 entries, each of which contains 500 entries.
		#These are the columns of the fits file, the pixel count values
		#Now need to compute the median of each and store in an array and return

		medArray = np.median(subData, axis=0)
		return medArray

		

	def plotMedian(self, rawSubFile, subFile, segmentsSubFile, top4SubFile, y1, y2, x1, x2):
		"""	
		Def: 
		Plots the median values of different subtracted frames 
		together on the same axes. Gives an indication for which 
		column correction method produces the smoothest results

		"""
		#Find the medians of the different subtracted frames and plot 
		rawMed = self.rowMedian(rawSubFile, y1, y2, x1, x2)		
		normMed = self.rowMedian(subFile, y1, y2, x1, x2)
		segmentsMed = self.rowMedian(segmentsSubFile, y1, y2, x1, x2)
		top4Med = self.rowMedian(top4SubFile, y1, y2, x1, x2)

		#print type(rawMed)
		#print len(normMed)
		#print segmentsMed
		#print top4Med

		#Generate the x-axis vector, which is just the number of pixels 
		xVec = range(len(rawMed))
		xVec = np.array(xVec)

		#Plot everything against this xVec in turn 

		plt.plot(xVec, normMed, label='Cor')
		plt.plot(xVec, segmentsMed, label='Segments')
		plt.plot(xVec, top4Med, label='top') 
		plt.plot(xVec, rawMed, label='raw')
		plt.xlabel('Pixel Number')
		plt.ylabel('Median')
		plt.title('Medians for different methods %s' % y1)
		plt.legend(loc='upper left', fontsize='small')
		plt.legend()
		plt.savefig(raw_input('Enter the File name: ') + '.png')
		plt.show()
		plt.close('all')

	def crossCorr(self, ext, objFile, skyFile, y1, y2, x1, x2):
		"""
		Def: 
		Compute the cross-correlation coefficient for a given object 
		and skyfile. Define the pixel range over which to compute the 
		coefficient. 

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		objFile - Input object file to compute correlation 
		skyFile - sky image to compare objFile with
		y1, y2, x1, x2 - the range to compute rho over 		
		"""
		#Trying to compute the similarity between a square grid of pixels 
		#from the object file and from the skyfile. Do this using the correlation coeff. 

		#First read in the full 2048x2048 data arrays from the object and sky files 
		#Will first consider just the first detector and can expand on this later 
		objData = fits.open(objFile)
  		objData = objData[ext].data

  		skyData = fits.open(skyFile)
  		skyData = skyData[ext].data

  		#Now have the arrays stored as vectors - split up into smaller grids to perform this test
  		#and save computational time. i.e. how long would it take to compute the correlation coef
  		#using the whole thing? And would this be meaningful? How do you decide upon which section 
  		#of the array to use for the correlation coefficient? 

  		objData = np.array(objData[y1:y2,x1:x2])
  		skyData = np.array(skyData[y1:y2,x1:x2])

  		#print objData
  		#print skyData

  		#These are both now 2D arrays - (Doesn't necessarily have to be square) let's compute the 
  		#Correlation coefficient 

  		objDataMedian = np.nanmedian(objData)
  		skyDataMedian = np.nanmedian(skyData)	
  		#print objDataMedian
  		#print skyDataMedian

  		firstPart = np.nansum((objData - objDataMedian)**2)
  		secondPart = np.nansum((skyData - skyDataMedian)**2)
  		denom = np.sqrt(firstPart * secondPart)
  		#print denom
  		numer = np.nansum((objData - objDataMedian)*(skyData - skyDataMedian))
  		rho = numer / denom
  		#print rho
  		return rho

	def crossCorrZeroth(self, objFile, skyFile, y1, y2, x1, x2):
		"""
		Def: 
		Compute the cross-correlation coefficient for a given object 
		and skyfile. Define the pixel range over which to compute the 
		coefficient. Zeroth because the zeroth and first extensions 
		only are used.

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		objFile - Input object file to compute correlation 
		skyFile - sky image to compare objFile with
		y1, y2, x1, x2 - the range to compute rho over 		
		"""
		#Trying to compute the similarity between a square grid of pixels 
		#from the object file and from the skyfile. Do this using the correlation coeff. 

		#First read in the full 2048x2048 data arrays from the object and sky files 
		#Will first consider just the first detector and can expand on this later 
		objData = fits.open(objFile)
  		objData = objData[0].data

  		skyData = fits.open(skyFile)
  		skyData = skyData[1].data

  		#Now have the arrays stored as vectors - split up into smaller grids to perform this test
  		#and save computational time. i.e. how long would it take to compute the correlation coef
  		#using the whole thing? And would this be meaningful? How do you decide upon which section 
  		#of the array to use for the correlation coefficient? 

  		objData = np.array(objData[y1:y2,x1:x2])
  		skyData = np.array(skyData[y1:y2,x1:x2])

  		#print objData
  		#print skyData

  		#These are both now 2D arrays - (Doesn't necessarily have to be square) let's compute the 
  		#Correlation coefficient 

  		objDataMedian = np.nanmedian(objData)
  		skyDataMedian = np.nanmedian(skyData)	
  		#print objDataMedian
  		#print skyDataMedian

  		firstPart = np.nansum((objData - objDataMedian)**2)
  		secondPart = np.nansum((skyData - skyDataMedian)**2)
  		denom = np.sqrt(firstPart * secondPart)
  		#print denom
  		numer = np.nansum((objData - objDataMedian)*(skyData - skyDataMedian))
  		rho = numer / denom
  		#print rho
  		return rho

	def crossCorrFirst(self, objFile, skyFile, y1, y2, x1, x2):
		"""
		Def: 
		Compute the cross-correlation coefficient for a given object 
		and skyfile. Define the pixel range over which to compute the 
		coefficient. First because only the first extension is used

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		objFile - Input object file to compute correlation 
		skyFile - sky image to compare objFile with
		y1, y2, x1, x2 - the range to compute rho over 		
		"""
		#Trying to compute the similarity between a square grid of pixels 
		#from the object file and from the skyfile. Do this using the correlation coeff. 

		#First read in the full 2048x2048 data arrays from the object and sky files 
		#Will first consider just the first detector and can expand on this later 
		objData = fits.open(objFile)
  		objData = objData[1].data

  		skyData = fits.open(skyFile)
  		skyData = skyData[1].data

  		#Now have the arrays stored as vectors - split up into smaller grids to perform this test
  		#and save computational time. i.e. how long would it take to compute the correlation coef
  		#using the whole thing? And would this be meaningful? How do you decide upon which section 
  		#of the array to use for the correlation coefficient? 

  		objData = np.array(objData[y1:y2,x1:x2])
  		skyData = np.array(skyData[y1:y2,x1:x2])

  		#print objData
  		#print skyData

  		#These are both now 2D arrays - (Doesn't necessarily have to be square) let's compute the 
  		#Correlation coefficient 

  		objDataMedian = np.nanmedian(objData)
  		skyDataMedian = np.nanmedian(skyData)	
  		#print objDataMedian
  		#print skyDataMedian

  		firstPart = np.nansum((objData - objDataMedian)**2)
  		secondPart = np.nansum((skyData - skyDataMedian)**2)
  		denom = np.sqrt(firstPart * secondPart)
  		#print denom
  		numer = np.nansum((objData - objDataMedian)*(skyData - skyDataMedian))
  		rho = numer / denom
  		#print rho
  		return rho

	def crossCorrOne(self, ext, objFile, skyFile, y1, y2, x1, x2):
		"""
		Def: 
		Compute the cross-correlation coefficient for a given object 
		and skyfile. Define the pixel range over which to compute the 
		coefficient. 

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		objFile - Input object file to compute correlation 
		skyFile - sky image to compare objFile with
		y1, y2, x1, x2 - the range to compute rho over 

		"""
		#Trying to compute the similarity between a square grid of pixels 
		#from the object file and from the skyfile. Do this using the correlation coeff. 

		#First read in the full 2048x2048 data arrays from the object and sky files 
		#Will first consider just the first detector and can expand on this later 
		objData = fits.open(objFile)
  		objData = objData[0].data

  		skyData = fits.open(skyFile)
  		skyData = skyData[ext].data

  		#Now have the arrays stored as vectors - split up into smaller grids to perform this test
  		#and save computational time. i.e. how long would it take to compute the correlation coef
  		#using the whole thing? And would this be meaningful? How do you decide upon which section 
  		#of the array to use for the correlation coefficient? 

  		objData = np.array(objData[y1:y2,x1:x2])
  		skyData = np.array(skyData[y1:y2,x1:x2])

  		#print objData
  		#print skyData

  		#These are both now 2D arrays - (Doesn't necessarily have to be square) let's compute the 
  		#Correlation coefficient 

  		objDataMedian = np.nanmedian(objData)
  		skyDataMedian = np.nanmedian(skyData)	
  		#print objDataMedian
  		#print skyDataMedian

  		firstPart = np.nansum((objData - objDataMedian)**2)
  		secondPart = np.nansum((skyData - skyDataMedian)**2)
  		denom = np.sqrt(firstPart * secondPart)
  		#print denom
  		numer = np.nansum((objData - objDataMedian)*(skyData - skyDataMedian))
  		rho = numer / denom
  		#print rho
  		return rho		

  	def shiftImage(self, ext, infile, skyfile, interp_type, stepsize, xmin, xmax, ymin, ymax):

  		"""
  		Def:
  		Compute the correlation coefficient for a grid of pixel shift values and 
  		decide which one is best (if better than the original) and apply this to 
  		the object image to align with the sky.

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		infile - Input object file to shift 
		skyFile - sky image to compare objFile with
		interp_type - type of interpolation function for the shift. 

			-'nearest': nearest neighbour
			-'linear':bilinear x,y, interpolation
			-'poly3':third order interior polynomial
			-'poly5':fifth order interior polynomial
			-'spline3':third order spline3

		stepsize - value to increment grid by each time, increasing 
		this increases the time taken for the computation 
		xmin, xmax, ymin, ymax - Grid extremes for brute force shift 

  		"""
  		#First compute the correlation coefficient with just infile 
  		rhoArray = []
  		rhoArray.append(self.crossCorr(ext, infile, skyfile, 0, 2048, 0, 2048))
  		print rhoArray

  		#Working. Now create grid of fractional shift values. 
  		xArray = np.arange(xmin, xmax, stepsize)
  		xArray = np.around(xArray, decimals = 4)
  		yArray = np.arange(ymin, ymax, stepsize)
  		yArray = np.around(yArray, decimals = 4)

  		#Loop over all values in the grid, shift the image by this 
  		#amount each time and compute the correlation coefficient
  		successDict = {}
  		for value in xArray:
  			for number in yArray:
  				#Perform the shift 
  				infileName = infile + '[' + str(ext) + ']'
  				pyraf.iraf.imshift(input=infileName, output='temp_shift.fits', \
  					xshift=value, yshift=number, interp_type=interp_type)
  				#re-open the shifted file and compute rho
  				rho = self.crossCorrOne(ext,'temp_shift.fits', skyfile,\
  				 0, 2048, 0, 2048)
  				#If the correlation coefficient improves, append to new array
  				if rho > rhoArray[0]:
  					print 'SUCCESS, made improvement!'
  					entryName = str(value) + 'and' + str(number)
  					entryValue = [value, number, rho]
  					successDict[entryName] = entryValue
  				rhoArray.append(rho)
  				#Clean up by deleting the created temporary fits file
  				os.system('rm temp_shift.fits')
  				#Go back through loop, append next value of rho
  				print 'Finished shift: %s %s, rho = %s ' % (value, number, rho)
  				#sys.stdout.flush()

  		print max(rhoArray)
  		print rhoArray[0]		
  		print successDict


  	def shiftImageFirst(self, ext, infile, skyfile, badpmap, interp_type, stepsize, xmin, xmax, ymin, ymax):

  		"""
  		Def:
  		Compute the correlation coefficient for a grid of pixel shift values and 
  		decide which one is best (if better than the original) and apply this to 
  		the object image to align with the sky. First because we use only the 
  		first extension because of the way the shiftImageSegments function is 
  		defined.

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		infile - Input object file to shift 
		skyFile - sky image to compare objFile with
		interp_type - type of interpolation function for the shift. 

			-'nearest': nearest neighbour
			-'linear':bilinear x,y, interpolation
			-'poly3':third order interior polynomial
			-'poly5':fifth order interior polynomial
			-'spline3':third order spline3

		stepsize - value to increment grid by each time, increasing 
		this increases the time taken for the computation 
		xmin, xmax, ymin, ymax - Grid extremes for brute force shift 

  		"""
  		#We first want to apply the bad pixel map 
  		objData = fits.open(infile)
  		


		#Read in the bad pixel and lcal maps
		bad_pixel_data = bad_pixel_table[count].data			
		lcal_data = lcal_table[val].data

		#Find the coordinates of the bad pixels and the slitlets 
		bad_pixel_coords = np.where(bad_pixel_data == 0)
		lcal_pixel_coords = np.where(lcal_data > 0)

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = bad_pixel_coords[0][i]
			ycoord = bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the object and sky
			manObjData[xcoord][ycoord] = np.nan
			manSkyData[xcoord][ycoord] = np.nan


  		#First compute the correlation coefficient with just infile 
  		rhoArray = []
  		rhoArray.append(self.crossCorrFirst(infile, skyfile, 0, 2048, 0, 2048))
  		print rhoArray

  		#Working. Now create grid of fractional shift values. 
  		xArray = np.arange(xmin, xmax, stepsize)
  		xArray = np.around(xArray, decimals = 4)
  		yArray = np.arange(ymin, ymax, stepsize)
  		yArray = np.around(yArray, decimals = 4)

  		#Before attempting the interpolation, we want to mask the bad pixel values, 
  		#and save to a fresh temporary fits file. 

  		#Loop over all values in the grid, shift the image by this 
  		#amount each time and compute the correlation coefficient
  		successDict = {}
  		for value in xArray:
  			for number in yArray:
  				#Perform the shift 
  				infileName = infile + '[1]'
  				pyraf.iraf.imshift(input=infileName, output='temp_shift.fits', \
  					xshift=value, yshift=number, interp_type=interp_type)
  				#re-open the shifted file and compute rho
  				rho = self.crossCorrZeroth('temp_shift.fits', skyfile,\
  				 0, 2048, 0, 2048)
  				#If the correlation coefficient improves, append to new array
  				if rho > rhoArray[0]:
  					print 'SUCCESS, made improvement!'
  					entryName = str(value) + 'and' + str(number)
  					entryValue = [value, number, rho]
  					successDict[entryName] = entryValue
  				rhoArray.append(rho)
  				#Clean up by deleting the created temporary fits file
  				os.system('rm temp_shift.fits')
  				#Go back through loop, append next value of rho
  				print 'Finished shift: %s %s, rho = %s ' % (value, number, rho)
  				#sys.stdout.flush()

  		print max(rhoArray)
  		print rhoArray[0]		
  		print successDict


  	def rotateImage(self, ext, infile, skyfile, interp_type, minAngle, maxAngle, stepsize):

  		"""
  		Def:
  		Compute the correlation coefficient for a grid of pixel shift values and 
  		decide which one is best (if better than the original) and apply this to 
  		the object image to align with the sky.

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		infile - Input object file to shift 
		skyFile - sky image to compare objFile with
		interp_type - type of interpolation function for the shift. 

			-'nearest': nearest neighbour
			-'linear':bilinear x,y, interpolation
			-'poly3':third order interior polynomial
			-'poly5':fifth order interior polynomial
			-'spline3':third order spline3

		stepsize - value to increment grid by each time, increasing 
		this increases the time taken for the computation 
		xmin, xmax, ymin, ymax - Grid extremes for brute force shift 

  		"""
  		#First compute the correlation coefficient with just infile 
  		rhoArray = []
  		rhoArray.append(self.crossCorr(ext, infile, skyfile, 1000, 1200, 1000, 1200))
  		print rhoArray

  		#Working. Now create grid of fractional shift values. 
  		rotArray = np.arange(minAngle, maxAngle, stepsize)
  		rotArray = np.around(rotArray, decimals = 4)


  		#Loop over all values in the grid, shift the image by this 
  		#amount each time and compute the correlation coefficient
  		successDict = {}

		for number in rotArray:
			#Perform the shift 
			infileName = infile + '[' + str(ext) + ']'
			pyraf.iraf.rotate(input=infileName, output='temp_rot.fits', \
				rotation=number, interpolant=interp_type)
			#re-open the shifted file and compute rho
			rho = self.crossCorrOne(ext,'temp_rot.fits', skyfile,\
			 1000, 1200, 1000, 1200)
			#If the correlation coefficient improves, append to new array
			if rho > rhoArray[0]:
				print 'SUCCESS, made improvement!'
				entryName = str(number)
				entryValue = [number, rho]
				successDict[entryName] = entryValue
			rhoArray.append(rho)
			#Clean up by deleting the created temporary fits file
			os.system('rm temp_rot.fits')
			#Go back through loop, append next value of rho
			print 'Finished shift: %s, rho = %s ' % (number, rho)
			#sys.stdout.flush()

  		print max(rhoArray)
  		print rhoArray[0]		
  		print successDict 		


  	def imSplit(self, ext, infile, vertSegments, horSegments):

  		"""
  		Def: 
  		Take an input image file and split up into a series of squares 
  		Main purpose is for use in the shiftImageSegments functino

  		Input: 
  		infile - file to be divided
  		vertSegments - number of vertical segments (2048 must be divisible by) 
  		horSegments - number of horizontal segments (2048 must be divisible by)
  		ext - extension number

  		Output: 
  		segmentArray - 1D array containing 2D square array segments

  		"""
  		#Read in the data file at the given extension
  		data = fits.open(infile)
  		data = data[ext].data

  		#Initialise the empty array
  		segmentArray = []

		#Counters for the horizontal slicing
		hor1 = 0
		hor2 = (2048 / horSegments)

		for j in range(horSegments):

			#Counters for the vertical slicing
			x = 0
			y = (2048 / vertSegments)

			for i in range(vertSegments):
			   
			   #Slice the data according to user selection
			   segmentArray.append(data[hor1:hor2,x:y])
			   x += (2048 / vertSegments)
			   y += (2048 / vertSegments)

			hor1 += (2048 / horSegments)
			hor2 += (2048 / horSegments)   

		return segmentArray	





  	def shiftImageSegments(self, ext, infile, skyfile, badpmap,\
  	 vertSegments, horSegments, interp_type, stepsize, xmin, xmax, ymin, ymax):

  		"""
  		Def: 
  		Lots of arguments because of using lots of different functions. 
  		This is taking an object and a sky image, splitting them into a specified 
  		number of segments, performing shifts to each of the segments and then 
  		computing the cross-correlation function to see if we can improve the 
  		alignment at all. Should give better results than a global shift 

  		Inputs: 
  		infile - file to be divided
		skyFile - sky image to compare objFile with
  		vertSegments - number of vertical segments (2048 must be divisible by) 
  		horSegments - number of horizontal segments (2048 must be divisible by)
  		ext - extension number
		interp_type - type of interpolation function for the shift. 

			-'nearest': nearest neighbour
			-'linear':bilinear x,y, interpolation
			-'poly3':third order interior polynomial
			-'poly5':fifth order interior polynomial
			-'spline3':third order spline3

		stepsize - value to increment grid by each time, increasing 
		this increases the time taken for the computation 
		xmin, xmax, ymin, ymax - Grid extremes for brute force shift 

		"""

		#Create arrays of the split files using the imSplit function 
		objArray = self.imSplit(ext, infile, vertSegments, horSegments)
		skyArray = self.imSplit(ext, skyfile, vertSegments, horSegments)
		badpArray = self.imSplit(ext, badpmap, vertSegments, horSegments)

		#Find the headers of the primary HDU and chosen extension 
		objTable = fits.open(infile)
		objPrimHeader = objTable[0].header
		objExtHeader = objTable[ext].header
		skyTable = fits.open(skyfile)
		skyPrimHeader = skyTable[0].header
		skyExtHeader = skyTable[ext].header
		badpTable = fits.open(badpmap)
		badpPrimHeader = badpTable[0].header
		badpExtHeader = badpTable[ext].header


		print (objPrimHeader)
		print (objExtHeader)
		print (skyPrimHeader)
		print (skyExtHeader)
		print (badpPrimHeader)
		print (badpExtHeader)

		#Should now have two 1D arrays of 2D arrays of equal size
		for i in range(len(objArray)):


			#Write out to new temporary fits files - annoyingly need to have 
			#the data in fits files to be able to use pyraf functions
			#######OBJECT##########
			objhdu = fits.PrimaryHDU(header=objPrimHeader)
			objhdu.writeto('tempObj.fits', clobber=True)
			fits.append('tempObj.fits', data=objArray[i], header=objExtHeader)

			#######SKY#############
			skyhdu = fits.PrimaryHDU(header=skyPrimHeader)
			skyhdu.writeto('tempSky.fits', clobber=True)
			fits.append('tempSky.fits', data=skyArray[i], header=skyExtHeader)

			#######BADPIXEL#############
			badphdu = fits.PrimaryHDU(header=badpPrimHeader)
			badphdu.writeto('tempbadp.fits', clobber=True)
			fits.append('tempbadp.fits', data=badpArray[i], header=badpExtHeader)

			#NOTES FOR RESUMING - I now have temporary files containing the object 
			#and sky segments to be shifted and cross correlated. The shiftImage function 
			#can be applied to each of these directly within the for loop!! - remember to 
			# a) clean up the fits files after each loop 
			# b) Find a way to look at the cross correlation results for each segment independently
			# probably by returning the cross correlation arrays into a new array  
			# c) find a way to search specifically for shift success and apply to each segment 
			# d) find a way to recombine all segments together after the shift has happened 
			# e) make sure to use the crossCorrFirst function here, otherwise it will break in 
			# certain situations (i.e. when not using extension one	)

			#Now need to apply the shiftImageFirst function, which compares the chosen 
			#extension shifted object and sky files. 

			self.shiftImageFirst(ext, 'tempObj.fits', 'tempSky.fits', 'tempbadp.fits', \
			 interp_type, stepsize, xmin, xmax, ymin, ymax)

			#Clean up the temporary fits files during each part of the loop 
			os.system('rm tempObj.fits')
			os.system('rm tempSky.fits')
			os.system('rm tempbadp.fits')

			#That should work then for each segment in turn. Does work. 

























