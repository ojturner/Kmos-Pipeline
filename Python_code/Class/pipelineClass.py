#This class houses the methods which are relevant 
#For fiting gaussians to a galactic spectra, 
#fitting template spectra to an observed spectrum 
#and estimating the galactic physical properties 

#import the relevant modules
import os, sys, numpy as np, random, matplotlib.mlab as mlab, matplotlib.pyplot as plt, math, matplotlib.cm as cm
from matplotlib.backends.backend_pdf import PdfPages
from pylab import *
from matplotlib.colors import LogNorm
import numpy.polynomial.polynomial as poly
import lmfit
from lmfit.models import GaussianModel, ExponentialModel, LorentzianModel, VoigtModel, PolynomialModel

#add my toolkit.py file to the PYTHONPATH
sys.path.append('/Users/owenturner/Documents/PhD/SUPA_Courses/AdvancedDataAnalysis/Homeworks')

#and import the toolkit
import toolkit
from astropy.io import fits

#Import pyraf - python iraf 
import pyraf

####################################################################

class pipelineOps(object):
	#Initialiser creates an instance of the spectrumFit object 
	def __init__(self):
		self.self = self

############################################################################################
#Current functions in this file: 
#computeOffset - from a sky file, object file, bad pixel map and full lcal file 
#                compute the column correction for the 2048x64 pixel chunks 
#
#computeOffsetTwo - Same as above, but uses the output of the stackLcal function 
#                   which are a more conservative set of stacked lcal maps for each detector
#
#computeOffsetTopFour - Use the top four pixels on the raw subtracted frame to compute the offset
#
#computeOffsetSegments  - Same as top method but splitting the resultant into more chunks
#
#subFrames       - Take one fits image with three data extensions and subtract another 
#
#pixelHistogram  - select a chunk of pixels and compute and draw a histogram of their values
#
#stackLcal       - stack the lcal frames from the six different rotation angles. Makes sure 
#                   that the median pixel value will be more reliable 
#
#applyCorrections - takes a sorted list of fits files (piped from dfits) and applies the correction
#	                from the computeOffsetSegments method		
############################################################################################

######################################################################################
#MODULE: computeOffset
#
#PURPOSE:
#Take the object image after calibration, along with a sky frame, bad pixel frame and 
#the lcal frame and homogenise the readout columns, so that after subtraction gives 0
#
#INPUTS:
#
#			objectFile: The object image to be corrected 	
#			skyFile: The sky image taken directly before or after the object frame
#			badPMap: The bad pixel frame generated by the pipeline
#			lcalMap: The calibration frame generated by the pipeline
#
#OUTPUTS: 	newObjData: The corrected 2048x2048 object arrray which can then be saved
#						
#
#USAGE: 	newObjData = computeOffset(objectFile, skyFile, badPMap, lcalMap)
#######################################################################################	

	
	#Access the primary extension, later this will be looped  
	def computeOffset(self, objectFile, skyFile, badPMap, lcalMap): 
		
		#Set up vector to house the corrected extensions
		correctedExtensions=[]
		#Read in the tables of data
		table_o = fits.open(objectFile)
		fitsHeader = table_o[0].header
		header_one = table_o[1].header
		header_two = table_o[2].header 
		header_three = table_o[3].header

		print fitsHeader
		print header_one
		print header_two
		print header_three

		table_s = fits.open(skyFile)
		bad_pixel_table = fits.open(badPMap)

		#Now choose the correct rotation angle 
		lcal_table = fits.open(lcalMap)
		#This is a list of all possible rotation angles 
		angleList = np.array([0, 60, 120, 180, 240, 300])
		#Select the ocs.rot.naangle keyword 
		obsAngle = table_o[0].header["HIERARCH ESO OCS ROT NAANGLE"]
		#Find where the difference between the observed and idealised angle is minimum
		newAngleList = abs( abs(obsAngle) - angleList )
		n = newAngleList.argmin()
		obsAngleNew = angleList[n]


		#Find the extension to which this corresponds
		val = 0
		if obsAngleNew == 0:
			val = 1 
		elif obsAngleNew == 60:
			val = 4
		elif obsAngleNew == 120:
			val = 7
		elif obsAngleNew == 180: 
			val = 10
		elif obsAngleNew == 240:
			val = 13
		elif obsAngleNew == 300:
			val = 16		
		print val	


		#Loop over the fits image extensions, do the same each time
		for count in range(1,4):
			print val
			data_o = table_o[count].data
			data_s = table_s[count].data

			#Create copies of the data arrays so that I can mask the bad pixels 
			#and lcal pixels outside of the loop, instead of wasting time inside 
			manObjData = copy(data_o)
			manSkyData = copy(data_s)

			#Read in the bad pixel and lcal maps
			bad_pixel_data = bad_pixel_table[count].data			
			lcal_data = lcal_table[val].data

			#Find the coordinates of the bad pixels and the slitlets 
			bad_pixel_coords = np.where(bad_pixel_data == 0)
			lcal_pixel_coords = np.where(lcal_data > 0)

			#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
			for i in range(len(bad_pixel_coords[0])):
				#Because of the way np.where works, need to define the x and y coords in this way
				xcoord = bad_pixel_coords[0][i]
				ycoord = bad_pixel_coords[1][i]
				#Now set all positions where there is a dead pixel to np.nan in the object and sky
				manObjData[xcoord][ycoord] = np.nan
				manSkyData[xcoord][ycoord] = np.nan

			#Loop around the slitlet positions
			for i in range(len(lcal_pixel_coords[0])):
				#Do the same, this time for the slitlet positions (substantially more will have a value)
				xcoord = lcal_pixel_coords[0][i]
				ycoord = lcal_pixel_coords[1][i]
				#Set all of these locations to nan 
				manObjData[xcoord][ycoord] = np.nan
				manSkyData[xcoord][ycoord] = np.nan			

			#Debug to see if mask is being applied properly
			#test_array = np.zeros(shape=[2048, 2048])
			#tempName = 'lcal' + str(count) + '.fits'
			#Loop around the bad pixel locations

			#Now we have both the object and sky data these are 2D arrays, 
			#essentially a matrix where each number represents a pixel flux, 
			#And the location of the number in the matrix represents the 
			#Pixel position on the detector, which in turn corresponds to the 
			#objects position on the sky. Need to slice this 2D array into a 
			#1D array of 2D arrays, each of which is 64 pixels wide 
			#so that I can examine these in turn and loop over them 

			#Counters for the slicing 
			x = 0
			y = 64

			#1D arrays to host the data 
			skyArray = []
			objArray = []
			manObjArray = []
			manSkyArray = []
			badPArray = []
			lcalArray = []
			testArray = []

			for i in range(32):
			   
			   #Slice each of the data files into 32 columns of 64 pixels width
			   newObjectArray = data_o[:,x:y]
			   newSkyArray = data_s[:,x:y]
			   newManObjArray = manObjData[:,x:y]
			   newManSkyArray = manSkyData[:,x:y]
			   newPArray = bad_pixel_data[:,x:y]
			   newCalArray = lcal_data[:,x:y]
			   #newTestArray = test_array[:,x:y]
			   #testArray.append(newTestArray)
			   objArray.append(newObjectArray)
			   skyArray.append(newSkyArray)
			   manObjArray.append(newManObjArray)
			   manSkyArray.append(newManSkyArray)
			   badPArray.append(newPArray)
			   lcalArray.append(newCalArray)

			   #Add 64 to the counters each time to create the slices
			   x += 64
			   y += 64

			#Now wanto to read in the lcal and the bad pixel map, 
			#To get a list of the pixel indices which should be averaged
			#First need to slice this in the same way as the other file 
			#Then simply find the bad pixel and the lcal positions, hide 
			#these as nan and compute the median in each 64 pixel column 
			#from everything that isn't nan, then apply the correction to the 
			#data before stitching everything back together as a data file
			#And feeding back into the pipeline at the appropriate location

			#Redefine objArray and sky array, do manipulations to temp
			#objTemp = copy(objArray)
			#skyTemp = copy(skyArray)
			#Start the loop for all the columns in the Array vectors 
			for num in range(len(objArray)):


				#Now all the pixels that shouldn't be included in the median 
				#have value nan. Can then just do np.nanmean(objTemp) which will ignore nan's
				#then repeat the process for the sky, compare the mean's, compute and apply the offset.

				obj_mean = np.nanmedian(manObjArray[num])
				sky_mean = np.nanmedian(manSkyArray[num])
				print sky_mean
				print obj_mean

				#Need to compare the two medians to see how to apply the offset.
				#If the sky is brighter, add the difference to the object image

				if sky_mean > obj_mean:
					objArray[num] += abs(sky_mean - obj_mean)
				elif obj_mean > sky_mean:
					objArray[num] -= abs(obj_mean - sky_mean)	

			#Have now made the correction to all 32 of the 64 pixel width columns.
			#All that is left to do is stitch the objArray arrays back together to 
			#give a single 2048x2048 array.
			newObjData = np.hstack(objArray)
			correctedExtensions.append(newObjData)
			if count == 1:
				print 'Computed First Correction'
			elif count == 2:
				print 'Computed Second Correction'
			elif count == 3: 
				print 'Computed Third Correction'			
			val += 1				
		#Create the object fits file with the three corrected extensions
		fileName = raw_input('Enter a name for the corrected fits file: ') + '.fits'
		#Note that the readout complained about the header not being 
		#in the correct fits format
		hdu = fits.PrimaryHDU(header=fitsHeader)
		hdu.writeto(fileName, clobber=True)
		fits.append(fileName, data=correctedExtensions[0], header=header_one)	
		fits.append(fileName, data=correctedExtensions[1], header=header_two)	
		fits.append(fileName, data=correctedExtensions[2], header=header_three)

	#Second compute offset method, this time with the refined lcal
	#Access the primary extension, later this will be looped  
	def computeOffsetTwo(self, objectFile, skyFile, badPMap, lcal1, lcal2, lcal3): 
		#Set up vector to house the corrected extensions
		correctedExtensions=[]
		#Read in the tables of data
		table_o = fits.open(objectFile)
		fitsHeader = table_o[0].header
		header_one = table_o[1].header
		header_two = table_o[2].header 
		header_three = table_o[3].header

		print fitsHeader
		print header_one
		print header_two
		print header_three

		table_s = fits.open(skyFile)
		bad_pixel_table = fits.open(badPMap)

		#Now read in the refined lcal maps 
		lcal1 = fits.open(lcal1)
		lcal2 = fits.open(lcal2)
		lcal3 = fits.open(lcal3)

		#Save to a dictionary 
		d = {}
		d[1] = lcal1[0].data
		d[2] = lcal2[0].data
		d[3] = lcal3[0].data

		#Loop over the fits image extensions, do the same each time
		for count in range(1,4):
			data_o = table_o[count].data
			data_s = table_s[count].data
			bad_pixel_data = bad_pixel_table[count].data			
			lcal_data = d[count]

			#Loop around the bad pixel locations
			#Now we have both the object and sky data these are 2D arrays, 
			#essentially a matrix where each number represents a pixel flux, 
			#And the location of the number in the matrix represents the 
			#Pixel position on the detector, which in turn corresponds to the 
			#objects position on the sky. Need to slice this 2D array into a 
			#1D array of 2D arrays, each of which is 64 pixels wide 
			#so that I can examine these in turn and loop over them 

			#Counters for the slicing 
			x = 0
			y = 64

			#1D arrays to host the data 
			skyArray = []
			objArray = []
			badPArray = []
			lcalArray = []

			for i in range(32):
			   
			   #Slice each of the data files into 32 columns of 64 pixels width
			   newObjectArray = data_o[:,x:y]
			   newSkyArray = data_s[:,x:y]
			   newPArray = bad_pixel_data[:,x:y]
			   newCalArray = lcal_data[:,x:y]
			   objArray.append(newObjectArray)
			   skyArray.append(newSkyArray)
			   badPArray.append(newPArray)
			   lcalArray.append(newCalArray)

			   #Add 64 to the counters each time to create the slices
			   x += 64
			   y += 64

			#Now wanto to read in the lcal and the bad pixel map, 
			#To get a list of the pixel indices which should be averaged
			#First need to slice this in the same way as the other file 
			#Then simply find the bad pixel and the lcal positions, hide 
			#these as nan and compute the median in each 64 pixel column 
			#from everything that isn't nan, then apply the correction to the 
			#data before stitching everything back together as a data file
			#And feeding back into the pipeline at the appropriate location

			#Redefine objArray and sky array, do manipulations to temp
			objTemp = copy(objArray)
			skyTemp = copy(skyArray)
			#Start the loop for all the columns in the Array vectors 
			for num in range(len(objArray)):


				#Find the coordinates of the bad pixels and the slitlets 
				bad_pixel_coords = np.where(badPArray[num] == 0)
				lcal_pixel_coords = np.where(lcalArray[num] == 0)


				#Loop through the first 2048 x 64 data and sky columns and mask off these coords
				#Then compute the median in both columns. If median sky > median obj, add the difference 
				#between the median values to the obj. If the other way around, decrement

				#Loop around the bad pixel locations
				for i in range(len(bad_pixel_coords[0])):
					#Because of the way np.where works, need to define the x and y coords in this way
					xcoord = bad_pixel_coords[0][i]
					ycoord = bad_pixel_coords[1][i]
					#Now set all positions where there is a dead pixel to np.nan in the object and sky
					objTemp[num][xcoord][ycoord] = np.nan
					skyTemp[num][xcoord][ycoord] = np.nan

				#Loop around the slitlet positions
				for i in range(len(lcal_pixel_coords[0])):
					#Do the same, this time for the slitlet positions (substantially more will have a value)
					xcoord = lcal_pixel_coords[0][i]
					ycoord = lcal_pixel_coords[1][i]
					#Set all of these locations to nan 
					objTemp[num][xcoord][ycoord] = np.nan
					skyTemp[num][xcoord][ycoord] = np.nan


				#Now all the pixels that shouldn't be included in the median 
				#have value nan. Can then just do np.nanmean(objTemp) which will ignore nan's
				#then repeat the process for the sky, compare the mean's, compute and apply the offset.

				obj_mean = np.nanmedian(objTemp[num])
				sky_mean = np.nanmedian(skyTemp[num])
				print sky_mean
				print obj_mean

				#Need to compare the two medians to see how to apply the offset.
				#If the sky is brighter, add the difference to the object image

				if sky_mean > obj_mean:
					objArray[num] += abs(sky_mean - obj_mean)
				elif obj_mean > sky_mean:
					objArray[num] -= abs(obj_mean - sky_mean)	


			#Have now made the correction to all 32 of the 64 pixel width columns.
			#All that is left to do is stitch the objArray arrays back together to 
			#give a single 2048x2048 array.
			newObjData = np.hstack(objArray)
			correctedExtensions.append(newObjData)
			if count == 1:
				print 'Computed First Correction'
			elif count == 2:
				print 'Computed Second Correction'
			elif count == 3: 
				print 'Computed Third Correction'			
		#Create the object fits file with the three corrected extensions
		fileName = raw_input('Enter a name for the corrected fits file: ') + '.fits'
		#Note that the readout complained about the header not being 
		#in the correct fits format
		hdu = fits.PrimaryHDU(header=fitsHeader)
		hdu.writeto(fileName, clobber=True)
		fits.append(fileName, data=correctedExtensions[0], header=header_one)	
		fits.append(fileName, data=correctedExtensions[1], header=header_two)	
		fits.append(fileName, data=correctedExtensions[2], header=header_three)	

	def computeOffsetTopFour(self, rawSubFile, objectFile):

		correctedExtensions = []
		#Read in the tables of data
		table_o = fits.open(objectFile)
		fitsHeader = table_o[0].header
		header_one = table_o[1].header
		header_two = table_o[2].header 
		header_three = table_o[3].header

		print fitsHeader
		print header_one
		print header_two
		print header_three
		table_s = fits.open(rawSubFile)	


		#Loop over the fits image extensions, do the same each time
		for count in range(1,4):
			data_o = table_o[count].data
			data_s = table_s[count].data

			#Counters for the slicing 
			x = 0
			y = 64

			#1D arrays to host the data 
			subArray = []
			objArray = []

			for i in range(32):
			   
			   #Slice each of the data files into 32 columns of 64 pixels width
			   newObjectArray = data_o[:,x:y]
			   newSubArray = data_s[2044:2048,x:y]

			   objArray.append(newObjectArray)
			   subArray.append(newSubArray)

			   #Add 64 to the counters each time to create the slices
			   x += 64
			   y += 64		

			#testData = np.hstack(subArray)
			#fileName = 'testTopFour' + str(count) + '.fits'
			#fits.writeto(fileName, data=testData, clobber=True)   

			for num in range(len(objArray)):
				correctionMedian = np.median(subArray[num])
				objArray[num] -= correctionMedian
				print correctionMedian
				print subArray[num][:,0:1]
			#Have now made the correction to all 32 of the 64 pixel width columns.
			#All that is left to do is stitch the objArray arrays back together to 
			#give a single 2048x2048 array.
			newObjData = np.hstack(objArray)
			correctedExtensions.append(newObjData)
			if count == 1:
				print 'Computed First Correction'
			elif count == 2:
				print 'Computed Second Correction'
			elif count == 3: 
				print 'Computed Third Correction'			
		#Create the object fits file with the three corrected extensions
		fileName = raw_input('Enter a name for the corrected fits file: ') + '.fits'
		#Note that the readout complained about the header not being 
		#in the correct fits format
		hdu = fits.PrimaryHDU(header=fitsHeader)
		hdu.writeto(fileName, clobber=True)
		fits.append(fileName, data=correctedExtensions[0], header=header_one)	
		fits.append(fileName, data=correctedExtensions[1], header=header_two)	
		fits.append(fileName, data=correctedExtensions[2], header=header_three)				
			
	def computeOffsetSegments(self, objectFile, skyFile, badPMap, lcalMap):
		#function should be identical to compute Offset until the initial pixel loop

		#Set up vector to house the corrected extensions
		correctedExtensions=[]

		#Set up vector to house the segments to be vStacked. This is differnet from the 
		#usual compute offset method which just uses each individual readout column

		#Read in the tables of data
		table_o = fits.open(objectFile)
		fitsHeader = table_o[0].header
		header_one = table_o[1].header
		header_two = table_o[2].header 
		header_three = table_o[3].header

		print fitsHeader
		print header_one
		print header_two
		print header_three

		table_s = fits.open(skyFile)
		bad_pixel_table = fits.open(badPMap)

		#Now choose the correct rotation angle 
		lcal_table = fits.open(lcalMap)
		#This is a list of all possible rotation angles 
		angleList = np.array([0, 60, 120, 180, 240, 300])
		#Select the ocs.rot.naangle keyword 
		obsAngle = table_o[0].header["HIERARCH ESO OCS ROT NAANGLE"]
		print obsAngle
		#Find where the difference between the observed and idealised angle is minimum
		newAngleList = abs( abs(obsAngle) - angleList)
		n = newAngleList.argmin()
		obsAngleNew = angleList[n]


		#Find the extension to which this corresponds
		val = 0
		if obsAngleNew == 0:
			val = 1 
		elif obsAngleNew == 60:
			val = 4
		elif obsAngleNew == 120:
			val = 7
		elif obsAngleNew == 180: 
			val = 10
		elif obsAngleNew == 240:
			val = 13
		elif obsAngleNew == 300:
			val = 16		
		print val	


		#Loop over the fits image extensions, do the same each time
		for count in range(1,4):
			vStackArray = []

			print val
			data_o = table_o[count].data
			data_s = table_s[count].data

			#Create copies of the data arrays so that I can mask the bad pixels 
			#and lcal pixels outside of the loop, instead of wasting time inside 
			manObjData = copy(data_o)
			manSkyData = copy(data_s)

			#Read in the bad pixel and lcal maps
			bad_pixel_data = bad_pixel_table[count].data			
			lcal_data = lcal_table[val].data

			#Find the coordinates of the bad pixels and the slitlets 
			bad_pixel_coords = np.where(bad_pixel_data == 0)
			lcal_pixel_coords = np.where(lcal_data > 0)

			#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
			for i in range(len(bad_pixel_coords[0])):
				#Because of the way np.where works, need to define the x and y coords in this way
				xcoord = bad_pixel_coords[0][i]
				ycoord = bad_pixel_coords[1][i]
				#Now set all positions where there is a dead pixel to np.nan in the object and sky
				manObjData[xcoord][ycoord] = np.nan
				manSkyData[xcoord][ycoord] = np.nan

			#Loop around the slitlet positions
			for i in range(len(lcal_pixel_coords[0])):
				#Do the same, this time for the slitlet positions (substantially more will have a value)
				xcoord = lcal_pixel_coords[0][i]
				ycoord = lcal_pixel_coords[1][i]
				#Set all of these locations to nan 
				manObjData[xcoord][ycoord] = np.nan
				manSkyData[xcoord][ycoord] = np.nan			

			#fits.writeto('test.fits', data=manObjData, clobber=True)
			#fits.writeto('test1.fits', data=manSkyData, clobber=True)	
			#Debug to see if mask is being applied properly
			#test_array = np.zeros(shape=[2048, 2048])
			#tempName = 'lcal' + str(count) + '.fits'
			#Loop around the bad pixel locations

			#Now we have both the object and sky data these are 2D arrays, 
			#essentially a matrix where each number represents a pixel flux, 
			#And the location of the number in the matrix represents the 
			#Pixel position on the detector, which in turn corresponds to the 
			#objects position on the sky. Need to slice this 2D array into a 
			#1D array of 2D arrays, each of which is 64 pixels wide 
			#so that I can examine these in turn and loop over them 


			#Counters for the horizontal slicing
			hor1 = 0
			hor2 = 128

			for j in range(16):
				print hor1
				print hor2
				#Counters for the slicing vertical slicing
				x = 0
				y = 64
				#1D arrays to host the data 
				skyArray = []
				objArray = []
				manObjArray = []
				manSkyArray = []
				badPArray = []
				lcalArray = []
				testArray = []

				for i in range(32):
				   
				   #Slice each of the data files into 32 columns of 64 pixels width
				   newObjectArray = data_o[hor1:hor2,x:y]
				   newSkyArray = data_s[hor1:hor2,x:y]
				   newManObjArray = manObjData[hor1:hor2,x:y]
				   newManSkyArray = manSkyData[hor1:hor2,x:y]
				   newPArray = bad_pixel_data[hor1:hor2,x:y]
				   newCalArray = lcal_data[hor1:hor2,x:y]
				   #newTestArray = test_array[:,x:y]
				   #testArray.append(newTestArray)
				   objArray.append(newObjectArray)
				   skyArray.append(newSkyArray)
				   manObjArray.append(newManObjArray)
				   manSkyArray.append(newManSkyArray)
				   badPArray.append(newPArray)
				   lcalArray.append(newCalArray)

				   #Add 64 to the counters each time to create the slices
				   x += 64
				   y += 64

				   #Have sliced each matrix into 2048x64. All that's left to do is slice 
				   #Each of these into 8 chunks of 256x64, 16 chunks of 128x64 and 32 chunks of
				   #64x64. Check the length of the array each time to see if there are enough pixels 
				   #for computing the median
				print objArray[1].shape   
				#Start the loop for all the columns in the Array vectors 
				for num in range(len(objArray)):


					#Now all the pixels that shouldn't be included in the median 
					#have value nan. Can then just do np.nanmean(objTemp) which will ignore nan's
					#then repeat the process for the sky, compare the mean's, compute and apply the offset.

					obj_mean = np.nanmedian(manObjArray[num])
					sky_mean = np.nanmedian(manSkyArray[num])
					#print sky_mean
					#print obj_mean

					#Need to compare the two medians to see how to apply the offset.
					#If the sky is brighter, add the difference to the object image

					if sky_mean > obj_mean:
						objArray[num] += abs(sky_mean - obj_mean)
					elif obj_mean > sky_mean:
						objArray[num] -= abs(obj_mean - sky_mean)	

				#Have now made the correction to all 32 of the 64 pixel width columns.
				#Previously would stack the data here, but the loop goes back to the beginning
				#So need to save to a different object, and then vstack at the end. 
				vStackArray.append(np.hstack(objArray))		
				hor1 += 128
				hor2 += 128	

			#Now just need to vstack all of these arrays and will have a 2048x2048 corrected array
			newObjData = np.vstack(vStackArray)	
			print newObjData.shape

			correctedExtensions.append(newObjData)
			if count == 1:
				print 'Computed First Correction'
			elif count == 2:
				print 'Computed Second Correction'
			elif count == 3: 
				print 'Computed Third Correction'			
			val += 1				
		#Create the object fits file with the three corrected extensions
		fileName = objectFile[0:-5] + '_Corrected'  + '.fits'
		#Note that the readout complained about the header not being 
		#in the correct fits format
		hdu = fits.PrimaryHDU(header=fitsHeader)
		hdu.writeto(fileName, clobber=True)
		fits.append(fileName, data=correctedExtensions[0], header=header_one)	
		fits.append(fileName, data=correctedExtensions[1], header=header_two)	
		fits.append(fileName, data=correctedExtensions[2], header=header_three)




	def subFrames(self, objectFile, skyFile):	

		#Read in the object and sky files 
		objData = fits.open(objectFile)
		skyData = fits.open(skyFile)

		#Find the header and extensions of the new fits file
		header = objData[0].header
		headerOne = objData[1].header
		headerTwo = objData[2].header
		headerThree = objData[3].header

		print header
		ext1 = objData[1].data - skyData[1].data
		ext2 = objData[2].data - skyData[2].data
		ext3 = objData[3].data - skyData[3].data

		#Write out to a different fits file, with name user specified
		nameOfFile = objectFile[:-5] + '_Subtracted.fits'  
		hdu = fits.PrimaryHDU(header=header)
		hdu.writeto(nameOfFile, clobber=True)
		fits.append(nameOfFile, data=ext1, header=headerOne)	
		fits.append(nameOfFile, data=ext2, header=headerTwo)	
		fits.append(nameOfFile, data=ext3, header=headerThree)

	def pixelHistogram(self, subFile, subCorFile, x1, x2):

		#Create a histogram of pixel values on the 
		#subtracted frame before and after correction	

		#First read in the files 
		subData = fits.open(subFile)
		subCorData = fits.open(subCorFile)

		#At the moment we'll just consider the first extension for our data
		subData = subData[1].data
		subCorData = subCorData[1].data

		#The input numbers define the left and right edges of the pixel section
		subData = subData[:,x1:x2]
		subCorData = subCorData[:,x1:x2]
		
		#This gives an array of arrays, we just care about the numbers, not spatial info
		#Use ravel() to convert these into lists for the histogram
		subData = subData.ravel()
		subCorData = subCorData.ravel()	
		print len(subData)
		print subData
		print np.median(subData)
		print np.median(subCorData)

		#Create the bins array for both histograms 
		bins = np.arange(-15, 15, 1)

		plt.close('all')
		#Plot the histograms 
		n1, bins1, patches1 = plt.hist(subData, bins=bins, histtype='step',\
		 color='green', linewidth=3, label='Before Correction') 

		n2, bins2, patches2 = plt.hist(subCorData, bins=bins, histtype='step',\
		 color='blue', linewidth=3, alpha=0.5, label='After Correction')

		#Now want to fit the gaussians to the histograms using lmfit gaussian models 
		#gaussian model number 1
		mod1 = GaussianModel()

		#Create a new bins vector for the fit 
		fitBins = np.arange(-14.5, 14.5, 1)
		print len(fitBins)

		#Take an initial guess at what the model parameters are 
		#In this case the gaussian model has three parameters, 
		#Which are amplitude, center and sigma
		pars1 = mod1.guess(n1, x=fitBins)
		#Perform the actual fit 
		out1  = mod1.fit(n1, pars1, x=fitBins)
		#Now want to add this curve to our plot 
		plt.plot(fitBins, out1.best_fit, linewidth=2.0, label='b.c. model', color='green')

		#Repeat for the corrected data
		mod2 = GaussianModel()
		pars2 = mod2.guess(n2, x=fitBins)
		out2  = mod2.fit(n2, pars2, x=fitBins)
		plt.plot(fitBins, out2.best_fit, linewidth=2.0, label='a.c. model', color='blue')
		plt.xlabel('Counts per pixel')
		plt.ylabel('Number per bin')
		plt.title('Subtracted Frame, Improvement after Column Correction')
		plt.legend(loc='upper left', fontsize='small')
		plt.savefig(raw_input('Enter the plot name: '), papertype='a4', orientation='landscape')
		plt.show()

		#Print out the fit reports to look at the centre at S.D. of each model
		print out1.fit_report() 
		print out2.fit_report() 

	#def topFourCorrection()	
	def stackLcal(self, lcalFile):
		#Read in the file 
		lcal = fits.open(lcalFile)
		#Want to have it so that only nan values survive. Can do this 
		#By substituting 'False' values for the numbers and true values for nan
		#So that when multiplied only True * True all the way will survive 

		#Loop round all the extensions and change the np.nan values to True 
		#and the pixels with values to False 
		d = {}
		for i in range(1, 19):
			data = lcal[i].data 

			#Define the coordinates where there is a value 
			value_coords = np.where(data > 0)

			#Define where there is np.nan
			nan_coords = np.where(np.isnan(data))
			temp = np.empty(shape=[2048,2048], dtype=float)
			#loop over the pixel values in data and change to either True or False 
			for j in range(len(value_coords[0])):
				xcoord = value_coords[0][j]
				ycoord = value_coords[1][j]
				temp[xcoord][ycoord] = 0

			for j in range(len(nan_coords[0])):
				xcoord = nan_coords[0][j]
				ycoord = nan_coords[1][j]
				temp[xcoord][ycoord] = 1

			d[i] = temp
			print 'done' + str(i)
		print len(d)
		#Check to see if we're creating the right array, which we are	
		#fits.writeto(filename='test.fits', data=d[1], clobber=True)	

		#Now create the individual lcal frames by stacking the extensions together
		#This gives a more conservative estimate of the pixels we should and should 
		#not be using throughout the data analysis stage 

		lcal1 = d[1] * d[4] * d[7] * d[10] * d[13] * d[16]
		lcal2 = d[2] * d[5] * d[8] * d[11] * d[14] * d[17]
		lcal3 = d[3] * d[6] * d[9] * d[12] * d[15] * d[18]

		fits.writeto(filename='lcal1.fits', data=lcal1, clobber=True)
		fits.writeto(filename='lcal2.fits', data=lcal2, clobber=True)
		fits.writeto(filename='lcal3.fits', data=lcal3, clobber=True)

	def applyCorrection(self, fileList, badPMap, lcalMap):
		#Read in the data from the fileList
		data = np.genfromtxt(fileList, dtype='str')
		#Save the names and types as lists 
		names = data[0:,0]
		types = data[0:,1]
		#Loop round all names and apply the computeOffsetSegments method
		for i in range(1, len(names)):
			if types[i] == 'O':
				objFile = names[i]
				if i == 1:
					skyFile = names[i + 1]
				elif types[i - 1] == 'S':
					skyFile = names[i - 1]
				else:
					skyFile = names[i + 1]
				#Now use the method defined within this class 
				self.computeOffsetSegments(objFile, skyFile, badPMap, lcalMap)
				#Which will loop through all and save the corrected object file 
				#as objectFile_Corrected.fits. These are then fed through the pipeline. 	

	def rowMedian(self, subFile, y1, y2, x1, x2):

		#Create a histogram of pixel values on the 
		#subtracted frame before and after correction	

		#First read in the files 
		subData = fits.open(subFile)

		#At the moment we'll just consider the first extension for our data
		subData = subData[1].data

		#The input numbers define the left and right edges of the pixel section
		subData = subData[y1:y2,x1:x2]

		#What we have now is an array of 500 entries, each of which contains 500 entries.
		#These are the columns of the fits file, the pixel count values
		#Now need to compute the median of each and store in an array and return

		medArray = np.median(subData, axis=0)
		return medArray

		

	def plotMedian(self, rawSubFile, subFile, segmentsSubFile, top4SubFile, y1, y2, x1, x2):
		"""	
		Def: 
		Plots the median values of different subtracted frames 
		together on the same axes. Gives an indication for which 
		column correction method produces the smoothest results

		"""
		#Find the medians of the different subtracted frames and plot 
		rawMed = self.rowMedian(rawSubFile, y1, y2, x1, x2)		
		normMed = self.rowMedian(subFile, y1, y2, x1, x2)
		segmentsMed = self.rowMedian(segmentsSubFile, y1, y2, x1, x2)
		top4Med = self.rowMedian(top4SubFile, y1, y2, x1, x2)

		#print type(rawMed)
		#print len(normMed)
		#print segmentsMed
		#print top4Med

		#Generate the x-axis vector, which is just the number of pixels 
		xVec = range(len(rawMed))
		xVec = np.array(xVec)

		#Plot everything against this xVec in turn 

		plt.plot(xVec, normMed, label='Cor')
		plt.plot(xVec, segmentsMed, label='Segments')
		plt.plot(xVec, top4Med, label='top') 
		plt.plot(xVec, rawMed, label='raw')
		plt.xlabel('Pixel Number')
		plt.ylabel('Median')
		plt.title('Medians for different methods %s' % y1)
		plt.legend(loc='upper left', fontsize='small')
		plt.legend()
		plt.savefig(raw_input('Enter the File name: ') + '.png')
		plt.show()
		plt.close('all')

	def badPixelextend(self, badpmap): 
		"""
		Def: 
		Take an arbitary bad pixel mask and mask off the pixels in a ring around 
		the original bad pixel location. Good for determining the pixel shift location
		as by inspection many of the pixels around the bad pixel locations are also 
		pretty rubbish 

		Inputs: 
		badpmap - any bad pixel mask with 0 as bad and 1 as good 

		Outputs: 
		newbadpmap - remasked bad pixel. 

		"""
		#Read in data 
		badpTable = fits.open(badpmap)
		extArray = []

		primHeader = badpTable[0].header
		ext1Header = badpTable[1].header
		ext2Header = badpTable[2].header
		ext3Header = badpTable[3].header

		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')		
		print primHeader
		print ext1Header
		print ext2Header
		print ext3Header
		sys.stdout.close()
		sys.stdout = temp

		#Loop around the extensions and make ammendments  
		for i in range(1,4):
			badpData = badpTable[i].data

			#Now have the 2048x2048 data array. Want to find all the 0 values 
			#And make the points surrounding this zero also 
			badpCoords = np.where(badpData == 0)

			#Loop around the bad pixel locations and mask off 
			for i in range(len(badpCoords[0])):
				#Because of the way np.where works, need to define the x and y coords in this way

				if (badpCoords[0][i] < 2047) and (badpCoords[1][i] < 2047):
					#print badpCoords[0][i], badpCoords[1][i]
					xcoord = badpCoords[0][i]
					ycoord = badpCoords[1][i]
					#Now set all positions where there is a dead pixel to np.nan in the object and sky
					badpData[xcoord][ycoord + 1] = 0
					badpData[xcoord][ycoord - 1] = 0
					badpData[xcoord + 1][ycoord] = 0								
					badpData[xcoord - 1][ycoord] = 0
					badpData[xcoord + 1][ycoord + 1] = 0
					badpData[xcoord - 1][ycoord + 1] = 0
					badpData[xcoord + 1][ycoord - 1] = 0
					badpData[xcoord - 1][ycoord - 1] = 0


			extArray.append(badpData)

		#Now write out to a new fits file 
		badpName = badpmap[:-5] + '_Added.fits'	
		hdu = fits.PrimaryHDU(header=primHeader)
		hdu.writeto(badpName, clobber=True)
		fits.append(badpName, data=extArray[0], header=ext1Header)	
		fits.append(badpName, data=extArray[1], header=ext2Header)	
		fits.append(badpName, data=extArray[2], header=ext3Header)

		os.system('rm log.txt')

	def extensionMedians(self, subbedFile):
		"""
		Def:
		Take the masked subtracted file and compute the median and standard deviation of each extension 
		then print out these values to the terminal

		"""	

		dataTable = fits.open(subbedFile)
		for i in range(1, 4):
			print i
			data = dataTable[i].data
			med = np.nanmedian(data)
			st = np.nanstd(data)
			print 'The median of extension %s is: %s \n The standard Deviation of extension %s is: %s' % (i, med, i, st)



	def maskFile(self, inFile, badpFile):
		"""	
		Def:
		Take the badPixelMap and apply it to any inFile 
		to mask off the bad pixels. Particularly useful 
		for applying to skyFiles before subtraction

		"""	

		dataTable = fits.open(inFile)
		badPTable = fits.open(badpFile)
		extArray = []

		primHeader = dataTable[0].header
		ext1Header = dataTable[1].header
		ext2Header = dataTable[2].header
		ext3Header = dataTable[3].header


		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')
		print primHeader
		print ext1Header
		print ext2Header
		print ext3Header
		sys.stdout.close()
		sys.stdout = temp

		for i in range(1, 4):

			data = dataTable[i].data
			badpData = badPTable[i].data

			#Now find the bad pixel locations and mask off the data appropriately

			bad_pixel_coords = np.where(badpData == 0)

			#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
			for i in range(len(bad_pixel_coords[0])):
				#Because of the way np.where works, need to define the x and y coords in this way
				xcoord = bad_pixel_coords[0][i]
				ycoord = bad_pixel_coords[1][i]
				#Now set all positions where there is a dead pixel to np.nan in the object and sky
				data[xcoord][ycoord] = np.nan

			extArray.append(data)
		#Write out the new data
		fileName = inFile[:-5] + '_masked.fits'	
		hdu = fits.PrimaryHDU(header=primHeader)
		hdu.writeto(fileName, clobber=True)
		fits.append(fileName, data=extArray[0], header=ext1Header)	
		fits.append(fileName, data=extArray[1], header=ext2Header)	
		fits.append(fileName, data=extArray[2], header=ext3Header)	
		os.system('rm log.txt')


								
	def crossCorr(self, ext, objFile, skyFile, y1, y2, x1, x2):
		"""
		Def: 
		Compute the cross-correlation coefficient for a given object 
		and skyfile. Define the pixel range over which to compute the 
		coefficient. 

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		objFile - Input object file to compute correlation 
		skyFile - sky image to compare objFile with
		y1, y2, x1, x2 - the range to compute rho over 		
		"""
		#Trying to compute the similarity between a square grid of pixels 
		#from the object file and from the skyfile. Do this using the correlation coeff. 

		#First read in the full 2048x2048 data arrays from the object and sky files 
		#Will first consider just the first detector and can expand on this later 
		objData = fits.open(objFile)
  		objData = objData[ext].data

  		skyData = fits.open(skyFile)
  		skyData = skyData[ext].data

  		#Now have the arrays stored as vectors - split up into smaller grids to perform this test
  		#and save computational time. i.e. how long would it take to compute the correlation coef
  		#using the whole thing? And would this be meaningful? How do you decide upon which section 
  		#of the array to use for the correlation coefficient? 

  		objData = np.array(objData[y1:y2,x1:x2])
  		skyData = np.array(skyData[y1:y2,x1:x2])

  		#print objData
  		#print skyData

  		#These are both now 2D arrays - (Doesn't necessarily have to be square) let's compute the 
  		#Correlation coefficient 

  		#The imshift method converts nan pixels to 0. Investigating the impact of this right now. 
  		#Maybe there are implications for the imshift actually being performed properly. 
  		#If we can mask off all the pixels marked as 0, marked as np.nan and lying a certain 
  		#number of standard deviations from the median, this should be a robust way of computing 
  		#the correlation coefficient and should lead to an exact match between the 0 shift 
  		#value and the value found before shifting.

  		###############FIRST MASK OFF THE PIXELS WITH 0 VALUE FROM BOTH OBJECT AND SKY############

		obj_bad_pixel_coords = np.where(objData == 0)
		sky_bad_pixel_coords = np.where(skyData == 0)

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(obj_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = obj_bad_pixel_coords[0][i]
			ycoord = obj_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the object and sky
			objData[xcoord][ycoord] = np.nan

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(sky_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = sky_bad_pixel_coords[0][i]
			ycoord = sky_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the skyect and sky
			skyData[xcoord][ycoord] = np.nan


  		objDataMedian = np.nanmedian(objData)
  		skyDataMedian = np.nanmedian(skyData)	
		skyDataStd = np.nanstd(skyData)
  		objDataStd = np.nanstd(objData)

  		###############MASKING THE HIGH SIGMA PIXELS FOR BETTER RHO##########################
  		#Let's try masking the pixels which are bigger than a standard deviation from the median
		#Find the coordinates of the bad pixels and the slitlets 
		obj_bad_pixel_coords = np.where(abs(objData) >  (objDataMedian + objDataStd))
		sky_bad_pixel_coords = np.where(abs(skyData) >  (skyDataMedian + skyDataStd))

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(obj_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = obj_bad_pixel_coords[0][i]
			ycoord = obj_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the object and sky
			objData[xcoord][ycoord] = np.nan

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(sky_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = sky_bad_pixel_coords[0][i]
			ycoord = sky_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the skyect and sky
			skyData[xcoord][ycoord] = np.nan

		newobjDataMedian = np.nanmedian(objData)				
		newskyDataMedian = np.nanmedian(skyData)			

  		firstPart = np.sqrt(np.nansum((objData - newobjDataMedian)**2))
  		secondPart = np.sqrt(np.nansum((skyData - newskyDataMedian)**2))
  		denom = firstPart * secondPart
  		#print denom
  		numer = np.nansum((objData - newobjDataMedian)*(skyData - newskyDataMedian))
  		rho = numer / denom
  		print rho
  		return rho

	def crossCorrZeroth(self, objFile, skyFile, y1, y2, x1, x2):
		"""
		Def: 
		Compute the cross-correlation coefficient for a given object 
		and skyfile. Define the pixel range over which to compute the 
		coefficient. Zeroth because the zeroth and first extensions 
		only are used.

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		objFile - Input object file to compute correlation 
		skyFile - sky image to compare objFile with
		y1, y2, x1, x2 - the range to compute rho over 		
		"""
		#Trying to compute the similarity between a square grid of pixels 
		#from the object file and from the skyfile. Do this using the correlation coeff. 

		#First read in the full 2048x2048 data arrays from the object and sky files 
		#Will first consider just the first detector and can expand on this later 
		objData = fits.open(objFile)
  		objData = objData[0].data

  		skyData = fits.open(skyFile)
  		skyData = skyData[1].data

  		#Now have the arrays stored as vectors - split up into smaller grids to perform this test
  		#and save computational time. i.e. how long would it take to compute the correlation coef
  		#using the whole thing? And would this be meaningful? How do you decide upon which section 
  		#of the array to use for the correlation coefficient? 

  		objData = np.array(objData[y1:y2,x1:x2])
  		skyData = np.array(skyData[y1:y2,x1:x2])

  		#print objData
  		#print skyData

  		#These are both now 2D arrays - (Doesn't necessarily have to be square) let's compute the 
  		#Correlation coefficient 

  		#The imshift method converts nan pixels to 0. Investigating the impact of this right now. 
  		#Maybe there are implications for the imshift actually being performed properly. 
  		#If we can mask off all the pixels marked as 0, marked as np.nan and lying a certain 
  		#number of standard deviations from the median, this should be a robust way of computing 
  		#the correlation coefficient and should lead to an exact match between the 0 shift 
  		#value and the value found before shifting.

  		###############FIRST MASK OFF THE PIXELS WITH 0 VALUE FROM BOTH OBJECT AND SKY############

		obj_bad_pixel_coords = np.where(objData == 0)
		sky_bad_pixel_coords = np.where(skyData == 0)

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(obj_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = obj_bad_pixel_coords[0][i]
			ycoord = obj_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the object and sky
			objData[xcoord][ycoord] = np.nan

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(sky_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = sky_bad_pixel_coords[0][i]
			ycoord = sky_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the skyect and sky
			skyData[xcoord][ycoord] = np.nan


  		objDataMedian = np.nanmedian(objData)
  		skyDataMedian = np.nanmedian(skyData)	
		skyDataStd = np.nanstd(skyData)
  		objDataStd = np.nanstd(objData)

  		###############MASKING THE HIGH SIGMA PIXELS FOR BETTER RHO##########################
  		#Let's try masking the pixels which are bigger than a standard deviation from the median
		#Find the coordinates of the bad pixels and the slitlets 
		obj_bad_pixel_coords = np.where(abs(objData) >  (objDataMedian + objDataStd))
		sky_bad_pixel_coords = np.where(abs(skyData) >  (skyDataMedian + skyDataStd))

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(obj_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = obj_bad_pixel_coords[0][i]
			ycoord = obj_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the object and sky
			objData[xcoord][ycoord] = np.nan

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(sky_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = sky_bad_pixel_coords[0][i]
			ycoord = sky_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the skyect and sky
			skyData[xcoord][ycoord] = np.nan

		newobjDataMedian = np.nanmedian(objData)				
		newskyDataMedian = np.nanmedian(skyData)		

  		firstPart = np.sqrt(np.nansum((objData - newobjDataMedian)**2))
  		secondPart = np.sqrt(np.nansum((skyData - newskyDataMedian)**2))
  		denom = firstPart * secondPart
  		#print denom
  		numer = np.nansum((objData - newobjDataMedian)*(skyData - newskyDataMedian))
  		rho = numer / denom
  		#print rho
  		return rho

	def crossCorrFirst(self, objFile, skyFile, y1, y2, x1, x2):
		"""
		Def: 
		Compute the cross-correlation coefficient for a given object 
		and skyfile. Define the pixel range over which to compute the 
		coefficient. First because only the first extension is used

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		objFile - Input object file to compute correlation 
		skyFile - sky image to compare objFile with
		y1, y2, x1, x2 - the range to compute rho over 		
		"""
		#Trying to compute the similarity between a square grid of pixels 
		#from the object file and from the skyfile. Do this using the correlation coeff. 

		#First read in the full 2048x2048 data arrays from the object and sky files 
		#Will first consider just the first detector and can expand on this later 
		objData = fits.open(objFile)
  		objData = objData[1].data

  		skyData = fits.open(skyFile)
  		skyData = skyData[1].data

  		#Now have the arrays stored as vectors - split up into smaller grids to perform this test
  		#and save computational time. i.e. how long would it take to compute the correlation coef
  		#using the whole thing? And would this be meaningful? How do you decide upon which section 
  		#of the array to use for the correlation coefficient? 

  		objData = np.array(objData[y1:y2,x1:x2])
  		skyData = np.array(skyData[y1:y2,x1:x2])

  		#print objData
  		#print skyData

  		#These are both now 2D arrays - (Doesn't necessarily have to be square) let's compute the 
  		#Correlation coefficient 

  		#The imshift method converts nan pixels to 0. Investigating the impact of this right now. 
  		#Maybe there are implications for the imshift actually being performed properly. 
  		#If we can mask off all the pixels marked as 0, marked as np.nan and lying a certain 
  		#number of standard deviations from the median, this should be a robust way of computing 
  		#the correlation coefficient and should lead to an exact match between the 0 shift 
  		#value and the value found before shifting.

  		###############FIRST MASK OFF THE PIXELS WITH 0 VALUE FROM BOTH OBJECT AND SKY############

		obj_bad_pixel_coords = np.where(objData == 0)
		sky_bad_pixel_coords = np.where(skyData == 0)

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(obj_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = obj_bad_pixel_coords[0][i]
			ycoord = obj_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the object and sky
			objData[xcoord][ycoord] = np.nan

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(sky_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = sky_bad_pixel_coords[0][i]
			ycoord = sky_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the skyect and sky
			skyData[xcoord][ycoord] = np.nan


  		objDataMedian = np.nanmedian(objData)
  		skyDataMedian = np.nanmedian(skyData)	
		skyDataStd = np.nanstd(skyData)
  		objDataStd = np.nanstd(objData)

  		###############MASKING THE HIGH SIGMA PIXELS FOR BETTER RHO##########################
  		#Let's try masking the pixels which are bigger than a standard deviation from the median
		#Find the coordinates of the bad pixels and the slitlets 
		obj_bad_pixel_coords = np.where(abs(objData) >  (objDataMedian + objDataStd))
		sky_bad_pixel_coords = np.where(abs(skyData) >  (skyDataMedian + skyDataStd))

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(obj_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = obj_bad_pixel_coords[0][i]
			ycoord = obj_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the object and sky
			objData[xcoord][ycoord] = np.nan

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(sky_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = sky_bad_pixel_coords[0][i]
			ycoord = sky_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the skyect and sky
			skyData[xcoord][ycoord] = np.nan

		newobjDataMedian = np.nanmedian(objData)				
		newskyDataMedian = np.nanmedian(skyData)		

  		firstPart = np.sqrt(np.nansum((objData - newobjDataMedian)**2))
  		secondPart = np.sqrt(np.nansum((skyData - newskyDataMedian)**2))
  		denom = firstPart * secondPart
  		#print denom
  		numer = np.nansum((objData - newobjDataMedian)*(skyData - newskyDataMedian))
  		rho = numer / denom
  		#print rho
  		return rho

	def crossCorrOne(self, ext, objFile, skyFile, y1, y2, x1, x2):
		"""
		Def: 
		Compute the cross-correlation coefficient for a given object 
		and skyfile. Define the pixel range over which to compute the 
		coefficient. 

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		objFile - Input object file to compute correlation 
		skyFile - sky image to compare objFile with
		y1, y2, x1, x2 - the range to compute rho over 

		"""
		#Trying to compute the similarity between a square grid of pixels 
		#from the object file and from the skyfile. Do this using the correlation coeff. 

		#First read in the full 2048x2048 data arrays from the object and sky files 
		#Will first consider just the first detector and can expand on this later 
		objData = fits.open(objFile)
  		objData = objData[0].data

  		skyData = fits.open(skyFile)
  		skyData = skyData[ext].data

  		#Now have the arrays stored as vectors - split up into smaller grids to perform this test
  		#and save computational time. i.e. how long would it take to compute the correlation coef
  		#using the whole thing? And would this be meaningful? How do you decide upon which section 
  		#of the array to use for the correlation coefficient? 

  		objData = np.array(objData[y1:y2,x1:x2])
  		skyData = np.array(skyData[y1:y2,x1:x2])

  		#print objData
  		#print skyData

  		#These are both now 2D arrays - (Doesn't necessarily have to be square) let's compute the 
  		#Correlation coefficient 


  		#print objDataMedian
  		#print skyDataMedian

  		#The imshift method converts nan pixels to 0. Investigating the impact of this right now. 
  		#Maybe there are implications for the imshift actually being performed properly. 
  		#If we can mask off all the pixels marked as 0, marked as np.nan and lying a certain 
  		#number of standard deviations from the median, this should be a robust way of computing 
  		#the correlation coefficient and should lead to an exact match between the 0 shift 
  		#value and the value found before shifting.

  		###############FIRST MASK OFF THE PIXELS WITH 0 VALUE FROM BOTH OBJECT AND SKY############

		obj_bad_pixel_coords = np.where(objData == 0)
		sky_bad_pixel_coords = np.where(skyData == 0)

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(obj_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = obj_bad_pixel_coords[0][i]
			ycoord = obj_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the object and sky
			objData[xcoord][ycoord] = np.nan

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(sky_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = sky_bad_pixel_coords[0][i]
			ycoord = sky_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the skyect and sky
			skyData[xcoord][ycoord] = np.nan


  		objDataMedian = np.nanmedian(objData)
  		skyDataMedian = np.nanmedian(skyData)	
		skyDataStd = np.nanstd(skyData)
  		objDataStd = np.nanstd(objData)

  		###############MASKING THE HIGH SIGMA PIXELS FOR BETTER RHO##########################
  		#Let's try masking the pixels which are bigger than a standard deviation from the median
		#Find the coordinates of the bad pixels and the slitlets 
		obj_bad_pixel_coords = np.where(abs(objData) >  (objDataMedian + objDataStd))
		sky_bad_pixel_coords = np.where(abs(skyData) >  (skyDataMedian + skyDataStd))

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(obj_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = obj_bad_pixel_coords[0][i]
			ycoord = obj_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the object and sky
			objData[xcoord][ycoord] = np.nan

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(sky_bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = sky_bad_pixel_coords[0][i]
			ycoord = sky_bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the skyect and sky
			skyData[xcoord][ycoord] = np.nan

		newobjDataMedian = np.nanmedian(objData)				
		newskyDataMedian = np.nanmedian(skyData)		

  		firstPart = np.sqrt(np.nansum((objData - newobjDataMedian)**2))
  		secondPart = np.sqrt(np.nansum((skyData - newskyDataMedian)**2))
  		denom = firstPart * secondPart
  		#print denom
  		numer = np.nansum((objData - newobjDataMedian)*(skyData - newskyDataMedian))
  		rho = numer / denom
  		print rho
  		return rho		

  	def shiftImage(self, ext, infile, skyfile, badpmap, interp_type, stepsize, xmin, xmax, ymin, ymax):

  		"""
  		Def:
  		Compute the correlation coefficient for a grid of pixel shift values and 
  		decide which one is best (if better than the original) and apply this to 
  		the object image to align with the sky. First because we use only the 
  		first extension because of the way the shiftImageSegments function is 
  		defined. This function now applies the bad pixel map both before cross 
  		correlating and before interpolation - need to ignore bad pixels.

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		infile - Input object file to shift 
		skyFile - sky image to compare objFile with
		interp_type - type of interpolation function for the shift. 

			-'nearest': nearest neighbour
			-'linear':bilinear x,y, interpolation
			-'poly3':third order interior polynomial
			-'poly5':fifth order interior polynomial
			-'spline3':third order spline3

		stepsize - value to increment grid by each time, increasing 
		this increases the time taken for the computation 
		xmin, xmax, ymin, ymax - Grid extremes for brute force shift 

		"""

  		#We first want to apply the bad pixel map 
  		objTable = fits.open(infile)
  		objData = objTable[ext].data
  		skyTable = fits.open(skyfile)
  		skyData = skyTable[ext].data
  		badpTable = fits.open(badpmap)
  		badpData = badpTable[ext].data

		#Find the headers of the primary HDU and chosen extension 
		objPrimHeader = objTable[0].header
		objExtHeader = objTable[ext].header
		skyPrimHeader = skyTable[0].header
		skyExtHeader = skyTable[ext].header		
		badpPrimHeader = badpTable[0].header
		badpExtHeader = badpTable[ext].header

		print (objPrimHeader)
		print (objExtHeader)
		print (skyPrimHeader)
		print (skyExtHeader)		
		print (badpPrimHeader)
		print (badpExtHeader) 

		#Find the coordinates of the bad pixels and the slitlets 
		bad_pixel_coords = np.where(badpData == 0)

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = bad_pixel_coords[0][i]
			ycoord = bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the object and sky
			objData[xcoord][ycoord] = np.nan
			skyData[xcoord][ycoord] = np.nan
		#fits.writeto('pom.fits', data=objData, clobber=True)	

		#Define the minimum and maximum ranges for the correlation 
		xMinCorr = (1 * len(objData[0]))/4
		xMaxCorr = (3 * len(objData[0]))/4
		yMinCorr = (1 * len(objData))/4
		yMaxCorr = (3 * len(objData))/4


		#Write out to new temporary fits files - annoyingly need to have 
		#the data in fits files to be able to use pyraf functions
		#######OBJECT##########
		objhdu = fits.PrimaryHDU(header=objPrimHeader)
		objhdu.writeto('maskedObj.fits', clobber=True)
		fits.append('maskedObj.fits', data=objData, header=objExtHeader)

		#######Sky##########
		skyhdu = fits.PrimaryHDU(header=skyPrimHeader)
		skyhdu.writeto('maskedSky.fits', clobber=True)
		fits.append('maskedSky.fits', data=skyData, header=skyExtHeader)		

  		#First compute the correlation coefficient with just the newly saved fits file 
  		rhoArray = []

  		rhoArray.append(self.crossCorrFirst('maskedObj.fits', 'maskedSky.fits', yMinCorr, yMaxCorr, xMinCorr, xMaxCorr))

  		print rhoArray

  		#Working. Now create grid of fractional shift values. 
  		xArray = np.arange(xmin, xmax, stepsize)
  		xArray = np.around(xArray, decimals = 4)
  		yArray = np.arange(ymin, ymax, stepsize)
  		yArray = np.around(yArray, decimals = 4)

  		#Set up mesh grid of rho values for contour plot 
  		rhoGrid = np.zeros(shape=(len(xArray), len(yArray)))

  		#Before attempting the interpolation, we want to mask the bad pixel values, 
  		#and save to a fresh temporary fits file. 

  		#Loop over all values in the grid, shift the image by this 
  		#amount each time and compute the correlation coefficient
  		successDict = {}
  		for i in range(len(xArray)):
  			for j in range(len(yArray)):
  				#Perform the shift 
  				infileName = 'maskedObj.fits[1]'
  				pyraf.iraf.imshift(input=infileName, output='temp_shift.fits', \
  					xshift=xArray[i], yshift=yArray[j], interp_type=interp_type)
  				#re-open the shifted file and compute rho

  				rho = self.crossCorrZeroth('temp_shift.fits', 'maskedSky.fits',\
  				  yMinCorr, yMaxCorr, xMinCorr, xMaxCorr)
  				rhoGrid[i][j] = rho	
  				#If the correlation coefficient improves, append to new array
  				if rho > rhoArray[0]:
  					print 'SUCCESS, made improvement!'
  					entryName = str(xArray[i]) + ' and ' + str(yArray[j])
  					entryValue = [round(xArray[i], 3), round(yArray[j], 3)]
  					successDict[str(round(rho, 4))] = entryValue
  				rhoArray.append(rho)
  				#Clean up by deleting the created temporary fits file
  				os.system('rm temp_shift.fits')
  				#Go back through loop, append next value of rho
  				print 'Finished shift: %s %s, rho = %s ' % (xArray[i], yArray[j], rho)
  				#sys.stdout.flush()
  		os.system('rm maskedObj.fits')
  		os.system('rm maskedSky.fits')
  		#print round(max(rhoArray), 4)
  		#print rhoArray[0]		
  		#print successDict
  		#Now we want to choose the best shift value and actually apply this 
  		#Need to find the x and y shift values which correspond to the maximum rho
  		#Only do this if the success dictionary is not empty, if it is empty return 0.0,0.0

  		print rhoGrid

		plt.contour(xArray, yArray, rhoGrid, levels = [(np.max(rhoGrid) - (0.25 * np.std(rhoGrid))), \
		 (np.max(rhoGrid) - (1*np.std(rhoGrid))), (np.max(rhoGrid) - (1.5 * np.std(rhoGrid)))])
		plt.xlabel('$\Delta x$')
		plt.ylabel('$\Delta y$')
		plt.title('Correlation Coefficient Grid')
		plt.savefig('Correlation_coefficient.png')
		plt.close('all')

		print 'Made plot Successfully'

  		if successDict: 
  			print 'Finding Best Shift Value...'
  			rhoMax = str(round(max(rhoArray), 4))
  			#print rhoMax
  			shiftVector = successDict[rhoMax]
  			return shiftVector
  		
  		else:
  			print 'No Shift Value Found'
  			shiftVector = [0.0, 0.0]
  			return shiftVector

  		#We can also save a contour plot of the results to show where the best shift location is 
  		#This only works properly right now for a shift segment of 1, otherwise will 
  		#overwrite the filename each time. Could easily fix this.


  	def shiftImageFirst(self, ext, infile, skyfile, badpmap, interp_type, stepsize, xmin, xmax, ymin, ymax):

  		"""
  		Def:
  		Compute the correlation coefficient for a grid of pixel shift values and 
  		decide which one is best (if better than the original) and apply this to 
  		the object image to align with the sky. First because we use only the 
  		first extension because of the way the shiftImageSegments function is 
  		defined. This function now applies the bad pixel map both before cross 
  		correlating and before interpolation - need to ignore bad pixels.

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		infile - Input object file to shift 
		skyFile - sky image to compare objFile with
		interp_type - type of interpolation function for the shift. 

			-'nearest': nearest neighbour
			-'linear':bilinear x,y, interpolation
			-'poly3':third order interior polynomial
			-'poly5':fifth order interior polynomial
			-'spline3':third order spline3

		stepsize - value to increment grid by each time, increasing 
		this increases the time taken for the computation 
		xmin, xmax, ymin, ymax - Grid extremes for brute force shift 

  		"""
  		#We first want to apply the bad pixel map 
  		objTable = fits.open(infile)
  		objData = objTable[1].data
  		skyTable = fits.open(skyfile)
  		skyData = skyTable[1].data
  		badpTable = fits.open(badpmap)
  		badpData = badpTable[1].data

		#Find the headers of the primary HDU and chosen extension 
		objPrimHeader = objTable[0].header
		objExtHeader = objTable[1].header
		skyPrimHeader = skyTable[0].header
		skyExtHeader = skyTable[1].header		
		badpPrimHeader = badpTable[0].header
		badpExtHeader = badpTable[1].header

		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')
		print (objPrimHeader)
		print (objExtHeader)
		print (skyPrimHeader)
		print (skyExtHeader)
		print (badpPrimHeader)
		print (badpExtHeader)
		sys.stdout.close()
		sys.stdout = temp
		

		#Find the coordinates of the bad pixels and the slitlets 
		bad_pixel_coords = np.where(badpData == 0)

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = bad_pixel_coords[0][i]
			ycoord = bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the object and sky
			objData[xcoord][ycoord] = np.nan
			skyData[xcoord][ycoord] = np.nan
		#fits.writeto('pom.fits', data=objData, clobber=True)	

		#Define the minimum and maximum ranges for the correlation 
		xMinCorr = (1 * len(objData[0]))/4
		xMaxCorr = (3 * len(objData[0]))/4
		yMinCorr = (1 * len(objData))/4
		yMaxCorr = (3 * len(objData))/4

		#Write out to new temporary fits files - annoyingly need to have 
		#the data in fits files to be able to use pyraf functions
		#######OBJECT##########
		objhdu = fits.PrimaryHDU(header=objPrimHeader)
		objhdu.writeto('maskedObj.fits', clobber=True)
		fits.append('maskedObj.fits', data=objData, header=objExtHeader)

		#######Sky##########
		skyhdu = fits.PrimaryHDU(header=skyPrimHeader)
		skyhdu.writeto('maskedSky.fits', clobber=True)
		fits.append('maskedSky.fits', data=skyData, header=skyExtHeader)		

  		#First compute the correlation coefficient with just the newly saved fits file 
  		rhoArray = []

  		rhoArray.append(self.crossCorrFirst('maskedObj.fits', 'maskedSky.fits', yMinCorr, yMaxCorr, xMinCorr, xMaxCorr))

  		print rhoArray

  		#Working. Now create grid of fractional shift values. 
  		xArray = np.arange(xmin, xmax, stepsize)
  		xArray = np.around(xArray, decimals = 4)
  		yArray = np.arange(ymin, ymax, stepsize)
  		yArray = np.around(yArray, decimals = 4)

  		x, y = np.meshgrid(xArray, yArray)

  		#Set up mesh grid of rho values for contour plot 
  		rhoGrid = np.zeros(shape=(len(x), len(x[0])))

  		#Before attempting the interpolation, we want to mask the bad pixel values, 
  		#and save to a fresh temporary fits file. 

  		#Loop over all values in the grid, shift the image by this 
  		#amount each time and compute the correlation coefficient
  		successDict = {}
  		for i in range(len(xArray)):
  			for j in range(len(yArray)):
  				#Perform the shift 
  				infileName = 'maskedObj.fits[1]'
  				pyraf.iraf.imshift(input=infileName, output='temp_shift.fits', \
  					xshift=xArray[i], yshift=yArray[j], interp_type=interp_type)
  				#re-open the shifted file and compute rho

  				rho = self.crossCorrZeroth('temp_shift.fits', 'maskedSky.fits',\
  				  yMinCorr, yMaxCorr, xMinCorr, xMaxCorr)
  				rhoGrid[j][i] = rho	
  				#If the correlation coefficient improves, append to new array
  				if rho > rhoArray[0]:
  					print 'SUCCESS, made improvement!'
  					entryName = str(xArray[i]) + ' and ' + str(yArray[j])
  					entryValue = [round(xArray[i], 3), round(yArray[j], 3)]
  					successDict[str(round(rho, 4))] = entryValue
  				rhoArray.append(rho)
  				#Clean up by deleting the created temporary fits file
  				os.system('rm temp_shift.fits')
  				#Go back through loop, append next value of rho
  				print 'Finished shift: %s %s, rho = %s ' % (xArray[i], yArray[j], rho)
  				#sys.stdout.flush()
  		os.system('rm maskedObj.fits')
  		os.system('rm maskedSky.fits')
  		#print round(max(rhoArray), 4)
  		#print rhoArray[0]		
  		#print successDict
  		#Now we want to choose the best shift value and actually apply this 
  		#Need to find the x and y shift values which correspond to the maximum rho
  		#Only do this if the success dictionary is not empty, if it is empty return 0.0,0.0

  		#print rhoGrid

		plt.contour(x, y, rhoGrid, levels = [(np.max(rhoGrid) - (0.25 * np.std(rhoGrid))), \
		 (np.max(rhoGrid) - (1*np.std(rhoGrid))), (np.max(rhoGrid) - (1.5 * np.std(rhoGrid)))])
		plt.xlabel('$\Delta x$')
		plt.ylabel('$\Delta y$')
		plt.title('Correlation Coefficient Grid')
		plotName = infile[:-5]  + '_'  + str(ext)  + '_' + interp_type + '_CorrelationGraph.png'
		plt.savefig(plotName)
		plt.close('all')

		print 'Made plot Successfully'

  		if successDict: 
  			print 'Finding Best Shift Value...'
  			rhoMax = str(round(max(rhoArray), 4))
  			#print rhoMax
  			shiftVector = successDict[rhoMax]
  			return shiftVector
  		
  		else:
  			print 'No Shift Value Found'
  			shiftVector = [0.0, 0.0]
  			return shiftVector

  		#We can also save a contour plot of the results to show where the best shift location is 
  		#This only works properly right now for a shift segment of 1, otherwise will 
  		#overwrite the filename each time. Could easily fix this.






  	def rotateImage(self, ext, infile, skyfile, interp_type, minAngle, maxAngle, stepsize):

  		"""
  		Def:
  		Compute the correlation coefficient for a line of rotation angles and 
  		decide which one is best (if better than the original) and apply this to 
  		the object image to align with the sky.

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		infile - Input object file to shift 
		skyFile - sky image to compare objFile with
		interp_type - type of interpolation function for the shift. 

			-'nearest': nearest neighbour
			-'linear':bilinear x,y, interpolation
			-'poly3':third order interior polynomial
			-'poly5':fifth order interior polynomial
			-'spline3':third order spline3

		stepsize - value to increment grid by each time, increasing 
		this increases the time taken for the computation 
		xmin, xmax, ymin, ymax - Grid extremes for brute force shift 

  		"""
  		#First compute the correlation coefficient with just infile 
  		rhoArray = []
  		rhoArray.append(self.crossCorr(ext, infile, skyfile, 1000, 1200, 1000, 1200))
  		print rhoArray

  		#Working. Now create grid of fractional shift values. 
  		rotArray = np.arange(minAngle, maxAngle, stepsize)
  		rotArray = np.around(rotArray, decimals = 4)


  		#Loop over all values in the grid, shift the image by this 
  		#amount each time and compute the correlation coefficient
  		successDict = {}

		for number in rotArray:
			#Perform the shift 
			infileName = infile + '[' + str(ext) + ']'
			pyraf.iraf.rotate(input=infileName, output='temp_rot.fits', \
				rotation=number, interpolant=interp_type)
			#re-open the shifted file and compute rho
			rho = self.crossCorrOne(ext,'temp_rot.fits', skyfile,\
			 1000, 1200, 1000, 1200)
			#If the correlation coefficient improves, append to new array
			if rho > rhoArray[0]:
				print 'SUCCESS, made improvement!'
				entryName = str(number)
				entryValue = [number, rho]
				successDict[entryName] = entryValue
			rhoArray.append(rho)
			#Clean up by deleting the created temporary fits file
			os.system('rm temp_rot.fits')
			#Go back through loop, append next value of rho
			print 'Finished shift: %s, rho = %s ' % (number, rho)
			#sys.stdout.flush()

  		print max(rhoArray)
  		print rhoArray[0]		
  		print successDict 		


  	def imSplit(self, ext, infile, vertSegments, horSegments):

  		"""
  		Def: 
  		Take an input image file and split up into a series of squares 
  		Main purpose is for use in the shiftImageSegments functino

  		Input: 
  		infile - file to be divided
  		vertSegments - number of vertical segments (2048 must be divisible by) 
  		horSegments - number of horizontal segments (2048 must be divisible by)
  		ext - extension number

  		Output: 
  		segmentArray - 1D array containing 2D square array segments

  		"""
  		#Read in the data file at the given extension
  		data = fits.open(infile)
  		data = data[ext].data

  		#Initialise the empty array
  		segmentArray = []

  		#We can't do this if 2048 isn't divisible by the segments 
  		#write an error function to check that this is the case 
  		#And exit if it isn't 
  		if ((2048 % vertSegments != 0) or  (2048 % horSegments != 0)):
  			raise ValueError('Please ensure that 2048 is divisible by your segment choice')

		#Counters for the horizontal slicing
		hor1 = 0
		hor2 = (2048 / horSegments)

		for j in range(horSegments):
			

			#Counters for the vertical slicing
			x = 0
			y = (2048 / vertSegments)

			for i in range(vertSegments):

			   
			   #Slice the data according to user selection
			   segmentArray.append(data[hor1:hor2,x:y])
			   x += (2048 / vertSegments)
			   y += (2048 / vertSegments)

			hor1 += (2048 / horSegments)
			hor2 += (2048 / horSegments)   

		#print segmentArray	
		return segmentArray	






  	def shiftImageSegments(self, ext, infile, skyfile, badpmap,\
  	 vertSegments, horSegments, interp_type, stepsize, xmin, xmax, ymin, ymax):

  		"""
  		Def: 
  		Lots of arguments because of using lots of different functions. 
  		This is taking an object and a sky image, splitting them into a specified 
  		number of segments, performing shifts to each of the segments and then 
  		computing the cross-correlation function to see if we can improve the 
  		alignment at all. Should give better results than a global shift 

  		Inputs: 
  		infile - file to be divided
		skyFile - sky image to compare objFile with
  		vertSegments - number of vertical segments (2048 must be divisible by) 
  		horSegments - number of horizontal segments (2048 must be divisible by)
  		ext - extension number
		interp_type - type of interpolation function for the shift. 

			-'nearest': nearest neighbour
			-'linear':bilinear x,y, interpolation
			-'poly3':third order interior polynomial
			-'poly5':fifth order interior polynomial
			-'spline3':third order spline3

		stepsize - value to increment grid by each time, increasing 
		this increases the time taken for the computation 
		xmin, xmax, ymin, ymax - Grid extremes for brute force shift 

		"""
		########################TO ADD###############################
		#Make sure that 2048 is divisible by the segment numbers 



		#Create arrays of the split files using the imSplit function 
		objArray = self.imSplit(ext, infile, vertSegments, horSegments)
		skyArray = self.imSplit(ext, skyfile, vertSegments, horSegments)
		badpArray = self.imSplit(ext, badpmap, vertSegments, horSegments)

		#Find the headers of the primary HDU and chosen extension 
		objTable = fits.open(infile)
		objPrimHeader = objTable[0].header
		objExtHeader = objTable[ext].header
		skyTable = fits.open(skyfile)
		skyPrimHeader = skyTable[0].header
		skyExtHeader = skyTable[ext].header
		badpTable = fits.open(badpmap)
		badpPrimHeader = badpTable[0].header
		badpExtHeader = badpTable[ext].header

		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')
		print (objPrimHeader)
		print (objExtHeader)
		print (skyPrimHeader)
		print (skyExtHeader)
		print (badpPrimHeader)
		print (badpExtHeader)
		sys.stdout.close()
		sys.stdout = temp


		shiftArray = []
		#Should now have two 1D arrays of 2D arrays of equal size
		for i in range(len(objArray)):

			tempObjName = infile[:-5] + str(vertSegments) + str(horSegments) + str(i) + '_temp.fits'
			#Write out to new temporary fits files - annoyingly need to have 
			#the data in fits files to be able to use pyraf functions
			#######OBJECT##########
			objhdu = fits.PrimaryHDU(header=objPrimHeader)
			objhdu.writeto(tempObjName, clobber=True)
			fits.append(tempObjName, data=objArray[i], header=objExtHeader)

			#######SKY#############
			skyhdu = fits.PrimaryHDU(header=skyPrimHeader)
			skyhdu.writeto('tempSky.fits', clobber=True)
			fits.append('tempSky.fits', data=skyArray[i], header=skyExtHeader)

			#######BADPIXEL#############
			badphdu = fits.PrimaryHDU(header=badpPrimHeader)
			badphdu.writeto('tempbadp.fits', clobber=True)
			fits.append('tempbadp.fits', data=badpArray[i], header=badpExtHeader)

			#NOTES FOR RESUMING - I now have temporary files containing the object 
			#and sky segments to be shifted and cross correlated. The shiftImage function 
			#can be applied to each of these directly within the for loop!! - remember to 
			# a) clean up the fits files after each loop 
			# b) Find a way to look at the cross correlation results for each segment independently
			# probably by returning the cross correlation arrays into a new array  
			# c) find a way to search specifically for shift success and apply to each segment 
			# d) find a way to recombine all segments together after the shift has happened 
			# e) make sure to use the crossCorrFirst function here, otherwise it will break in 
			# certain situations (i.e. when not using extension one	)

			#Now need to apply the shiftImageFirst function, which compares the chosen 
			#extension shifted object and sky files. 
			#Create an array to hold the shift coordinates for each segment. This is defined 
			#Outside the for loop so that I am not initialising it every time.
			

			print 'This is shift: %s' % i
			shiftArray.append(self.shiftImageFirst(ext, tempObjName, 'tempSky.fits', 'tempbadp.fits', \
			 interp_type, stepsize, xmin, xmax, ymin, ymax))



			#Clean up the temporary fits files during each part of the loop 
			os.system('rm %s' % tempObjName)
			os.system('rm tempSky.fits')
			os.system('rm tempbadp.fits')

			#That should work then for each segment in turn. Does work. 
			#vStackArray.append(np.hstack(objArray))		
			#	hor1 += 128
			#	hor2 += 128	

			#Now just need to vstack all of these arrays and will have a 2048x2048 corrected array
			#newObjData = np.vstack(vStackArray)	
		print shiftArray
		#Now the clever part - to actually apply the shifts to the unmasked infile 
		#imshift can be used with a list of infile names, outfile names and shift coordinates
		#If I create these lists I can imshift all at once, read in the data and then recombine
		#First get the x and y vectors for the shift coordinates
		xArray = []
		yArray = []
		for item in shiftArray:
			xArray.append(item[0])
			yArray.append(item[1])
		xArray = np.array(np.around(xArray, 3))
		yArray = np.array(np.around(yArray, 3))	
		#Create the .txt file with the two columns specifying the shift coordinates
		np.savetxt('coords.txt', np.c_[xArray, yArray], fmt=('%5.3f', '%5.3f'))

		#Coordinates list sorted. Now need list of input files and input file names 
		#To do this need to go back to the imSplit method with the unmasked file 
		#Write to a list of temporary fits files, which will become temporary 
		#Output fits files, which will be read back in as data before recombining 
		#Must clean everything up at the end by removing with os.system()
		#Create arrays of the split files using the imSplit function 

		#Need to apply the shifts to a masked object file. Open the bad pixel map 
		#and the object file and mask the pixels and save to temporary file 
		#Find the coordinates of the bad pixels and the slitlets 
		objData = objTable[ext].data
		badpData = badpTable[ext].data
		bad_pixel_coords = np.where(badpData == 0)
		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = bad_pixel_coords[0][i]
			ycoord = bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the object and sky
			objData[xcoord][ycoord] = np.nan

		#Write out to new file which will then be read in to split up the data
		objhdu = fits.PrimaryHDU(header=objPrimHeader)
		objhdu.writeto('temp_masked.fits', clobber=True)
		fits.append('temp_masked.fits', data=objData, header=objExtHeader)


		objArray = self.imSplit(1, 'temp_masked.fits', vertSegments, horSegments)
		inFileArray = []
		outFileArray = []
		shiftedDataArray = []
		vstackArray = []
		hstackArray = []


		#Should now have two 1D arrays of 2D arrays of equal size
		for i in range(len(objArray)):

			inFileName = 'tempObjin'+str(i)+'.fits'
			outFileName = 'tempObjout'+str(i)+'.fits'
			inFileArray.append(inFileName)
			outFileArray.append(outFileName)

			#Write out to new temporary fits files - annoyingly need to have 
			#the data in fits files to be able to use pyraf functions
			#######OBJECT##########
			objhdu = fits.PrimaryHDU(header=objPrimHeader)
			objhdu.writeto(inFileName, clobber=True)
			fits.append(inFileName, data=objArray[i], header=objExtHeader)

			inFileName = inFileName + '[1]'

  			#Now apply imshift with all the parameters 
  			pyraf.iraf.imshift(input = inFileName, output=outFileName, \
  				xshift=xArray[i], yshift=yArray[i] ,interp_type=interp_type)

  			#We want a 1D array of 2D arrays again, read the data files back in 
  			data = fits.open(outFileName)
  			data = data[0].data
  			shiftedDataArray.append(data)
  			#Go back to the top of the loop and grab the next file

  		#The final problem is that we have a 1D arrays of 2D arrays that needs 
  		#to be recombined into the original 2048 x 2048 which created it 
  		#Ordering depends on the number of vertical and horizontal segments 
 
  		x = len(shiftedDataArray) / horSegments	
  		a = 0 

  		while x <= len(shiftedDataArray):
  			for i in range(a, x): 
  				print i
  				hstackArray.append(shiftedDataArray[i])

  			vstackArray.append(np.hstack(hstackArray))
  			hstackArray = []
  			x += len(shiftedDataArray) / horSegments
  			a += len(shiftedDataArray) / horSegments
  			

  		#for item in vstackArray:
  			#print item.shape	
  		#Reconstruct by vstacking the final array	
  		reconstructedData = np.vstack(vstackArray)

  		#Clean up by getting rid of uneeded files
		for item in inFileArray:
			os.system('rm %s' % item)
		for item in outFileArray:
			os.system('rm %s' % item)
		os.system('rm coords.txt')	
		os.system('rm temp_masked.fits')
		os.system('rm log.txt')

		return reconstructedData



	def shiftAllExtensions(self, infile, skyfile, badpmap,\
  	 vertSegments, horSegments, interp_type, stepsize, xmin, xmax, ymin, ymax ):

		"""
		Def: Uses the shiftImageSegments method for each extensions and then combines
		all of these together into a single shifted fits file 

		"""

		#Prepare the headers for writing out the fits file
		objTable = fits.open(infile)
		objPrimHeader = objTable[0].header
		objExtHeader1 = objTable[1].header
		objExtHeader2 = objTable[2].header
		objExtHeader3 = objTable[3].header

		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')
		print (objPrimHeader)
		print (objExtHeader1)
		print (objExtHeader2)
		print (objExtHeader3)						
		sys.stdout.close()
		sys.stdout = temp

		#Set up the array 
		reconstructedDataArray = []

		#Use the shifted image segment function 
		for i in range(1, 4):
			print 'Shifting Extension: %s' % i
			reconstructedDataArray.append(self.shiftImageSegments(i, infile, skyfile, badpmap,\
  	 vertSegments, horSegments, interp_type, stepsize, xmin, xmax, ymin, ymax))


		#Name the shifted data file 
  		shiftedName = infile[:-5] + '_' + str(vertSegments) + str(horSegments) + '_' + interp_type + '_Shifted.fits' 
  		print 'Saving %s' % shiftedName

  		objhdu = fits.PrimaryHDU(header=objPrimHeader)
		objhdu.writeto(shiftedName, clobber=True)
		fits.append(shiftedName, data=reconstructedDataArray[0], header=objExtHeader1)
		fits.append(shiftedName, data=reconstructedDataArray[1], header=objExtHeader2)
		fits.append(shiftedName, data=reconstructedDataArray[2], header=objExtHeader3)


	def applyShiftAllExtensions(self, fileList, badpmap,\
  	 vertSegments, horSegments, interp_type, stepsize, xmin, xmax, ymin, ymax):
		#Read in the data from the fileList
		data = np.genfromtxt(fileList, dtype='str')
		#Save the names and types as lists 
		names = data[0:,0]
		types = data[0:,1]
		#Loop round all names and apply the computeOffsetSegments method
		for i in range(1, len(names)):
			if types[i] == 'O':
				objFile = names[i]
				if i == 1:
					skyFile = names[i + 1]
				elif types[i - 1] == 'S':
					skyFile = names[i - 1]
				else:
					skyFile = names[i + 1]

				print 'Shifting file: %s : %s' % (i, objFile)	
				#Now use the method defined within this class 
				self.shiftAllExtensions(objFile, skyFile, badpmap,\
  	 vertSegments, horSegments, interp_type, stepsize, xmin, xmax, ymin, ymax )
				#Which will loop through all and save the corrected object file 
				#as objectFile_Corrected.fits. These are then fed through the pipeline. 


























