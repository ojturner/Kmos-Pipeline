#This class houses the methods which are relevant to manual additions to the ESO KMOS pipeline
#Mainly concentrating on two procedures - 1) pedestal readout column correction and  
# 2) shifting and aligning sky and object images before data cube reconstruction


#import the relevant modules
import os, sys, numpy as np, random, matplotlib.mlab as mlab, matplotlib.pyplot as plt, math, matplotlib.cm as cm
from matplotlib.backends.backend_pdf import PdfPages
from pylab import *
from matplotlib.colors import LogNorm
import numpy.polynomial.polynomial as poly
import lmfit
import random
from itertools import cycle as cycle
from lmfit.models import GaussianModel, ExponentialModel, LorentzianModel, VoigtModel, PolynomialModel
from scipy import stats
from scipy.optimize import minimize
from scipy.optimize import basinhopping
from astropy.io import fits

#add the class file to the PYTHONPATH
sys.path.append('/Users/owenturner/Documents/PhD/KMOS/Analysis_Pipeline/Python_code/Class')
from cubeClass import cubeOps


#Import pyraf - python iraf 
import pyraf

####################################################################

class pipelineOps(object):
	#Initialiser creates an instance of the spectrumFit object 
	def __init__(self):
		self.self = self

############################################################################################
#Current functions in this file: 
#computeOffset - from a sky file, object file, bad pixel map and full lcal file 
#                compute the column correction for the 2048x64 pixel chunks 
#
#computeOffsetTwo - Same as above, but uses the output of the stackLcal function 
#                   which are a more conservative set of stacked lcal maps for each detector
#
#computeOffsetTopFour - Use the top four pixels on the raw subtracted frame to compute the offset
#
#computeOffsetSegments  - Same as top method but splitting the resultant into more chunks
#
#subFrames       - Take one fits image with three data extensions and subtract another 
#
#pixelHistogram  - select a chunk of pixels and compute and draw a histogram of their values
#
#stackLcal       - stack the lcal frames from the six different rotation angles. Makes sure 
#                   that the median pixel value will be more reliable 
#
#applyCorrections - takes a sorted list of fits files (piped from dfits) and applies the correction
#	                from the computeOffsetSegments method		
############################################################################################

######################################################################################
#MODULE: computeOffset
#
#PURPOSE:
#Take the object image after calibration, along with a sky frame, bad pixel frame and 
#the lcal frame and homogenise the readout columns, so that after subtraction gives 0
#
#INPUTS:
#
#			objectFile: The object image to be corrected 	
#			skyFile: The sky image taken directly before or after the object frame
#			badPMap: The bad pixel frame generated by the pipeline
#			lcalMap: The calibration frame generated by the pipeline
#
#OUTPUTS: 	newObjData: The corrected 2048x2048 object arrray which can then be saved
#						
#
#USAGE: 	newObjData = computeOffset(objectFile, skyFile, badPMap, lcalMap)
#######################################################################################	

	
	#Access the primary extension, later this will be looped  
	def computeOffset(self, objectFile, skyFile, badPMap, lcalMap): 
		
		#Set up vector to house the corrected extensions
		correctedExtensions=[]
		#Read in the tables of data
		table_o = fits.open(objectFile)
		fitsHeader = table_o[0].header
		header_one = table_o[1].header
		header_two = table_o[2].header 
		header_three = table_o[3].header

		print fitsHeader
		print header_one
		print header_two
		print header_three

		table_s = fits.open(skyFile)
		bad_pixel_table = fits.open(badPMap)

		#Now choose the correct rotation angle 
		lcal_table = fits.open(lcalMap)
		#This is a list of all possible rotation angles 
		angleList = np.array([0, 60, 120, 180, 240, 300])
		#Select the ocs.rot.naangle keyword 
		obsAngle = table_o[0].header["HIERARCH ESO OCS ROT NAANGLE"]
		#If negative, add 360 to get the actual rotation angle
		if obsAngle < 0:
			obsAngle = obsAngle + 360
		#Find where the difference between the observed and idealised angle is minimum
		newAngleList = abs( obsAngle - angleList )
		n = newAngleList.argmin()
		obsAngleNew = angleList[n]


		#Find the extension to which this corresponds
		val = 0
		if obsAngleNew == 0:
			val = 1 
		elif obsAngleNew == 60:
			val = 4
		elif obsAngleNew == 120:
			val = 7
		elif obsAngleNew == 180: 
			val = 10
		elif obsAngleNew == 240:
			val = 13
		elif obsAngleNew == 300:
			val = 16		
		print val	


		#Loop over the fits image extensions, do the same each time
		for count in range(1,4):
			print val
			data_o = table_o[count].data
			data_s = table_s[count].data

			#Create copies of the data arrays so that I can mask the bad pixels 
			#and lcal pixels outside of the loop, instead of wasting time inside 
			manObjData = copy(data_o)
			manSkyData = copy(data_s)

			#Read in the bad pixel and lcal maps
			bad_pixel_data = bad_pixel_table[count].data	
			bad_pixel_data[bad_pixel_data == 0] = np.nan		
			lcal_data = lcal_table[val].data
			lcal_data[np.invert(np.isnan(lcal_data))] = 1.0

			#Mask off the bad pixels in the object and sky data
			manObjData = manObjData * bad_pixel_data
			manObjData = manObjData * lcal_data
			manSkyData = manSkyData * bad_pixel_data
			manSkyData = manSkyData * lcal_data


			#Need to slice this 2D array into a 
			#1D array of 2D arrays, each of which is 64 pixels wide 
			#so that I can examine these in turn and loop over them 

			#Counters for the slicing 
			x = 0
			y = 64

			#1D arrays to host the data 
			skyArray = []
			objArray = []
			manObjArray = []
			manSkyArray = []
			badPArray = []
			lcalArray = []
			testArray = []

			for i in range(32):
			   
			   #Slice each of the data files into 32 columns of 64 pixels width
			   newObjectArray = data_o[:,x:y]
			   newSkyArray = data_s[:,x:y]
			   newManObjArray = manObjData[:,x:y]
			   newManSkyArray = manSkyData[:,x:y]
			   newPArray = bad_pixel_data[:,x:y]
			   newCalArray = lcal_data[:,x:y]
			   #newTestArray = test_array[:,x:y]
			   #testArray.append(newTestArray)
			   objArray.append(newObjectArray)
			   skyArray.append(newSkyArray)
			   manObjArray.append(newManObjArray)
			   manSkyArray.append(newManSkyArray)
			   badPArray.append(newPArray)
			   lcalArray.append(newCalArray)

			   #Add 64 to the counters each time to create the slices
			   x += 64
			   y += 64

			#Now wanto to read in the lcal and the bad pixel map, 
			#To get a list of the pixel indices which should be averaged
			#First need to slice this in the same way as the other file 
			#Then simply find the bad pixel and the lcal positions, hide 
			#these as nan and compute the median in each 64 pixel column 
			#from everything that isn't nan, then apply the correction to the 
			#data before stitching everything back together as a data file
			#And feeding back into the pipeline at the appropriate location

			#Redefine objArray and sky array, do manipulations to temp
			#objTemp = copy(objArray)
			#skyTemp = copy(skyArray)
			#Start the loop for all the columns in the Array vectors 
			for num in range(len(objArray)):


				#Now all the pixels that shouldn't be included in the median 
				#have value nan. Can then just do np.nanmean(objTemp) which will ignore nan's
				#then repeat the process for the sky, compare the mean's, compute and apply the offset.

				obj_mean = np.nanmedian(manObjArray[num])
				sky_mean = np.nanmedian(manSkyArray[num])
				print sky_mean
				print obj_mean

				#Need to compare the two medians to see how to apply the offset.
				#If the sky is brighter, add the difference to the object image

				if sky_mean > obj_mean:
					objArray[num] += abs(sky_mean - obj_mean)
				elif obj_mean > sky_mean:
					objArray[num] -= abs(obj_mean - sky_mean)	

			#Have now made the correction to all 32 of the 64 pixel width columns.
			#All that is left to do is stitch the objArray arrays back together to 
			#give a single 2048x2048 array.
			newObjData = np.hstack(objArray)
			correctedExtensions.append(newObjData)
			if count == 1:
				print 'Computed First Correction'
			elif count == 2:
				print 'Computed Second Correction'
			elif count == 3: 
				print 'Computed Third Correction'			
			val += 1				
		#Create the object fits file with the three corrected extensions
		fileName = raw_input('Enter a name for the corrected fits file: ') + '.fits'
		#Note that the readout complained about the header not being 
		#in the correct fits format
		hdu = fits.PrimaryHDU(header=fitsHeader)
		hdu.writeto(fileName, clobber=True)
		fits.append(fileName, data=correctedExtensions[0], header=header_one)	
		fits.append(fileName, data=correctedExtensions[1], header=header_two)	
		fits.append(fileName, data=correctedExtensions[2], header=header_three)

	#Second compute offset method, this time with the refined lcal
	#Access the primary extension, later this will be looped  
	def computeOffsetTwo(self, objectFile, skyFile, badPMap, lcal1, lcal2, lcal3): 
		#Set up vector to house the corrected extensions
		correctedExtensions=[]
		#Read in the tables of data
		table_o = fits.open(objectFile)
		fitsHeader = table_o[0].header
		header_one = table_o[1].header
		header_two = table_o[2].header 
		header_three = table_o[3].header

		print fitsHeader
		print header_one
		print header_two
		print header_three

		table_s = fits.open(skyFile)
		bad_pixel_table = fits.open(badPMap)

		#Now read in the refined lcal maps 
		lcal1 = fits.open(lcal1)
		lcal2 = fits.open(lcal2)
		lcal3 = fits.open(lcal3)

		#Save to a dictionary 
		d = {}
		d[1] = lcal1[0].data
		d[2] = lcal2[0].data
		d[3] = lcal3[0].data

		#Loop over the fits image extensions, do the same each time
		for count in range(1,4):
			data_o = table_o[count].data
			data_s = table_s[count].data
			bad_pixel_data = bad_pixel_table[count].data			
			lcal_data = d[count]

			#Loop around the bad pixel locations
			#Now we have both the object and sky data these are 2D arrays, 
			#essentially a matrix where each number represents a pixel flux, 
			#And the location of the number in the matrix represents the 
			#Pixel position on the detector, which in turn corresponds to the 
			#objects position on the sky. Need to slice this 2D array into a 
			#1D array of 2D arrays, each of which is 64 pixels wide 
			#so that I can examine these in turn and loop over them 

			#Counters for the slicing 
			x = 0
			y = 64

			#1D arrays to host the data 
			skyArray = []
			objArray = []
			badPArray = []
			lcalArray = []

			for i in range(32):
			   
			   #Slice each of the data files into 32 columns of 64 pixels width
			   newObjectArray = data_o[:,x:y]
			   newSkyArray = data_s[:,x:y]
			   newPArray = bad_pixel_data[:,x:y]
			   newCalArray = lcal_data[:,x:y]
			   objArray.append(newObjectArray)
			   skyArray.append(newSkyArray)
			   badPArray.append(newPArray)
			   lcalArray.append(newCalArray)

			   #Add 64 to the counters each time to create the slices
			   x += 64
			   y += 64

			#Now wanto to read in the lcal and the bad pixel map, 
			#To get a list of the pixel indices which should be averaged
			#First need to slice this in the same way as the other file 
			#Then simply find the bad pixel and the lcal positions, hide 
			#these as nan and compute the median in each 64 pixel column 
			#from everything that isn't nan, then apply the correction to the 
			#data before stitching everything back together as a data file
			#And feeding back into the pipeline at the appropriate location

			#Redefine objArray and sky array, do manipulations to temp
			objTemp = copy(objArray)
			skyTemp = copy(skyArray)
			#Start the loop for all the columns in the Array vectors 
			for num in range(len(objArray)):


				#Find the coordinates of the bad pixels and the slitlets 
				bad_pixel_coords = np.where(badPArray[num] == 0)
				lcal_pixel_coords = np.where(lcalArray[num] == 0)


				#Loop through the first 2048 x 64 data and sky columns and mask off these coords
				#Then compute the median in both columns. If median sky > median obj, add the difference 
				#between the median values to the obj. If the other way around, decrement

				#Loop around the bad pixel locations
				for i in range(len(bad_pixel_coords[0])):
					#Because of the way np.where works, need to define the x and y coords in this way
					xcoord = bad_pixel_coords[0][i]
					ycoord = bad_pixel_coords[1][i]
					#Now set all positions where there is a dead pixel to np.nan in the object and sky
					objTemp[num][xcoord][ycoord] = np.nan
					skyTemp[num][xcoord][ycoord] = np.nan

				#Loop around the slitlet positions
				for i in range(len(lcal_pixel_coords[0])):
					#Do the same, this time for the slitlet positions (substantially more will have a value)
					xcoord = lcal_pixel_coords[0][i]
					ycoord = lcal_pixel_coords[1][i]
					#Set all of these locations to nan 
					objTemp[num][xcoord][ycoord] = np.nan
					skyTemp[num][xcoord][ycoord] = np.nan


				#Now all the pixels that shouldn't be included in the median 
				#have value nan. Can then just do np.nanmean(objTemp) which will ignore nan's
				#then repeat the process for the sky, compare the mean's, compute and apply the offset.

				obj_mean = np.nanmedian(objTemp[num])
				sky_mean = np.nanmedian(skyTemp[num])
				print sky_mean
				print obj_mean

				#Need to compare the two medians to see how to apply the offset.
				#If the sky is brighter, add the difference to the object image

				if sky_mean > obj_mean:
					objArray[num] += abs(sky_mean - obj_mean)
				elif obj_mean > sky_mean:
					objArray[num] -= abs(obj_mean - sky_mean)	


			#Have now made the correction to all 32 of the 64 pixel width columns.
			#All that is left to do is stitch the objArray arrays back together to 
			#give a single 2048x2048 array.
			newObjData = np.hstack(objArray)
			correctedExtensions.append(newObjData)
			if count == 1:
				print 'Computed First Correction'
			elif count == 2:
				print 'Computed Second Correction'
			elif count == 3: 
				print 'Computed Third Correction'			
		#Create the object fits file with the three corrected extensions
		fileName = raw_input('Enter a name for the corrected fits file: ') + '.fits'
		#Note that the readout complained about the header not being 
		#in the correct fits format
		hdu = fits.PrimaryHDU(header=fitsHeader)
		hdu.writeto(fileName, clobber=True)
		fits.append(fileName, data=correctedExtensions[0], header=header_one)	
		fits.append(fileName, data=correctedExtensions[1], header=header_two)	
		fits.append(fileName, data=correctedExtensions[2], header=header_three)	

	def computeOffsetTopFour(self, rawSubFile, objectFile):

		correctedExtensions = []
		#Read in the tables of data
		table_o = fits.open(objectFile)
		fitsHeader = table_o[0].header
		header_one = table_o[1].header
		header_two = table_o[2].header 
		header_three = table_o[3].header

		print fitsHeader
		print header_one
		print header_two
		print header_three
		table_s = fits.open(rawSubFile)	


		#Loop over the fits image extensions, do the same each time
		for count in range(1,4):
			data_o = table_o[count].data
			data_s = table_s[count].data

			#Counters for the slicing 
			x = 0
			y = 64

			#1D arrays to host the data 
			subArray = []
			objArray = []

			for i in range(32):
			   
			   #Slice each of the data files into 32 columns of 64 pixels width
			   newObjectArray = data_o[:,x:y]
			   newSubArray = data_s[2044:2048,x:y]

			   objArray.append(newObjectArray)
			   subArray.append(newSubArray)

			   #Add 64 to the counters each time to create the slices
			   x += 64
			   y += 64		

			#testData = np.hstack(subArray)
			#fileName = 'testTopFour' + str(count) + '.fits'
			#fits.writeto(fileName, data=testData, clobber=True)   

			for num in range(len(objArray)):
				correctionMedian = np.median(subArray[num])
				objArray[num] -= correctionMedian
				print correctionMedian
				print subArray[num][:,0:1]
			#Have now made the correction to all 32 of the 64 pixel width columns.
			#All that is left to do is stitch the objArray arrays back together to 
			#give a single 2048x2048 array.
			newObjData = np.hstack(objArray)
			correctedExtensions.append(newObjData)
			if count == 1:
				print 'Computed First Correction'
			elif count == 2:
				print 'Computed Second Correction'
			elif count == 3: 
				print 'Computed Third Correction'			
		#Create the object fits file with the three corrected extensions
		fileName = raw_input('Enter a name for the corrected fits file: ') + '.fits'
		#Note that the readout complained about the header not being 
		#in the correct fits format
		hdu = fits.PrimaryHDU(header=fitsHeader)
		hdu.writeto(fileName, clobber=True)
		fits.append(fileName, data=correctedExtensions[0], header=header_one)	
		fits.append(fileName, data=correctedExtensions[1], header=header_two)	
		fits.append(fileName, data=correctedExtensions[2], header=header_three)				
			
	def computeOffsetSegments(self, objectFile, skyFile, badPMap, lcalMap):
		#function should be identical to compute Offset until the initial pixel loop

		#Set up vector to house the corrected extensions
		correctedExtensions=[]

		#Set up vector to house the segments to be vStacked. This is differnet from the 
		#usual compute offset method which just uses each individual readout column

		#Read in the tables of data
		table_o = fits.open(objectFile)
		fitsHeader = table_o[0].header
		header_one = table_o[1].header
		header_two = table_o[2].header 
		header_three = table_o[3].header

		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')		
		print fitsHeader
		print header_one
		print header_two
		print header_three
		sys.stdout.close()
		sys.stdout = temp

		table_s = fits.open(skyFile)
		bad_pixel_table = fits.open(badPMap)

		#Now choose the correct rotation angle 
		lcal_table = fits.open(lcalMap)
		#This is a list of all possible rotation angles 
		angleList = np.array([0, 60, 120, 180, 240, 300])
		#Select the ocs.rot.naangle keyword 
		obsAngle = table_o[0].header["HIERARCH ESO OCS ROT NAANGLE"]
		print obsAngle
		if obsAngle < 0:
			obsAngle = obsAngle + 360
		#Find where the difference between the observed and idealised angle is minimum
		newAngleList = abs( obsAngle - angleList )
		n = newAngleList.argmin()
		obsAngleNew = angleList[n]
		print obsAngleNew


		#Find the extension to which this corresponds
		val = 0
		if obsAngleNew == 0:
			val = 1 
		elif obsAngleNew == 60:
			val = 4
		elif obsAngleNew == 120:
			val = 7
		elif obsAngleNew == 180: 
			val = 10
		elif obsAngleNew == 240:
			val = 13
		elif obsAngleNew == 300:
			val = 16		
		print val	


		#Loop over the fits image extensions, do the same each time
		for count in range(1,4):
			vStackArray = []

			print val
			data_o = table_o[count].data
			data_s = table_s[count].data

			#Create copies of the data arrays so that I can mask the bad pixels 
			#and lcal pixels outside of the loop, instead of wasting time inside 
			manObjData = copy(data_o)
			manSkyData = copy(data_s)

			#Read in the bad pixel and lcal maps
			bad_pixel_data = bad_pixel_table[count].data			
			lcal_data = lcal_table[val].data

			#Find the coordinates of the bad pixels and the slitlets 
			bad_pixel_coords = np.where(bad_pixel_data == 0)
			lcal_pixel_coords = np.where(lcal_data > 0)

			#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
			for i in range(len(bad_pixel_coords[0])):
				#Because of the way np.where works, need to define the x and y coords in this way
				xcoord = bad_pixel_coords[0][i]
				ycoord = bad_pixel_coords[1][i]
				#Now set all positions where there is a dead pixel to np.nan in the object and sky
				manObjData[xcoord][ycoord] = np.nan
				manSkyData[xcoord][ycoord] = np.nan

			#Loop around the slitlet positions
			for i in range(len(lcal_pixel_coords[0])):
				#Do the same, this time for the slitlet positions (substantially more will have a value)
				xcoord = lcal_pixel_coords[0][i]
				ycoord = lcal_pixel_coords[1][i]
				#Set all of these locations to nan 
				manObjData[xcoord][ycoord] = np.nan
				manSkyData[xcoord][ycoord] = np.nan			

			#fits.writeto('test.fits', data=manObjData, clobber=True)
			#fits.writeto('test1.fits', data=manSkyData, clobber=True)	
			#Debug to see if mask is being applied properly
			#test_array = np.zeros(shape=[2048, 2048])
			#tempName = 'lcal' + str(count) + '.fits'
			#Loop around the bad pixel locations

			#Now we have both the object and sky data these are 2D arrays, 
			#essentially a matrix where each number represents a pixel flux, 
			#And the location of the number in the matrix represents the 
			#Pixel position on the detector, which in turn corresponds to the 
			#objects position on the sky. Need to slice this 2D array into a 
			#1D array of 2D arrays, each of which is 64 pixels wide 
			#so that I can examine these in turn and loop over them 


			#Counters for the horizontal slicing
			hor1 = 0
			hor2 = 128

			for j in range(16):
				print hor1
				print hor2
				#Counters for the slicing vertical slicing
				x = 0
				y = 64
				#1D arrays to host the data 
				skyArray = []
				objArray = []
				manObjArray = []
				manSkyArray = []
				badPArray = []
				lcalArray = []
				testArray = []

				for i in range(32):
				   
				   #Slice each of the data files into 32 columns of 64 pixels width
				   newObjectArray = data_o[hor1:hor2,x:y]
				   newSkyArray = data_s[hor1:hor2,x:y]
				   newManObjArray = manObjData[hor1:hor2,x:y]
				   newManSkyArray = manSkyData[hor1:hor2,x:y]
				   newPArray = bad_pixel_data[hor1:hor2,x:y]
				   newCalArray = lcal_data[hor1:hor2,x:y]
				   #newTestArray = test_array[:,x:y]
				   #testArray.append(newTestArray)
				   objArray.append(newObjectArray)
				   skyArray.append(newSkyArray)
				   manObjArray.append(newManObjArray)
				   manSkyArray.append(newManSkyArray)
				   badPArray.append(newPArray)
				   lcalArray.append(newCalArray)

				   #Add 64 to the counters each time to create the slices
				   x += 64
				   y += 64

				   #Have sliced each matrix into 2048x64. All that's left to do is slice 
				   #Each of these into 8 chunks of 256x64, 16 chunks of 128x64 and 32 chunks of
				   #64x64. Check the length of the array each time to see if there are enough pixels 
				   #for computing the median
				print objArray[1].shape   
				#Start the loop for all the columns in the Array vectors 
				for num in range(len(objArray)):


					#Now all the pixels that shouldn't be included in the median 
					#have value nan. Can then just do np.nanmean(objTemp) which will ignore nan's
					#then repeat the process for the sky, compare the mean's, compute and apply the offset.

					obj_mean = np.nanmedian(manObjArray[num])
					sky_mean = np.nanmedian(manSkyArray[num])
					#print sky_mean
					#print obj_mean

					#Need to compare the two medians to see how to apply the offset.
					#If the sky is brighter, add the difference to the object image

					if sky_mean > obj_mean:
						objArray[num] += abs(sky_mean - obj_mean)
					elif obj_mean > sky_mean:
						objArray[num] -= abs(obj_mean - sky_mean)	

				#Have now made the correction to all 32 of the 64 pixel width columns.
				#Previously would stack the data here, but the loop goes back to the beginning
				#So need to save to a different object, and then vstack at the end. 
				vStackArray.append(np.hstack(objArray))		
				hor1 += 128
				hor2 += 128	

			#Now just need to vstack all of these arrays and will have a 2048x2048 corrected array
			newObjData = np.vstack(vStackArray)	
			print newObjData.shape

			correctedExtensions.append(newObjData)
			if count == 1:
				print 'Computed First Correction'
			elif count == 2:
				print 'Computed Second Correction'
			elif count == 3: 
				print 'Computed Third Correction'			
			val += 1				
		#Create the object fits file with the three corrected extensions
		fileName = objectFile[0:-5] + '_Corrected'  + '.fits'
		#Note that the readout complained about the header not being 
		#in the correct fits format
		hdu = fits.PrimaryHDU(header=fitsHeader)
		hdu.writeto(fileName, clobber=True)
		fits.append(fileName, data=correctedExtensions[0], header=header_one)	
		fits.append(fileName, data=correctedExtensions[1], header=header_two)	
		fits.append(fileName, data=correctedExtensions[2], header=header_three)




	def subFrames(self, objectFile, skyFile):	

		#Read in the object and sky files 
		objData = fits.open(objectFile)
		skyData = fits.open(skyFile)

		#Find the header and extensions of the new fits file
		header = objData[0].header
		headerOne = objData[1].header
		headerTwo = objData[2].header
		headerThree = objData[3].header



		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')
		print header
		print headerOne
		print headerTwo
		print headerThree
		sys.stdout.close()
		sys.stdout = temp


		ext1 = objData[1].data - skyData[1].data
		ext2 = objData[2].data - skyData[2].data
		ext3 = objData[3].data - skyData[3].data

		#Write out to a different fits file, with name user specified
		nameOfFile = objectFile[:-5] + '_Subtracted.fits'  
		hdu = fits.PrimaryHDU(header=header)
		hdu.writeto(nameOfFile, clobber=True)
		fits.append(nameOfFile, data=ext1, header=headerOne)	
		fits.append(nameOfFile, data=ext2, header=headerTwo)	
		fits.append(nameOfFile, data=ext3, header=headerThree)

		os.system('rm log.txt')

	def pixelHistogram(self, subFile, subCorFile, x1, x2):

		#Create a histogram of pixel values on the 
		#subtracted frame before and after correction	

		#First read in the files 
		subData = fits.open(subFile)
		subCorData = fits.open(subCorFile)

		#At the moment we'll just consider the first extension for our data
		subData = subData[1].data
		subCorData = subCorData[1].data

		#The input numbers define the left and right edges of the pixel section
		subData = subData[:,x1:x2]
		subCorData = subCorData[:,x1:x2]
		
		#This gives an array of arrays, we just care about the numbers, not spatial info
		#Use ravel() to convert these into lists for the histogram
		subData = subData.ravel()
		subCorData = subCorData.ravel()	
		print len(subData)
		print subData
		print np.median(subData)
		print np.median(subCorData)

		#Create the bins array for both histograms 
		bins = np.arange(-15, 15, 1)

		plt.close('all')
		fig, ax = plt.subplots(1, 1, figsize=(12,12))
		#Plot the histograms 
		n1, bins1, patches1 = ax.hist(subData, bins=bins, histtype='step',\
		 color='green', linewidth=3, label='Before Correction') 

		n2, bins2, patches2 = ax.hist(subCorData, bins=bins, histtype='step',\
		 color='blue', linewidth=3, alpha=0.5, label='After Correction')

		#Now want to fit the gaussians to the histograms using lmfit gaussian models 
		#gaussian model number 1
		mod1 = GaussianModel()

		#Create a new bins vector for the fit 
		fitBins = np.arange(-14.5, 14.5, 1)
		print len(fitBins)

		#Take an initial guess at what the model parameters are 
		#In this case the gaussian model has three parameters, 
		#Which are amplitude, center and sigma
		pars1 = mod1.guess(n1, x=fitBins)
		#Perform the actual fit 
		out1  = mod1.fit(n1, pars1, x=fitBins)
		#Now want to add this curve to our plot 
		ax.plot(fitBins, out1.best_fit, linewidth=2.0, label='b.c. model', color='green')

		#Repeat for the corrected data
		mod2 = GaussianModel()
		pars2 = mod2.guess(n2, x=fitBins)
		out2  = mod2.fit(n2, pars2, x=fitBins)
		ax.plot(fitBins, out2.best_fit, linewidth=2.0, label='a.c. model', color='blue')
		ax.set_xlabel('Counts per pixel', fontsize=24)
		ax.set_ylabel('Number per bin', fontsize=24)
		ax.set_title('Sub Frame, Improvement After Correction', fontsize=28)
		ax.tick_params(axis='both', which='major', labelsize=15)
		ax.legend(loc='upper left', fontsize=10)
		fig.savefig(raw_input('Enter the plot name: '))
		plt.show()
		plt.close('all')

		#Print out the fit reports to look at the centre at S.D. of each model
		print out1.fit_report() 
		print out2.fit_report() 

	#def topFourCorrection()	
	def stackLcal(self, lcalFile):
		#Read in the file 
		lcal = fits.open(lcalFile)
		#Want to have it so that only nan values survive. Can do this 
		#By substituting 'False' values for the numbers and true values for nan
		#So that when multiplied only True * True all the way will survive 

		#Loop round all the extensions and change the np.nan values to True 
		#and the pixels with values to False 
		d = {}
		for i in range(1, 19):
			data = lcal[i].data 

			#Define the coordinates where there is a value 
			value_coords = np.where(data > 0)

			#Define where there is np.nan
			nan_coords = np.where(np.isnan(data))
			temp = np.empty(shape=[2048,2048], dtype=float)
			#loop over the pixel values in data and change to either True or False 
			for j in range(len(value_coords[0])):
				xcoord = value_coords[0][j]
				ycoord = value_coords[1][j]
				temp[xcoord][ycoord] = 0

			for j in range(len(nan_coords[0])):
				xcoord = nan_coords[0][j]
				ycoord = nan_coords[1][j]
				temp[xcoord][ycoord] = 1

			d[i] = temp
			print 'done' + str(i)
		print len(d)
		#Check to see if we're creating the right array, which we are	
		#fits.writeto(filename='test.fits', data=d[1], clobber=True)	

		#Now create the individual lcal frames by stacking the extensions together
		#This gives a more conservative estimate of the pixels we should and should 
		#not be using throughout the data analysis stage 

		lcal1 = d[1] * d[4] * d[7] * d[10] * d[13] * d[16]
		lcal2 = d[2] * d[5] * d[8] * d[11] * d[14] * d[17]
		lcal3 = d[3] * d[6] * d[9] * d[12] * d[15] * d[18]

		fits.writeto(filename='lcal1.fits', data=lcal1, clobber=True)
		fits.writeto(filename='lcal2.fits', data=lcal2, clobber=True)
		fits.writeto(filename='lcal3.fits', data=lcal3, clobber=True)

	def applyCorrection(self, fileList, badPMap, lcalMap):
		#Read in the data from the fileList
		data = np.genfromtxt(fileList, dtype='str')
		#Save the names and types as lists 
		names = data[0:,0]
		types = data[0:,1]
		#Loop round all names and apply the computeOffsetSegments method
		for i in range(1, len(names)):
			if types[i] == 'O':
				objFile = names[i]
				if i == 1:
					skyFile = names[i + 1]
				elif types[i - 1] == 'S':
					skyFile = names[i - 1]
				else:
					skyFile = names[i + 1]
				#Now use the method defined within this class 
				self.computeOffsetSegments(objFile, skyFile, badPMap, lcalMap)
				#Which will loop through all and save the corrected object file 
				#as objectFile_Corrected.fits. These are then fed through the pipeline. 	

	def rowMedian(self, subFile, y1, y2, x1, x2):

		#Create a histogram of pixel values on the 
		#subtracted frame before and after correction	

		#First read in the files 
		subData = fits.open(subFile)

		#At the moment we'll just consider the first extension for our data
		subData = subData[1].data

		#The input numbers define the left and right edges of the pixel section
		subData = subData[y1:y2,x1:x2]

		#What we have now is an array of 500 entries, each of which contains 500 entries.
		#These are the columns of the fits file, the pixel count values
		#Now need to compute the median of each and store in an array and return

		medArray = np.median(subData, axis=0)
		return medArray

		

	def plotMedian(self, rawSubFile, subFile, segmentsSubFile, top4SubFile, y1, y2, x1, x2):
		"""	
		Def: 
		Plots the median values of different subtracted frames 
		together on the same axes. Gives an indication for which 
		column correction method produces the smoothest results

		"""
		#Find the medians of the different subtracted frames and plot 
		rawMed = self.rowMedian(rawSubFile, y1, y2, x1, x2)		
		normMed = self.rowMedian(subFile, y1, y2, x1, x2)
		segmentsMed = self.rowMedian(segmentsSubFile, y1, y2, x1, x2)
		top4Med = self.rowMedian(top4SubFile, y1, y2, x1, x2)

		#print type(rawMed)
		#print len(normMed)
		#print segmentsMed
		#print top4Med

		#Generate the x-axis vector, which is just the number of pixels 
		xVec = range(len(rawMed))
		xVec = np.array(xVec)

		#Plot everything against this xVec in turn 
		fig, ax = plt.subplots(1,1, figsize=(18, 10))

		ax.plot(xVec, segmentsMed, label='Segments')
		ax.plot(xVec, normMed, label='Cor')
		ax.plot(xVec, top4Med, label='top') 
		ax.plot(xVec, rawMed, label='raw')
		ax.set_xlabel('Pixel Number', fontsize=24)
		ax.set_ylabel('Median', fontsize=24)
		ax.set_title('Pixel Medians for Different Methods', fontsize=30)
		ax.tick_params(axis='both', which='major', labelsize=15)
		ax.legend(loc='upper left', fontsize=10)
		fig.savefig(raw_input('Enter the File name: ') + '.png')
		plt.show()
		plt.close('all')

	def badPixelextend(self, badpmap): 
		"""
		Def: 
		Take an arbitary bad pixel mask and mask off the pixels in a ring around 
		the original bad pixel location. Good for determining the pixel shift location
		as by inspection many of the pixels around the bad pixel locations are also 
		pretty rubbish 

		Inputs: 
		badpmap - any bad pixel mask with 0 as bad and 1 as good 

		Outputs: 
		newbadpmap - remasked bad pixel. 

		"""
		#Read in data 
		badpTable = fits.open(badpmap)
		extArray = []

		primHeader = badpTable[0].header
		ext1Header = badpTable[1].header
		ext2Header = badpTable[2].header
		ext3Header = badpTable[3].header

		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')		
		print primHeader
		print ext1Header
		print ext2Header
		print ext3Header
		sys.stdout.close()
		sys.stdout = temp

		#Loop around the extensions and make ammendments  
		for i in range(1,4):
			badpData = badpTable[i].data

			#Now have the 2048x2048 data array. Want to find all the 0 values 
			#And make the points surrounding this zero also 
			badpCoords = np.where(badpData == 0)

			#Loop around the bad pixel locations and mask off 
			for i in range(len(badpCoords[0])):
				#Because of the way np.where works, need to define the x and y coords in this way

				if (badpCoords[0][i] < 2047) and (badpCoords[1][i] < 2047):
					#print badpCoords[0][i], badpCoords[1][i]
					xcoord = badpCoords[0][i]
					ycoord = badpCoords[1][i]
					#Now set all positions where there is a dead pixel to np.nan in the object and sky
					badpData[xcoord][ycoord + 1] = 0
					badpData[xcoord][ycoord - 1] = 0
					badpData[xcoord + 1][ycoord] = 0								
					badpData[xcoord - 1][ycoord] = 0
					badpData[xcoord + 1][ycoord + 1] = 0
					badpData[xcoord - 1][ycoord + 1] = 0
					badpData[xcoord + 1][ycoord - 1] = 0
					badpData[xcoord - 1][ycoord - 1] = 0


			extArray.append(badpData)

		#Now write out to a new fits file 
		badpName = badpmap[:-5] + '_Added.fits'	
		hdu = fits.PrimaryHDU(header=primHeader)
		hdu.writeto(badpName, clobber=True)
		fits.append(badpName, data=extArray[0], header=ext1Header)	
		fits.append(badpName, data=extArray[1], header=ext2Header)	
		fits.append(badpName, data=extArray[2], header=ext3Header)

		os.system('rm log.txt')

	def extensionMedians(self, subbedFile):
		"""
		Def:
		Take the masked subtracted file and compute the median and standard deviation of each extension 
		then print out these values to the terminal

		"""	

		dataTable = fits.open(subbedFile)
		for i in range(1, 4):
			print i
			data = dataTable[i].data
			med = np.nanmedian(data)
			st = np.nanstd(data)
			print 'The median of extension %s is: %s \n The standard Deviation of extension %s is: %s' % (i, med, i, st)



	def maskFile(self, inFile, badpFile):
		"""	
		Def:
		Take the badPixelMap and apply it to any inFile 
		to mask off the bad pixels. Particularly useful 
		for applying to skyFiles before subtraction

		"""	

		dataTable = fits.open(inFile)
		badPTable = fits.open(badpFile)
		extArray = []

		primHeader = dataTable[0].header
		ext1Header = dataTable[1].header
		ext2Header = dataTable[2].header
		ext3Header = dataTable[3].header


		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')
		print primHeader
		print ext1Header
		print ext2Header
		print ext3Header
		sys.stdout.close()
		sys.stdout = temp

		for i in range(1, 4):

			data = dataTable[i].data
			badpData = badPTable[i].data

			#Now find the bad pixel locations and mask off the data appropriately

			bad_pixel_coords = np.where(badpData == 0)

			#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
			for i in range(len(bad_pixel_coords[0])):
				#Because of the way np.where works, need to define the x and y coords in this way
				xcoord = bad_pixel_coords[0][i]
				ycoord = bad_pixel_coords[1][i]
				#Now set all positions where there is a dead pixel to np.nan in the object and sky
				data[xcoord][ycoord] = np.nan

			extArray.append(data)
		#Write out the new data
		fileName = inFile[:-5] + '_masked.fits'	
		hdu = fits.PrimaryHDU(header=primHeader)
		hdu.writeto(fileName, clobber=True)
		fits.append(fileName, data=extArray[0], header=ext1Header)	
		fits.append(fileName, data=extArray[1], header=ext2Header)	
		fits.append(fileName, data=extArray[2], header=ext3Header)	
		os.system('rm log.txt')

	def maskFilelcal(self, inFile, lcalFile):
		"""	
		Def:
		Take the badPixelMap and apply it to any inFile 
		to mask off the bad pixels. Particularly useful 
		for applying to skyFiles before subtraction

		"""	

		dataTable = fits.open(inFile)
		badPTable = fits.open(lcalFile)
		extArray = []

		primHeader = dataTable[0].header
		ext1Header = dataTable[1].header
		ext2Header = dataTable[2].header
		ext3Header = dataTable[3].header


		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')
		print primHeader
		print ext1Header
		print ext2Header
		print ext3Header
		sys.stdout.close()
		sys.stdout = temp

		for i in range(1, 4):

			data = dataTable[i].data
			badpData = badPTable[i].data

			#Now find the bad pixel locations and mask off the data appropriately

			bad_pixel_coords = np.where(np.isnan(badpData))

			#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
			for i in range(len(bad_pixel_coords[0])):
				#Because of the way np.where works, need to define the x and y coords in this way
				xcoord = bad_pixel_coords[0][i]
				ycoord = bad_pixel_coords[1][i]
				#Now set all positions where there is a dead pixel to np.nan in the object and sky
				data[xcoord][ycoord] = np.nan

			extArray.append(data)
		#Write out the new data
		fileName = inFile[:-5] + '_masked_lcal.fits'	
		hdu = fits.PrimaryHDU(header=primHeader)
		hdu.writeto(fileName, clobber=True)
		fits.append(fileName, data=extArray[0], header=ext1Header)	
		fits.append(fileName, data=extArray[1], header=ext2Header)	
		fits.append(fileName, data=extArray[2], header=ext3Header)	
		os.system('rm log.txt')


								
	def crossCorr(self, ext, objFile, skyFile, y1, y2, x1, x2):
		"""
		Def: 
		Compute the cross-correlation coefficient for a given object 
		and skyfile. Define the pixel range over which to compute the 
		coefficient. 

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		objFile - Input object file to compute correlation 
		skyFile - sky image to compare objFile with
		y1, y2, x1, x2 - the range to compute rho over 		
		"""
		#Trying to compute the similarity between a square grid of pixels 
		#from the object file and from the skyfile. Do this using the correlation coeff. 

		#First read in the full 2048x2048 data arrays from the object and sky files 
		#Will first consider just the first detector and can expand on this later 
		objData = fits.open(objFile)
  		objData = objData[ext].data

  		skyData = fits.open(skyFile)
  		skyData = skyData[ext].data

  		objData = np.array(objData[y1:y2,x1:x2])
  		skyData = np.array(skyData[y1:y2,x1:x2])

  		###############FIRST MASK OFF THE PIXELS WITH 0 VALUE FROM BOTH OBJECT AND SKY###########
  		objDataMedian = np.nanmedian(objData)
  		skyDataMedian = np.nanmedian(skyData)	
		skyDataStd = np.nanstd(skyData)
  		objDataStd = np.nanstd(objData)

  		###############MASKING THE HIGH SIGMA PIXELS FOR BETTER RHO##########################
  		#Let's try masking the pixels which are bigger than 1000 counts and less than 50

 		#NOTE FOR FUTURE - THIS IS NOT A TOTALLY SECURE WAY OF COMPUTING THE CROSS CORR 
 		#I'VE HAD VERY SUCCESSFUL RUNS OF THE ALGORITHM HARD-CODING IN THESE LIMITS 
 		#BUT FOUND THAT THE PERCENTILES GIVES ROUGHLY THE SAME NUMBERS, 
 		#BUT THESE CHANGE FROM DETECTOR TO DETECTOR.
 		

		objData[objData < 250] = np.nan
		objData[objData > 1500] = np.nan
		skyData[skyData < 250] = np.nan
		skyData[skyData > 1500] = np.nan		
		newobjDataMedian = np.nanmedian(objData)				
		newskyDataMedian = np.nanmedian(skyData)			

  		firstPart = np.sqrt(np.nansum((objData - newobjDataMedian)**2))
  		secondPart = np.sqrt(np.nansum((skyData - newskyDataMedian)**2))
  		denom = firstPart * secondPart
  		#print denom
  		numer = np.nansum((objData - newobjDataMedian)*(skyData - newskyDataMedian))
  		rho = numer / denom
  		#print rho
  		return rho

	def crossCorrZeroth(self, objFile, skyFile, y1, y2, x1, x2):
		"""
		Def: 
		Compute the cross-correlation coefficient for a given object 
		and skyfile. Define the pixel range over which to compute the 
		coefficient. Zeroth because the zeroth and first extensions 
		only are used.

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		objFile - Input object file to compute correlation 
		skyFile - sky image to compare objFile with
		y1, y2, x1, x2 - the range to compute rho over 		
		"""
		#Trying to compute the similarity between a square grid of pixels 
		#from the object file and from the skyfile. Do this using the correlation coeff. 

		#First read in the full 2048x2048 data arrays from the object and sky files 
		#Will first consider just the first detector and can expand on this later 
		objData = fits.open(objFile)
  		objData = objData[0].data

  		skyData = fits.open(skyFile)
  		skyData = skyData[1].data

  		objData = np.array(objData[y1:y2,x1:x2])
  		skyData = np.array(skyData[y1:y2,x1:x2])

  		###############FIRST MASK OFF THE PIXELS WITH 0 VALUE FROM BOTH OBJECT AND SKY###########
  		objDataMedian = np.nanmedian(objData)
  		skyDataMedian = np.nanmedian(skyData)	
		skyDataStd = np.nanstd(skyData)
  		objDataStd = np.nanstd(objData)

  		###############MASKING THE HIGH SIGMA PIXELS FOR BETTER RHO##########################
  		#Let's try masking the pixels which are bigger than 1000 counts and less than 50

 		#NOTE FOR FUTURE - THIS IS NOT A TOTALLY SECURE WAY OF COMPUTING THE CROSS CORR 
 		#I'VE HAD VERY SUCCESSFUL RUNS OF THE ALGORITHM HARD-CODING IN THESE LIMITS 
 		#BUT FOUND THAT THE PERCENTILES GIVES ROUGHLY THE SAME NUMBERS, 
 		#BUT THESE CHANGE FROM DETECTOR TO DETECTOR.
 		
		objData[objData < 250] = np.nan
		objData[objData > 1500] = np.nan
		skyData[skyData < 250] = np.nan
		skyData[skyData > 1500] = np.nan		
		newobjDataMedian = np.nanmedian(objData)				
		newskyDataMedian = np.nanmedian(skyData)		

  		firstPart = np.sqrt(np.nansum((objData - newobjDataMedian)**2))
  		secondPart = np.sqrt(np.nansum((skyData - newskyDataMedian)**2))
  		denom = firstPart * secondPart
  		#print denom
  		numer = np.nansum((objData - newobjDataMedian)*(skyData - newskyDataMedian))
  		rho = numer / denom
  		#print rho
  		return rho

	def crossCorrFirst(self, objFile, skyFile, y1, y2, x1, x2):
		"""
		Def: 
		Compute the cross-correlation coefficient for a given object 
		and skyfile. Define the pixel range over which to compute the 
		coefficient. First because only the first extension is used

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		objFile - Input object file to compute correlation 
		skyFile - sky image to compare objFile with
		y1, y2, x1, x2 - the range to compute rho over 		
		"""
		#Trying to compute the similarity between a square grid of pixels 
		#from the object file and from the skyfile. Do this using the correlation coeff. 

		#First read in the full 2048x2048 data arrays from the object and sky files 
		#Will first consider just the first detector and can expand on this later 
		objData = fits.open(objFile)
  		objData = objData[1].data

  		skyData = fits.open(skyFile)
  		skyData = skyData[1].data

  		objData = np.array(objData[y1:y2,x1:x2])
  		skyData = np.array(skyData[y1:y2,x1:x2])

  		###############FIRST MASK OFF THE PIXELS WITH 0 VALUE FROM BOTH OBJECT AND SKY###########
  		objDataMedian = np.nanmedian(objData)
  		skyDataMedian = np.nanmedian(skyData)	
		skyDataStd = np.nanstd(skyData)
  		objDataStd = np.nanstd(objData)

  		###############MASKING THE HIGH SIGMA PIXELS FOR BETTER RHO##########################
  		#Let's try masking the pixels which are bigger than 1000 counts and less than 50



 		x1 = np.percentile(objData, 94)
 		x2 = np.percentile(objData, 98)  		 
 		y1 = np.percentile(skyData, 94)
 		y2 = np.percentile(skyData, 98)

 		print x1, x2, y1, y2  		
#
 # 		print len(g)


 		#NOTE FOR FUTURE - THIS IS NOT A TOTALLY SECURE WAY OF COMPUTING THE CROSS CORR 
 		#I'VE HAD VERY SUCCESSFUL RUNS OF THE ALGORITHM HARD-CODING IN THESE LIMITS 
 		#BUT FOUND THAT THE PERCENTILES GIVES ROUGHLY THE SAME NUMBERS, 
 		#BUT THESE CHANGE FROM DETECTOR TO DETECTOR.

		objData[objData < 250] = np.nan
		objData[objData > 1500] = np.nan
		skyData[skyData < 250] = np.nan
		skyData[skyData > 1500] = np.nan	

		newobjDataMedian = np.nanmedian(objData)				
		newskyDataMedian = np.nanmedian(skyData)		

  		firstPart = np.sqrt(np.nansum((objData - newobjDataMedian)**2))
  		secondPart = np.sqrt(np.nansum((skyData - newskyDataMedian)**2))
  		denom = firstPart * secondPart
  		#print denom
  		numer = np.nansum((objData - newobjDataMedian)*(skyData - newskyDataMedian))
  		rho = numer / denom
  		#print rho
  		return rho

	def crossCorrOne(self, ext, objFile, skyFile, y1, y2, x1, x2):
		"""
		Def: 
		Compute the cross-correlation coefficient for a given object 
		and skyfile. Define the pixel range over which to compute the 
		coefficient. 

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		objFile - Input object file to compute correlation 
		skyFile - sky image to compare objFile with
		y1, y2, x1, x2 - the range to compute rho over 

		"""
		#Trying to compute the similarity between a square grid of pixels 
		#from the object file and from the skyfile. Do this using the correlation coeff. 

		#First read in the full 2048x2048 data arrays from the object and sky files 
		#Will first consider just the first detector and can expand on this later 
		objData = fits.open(objFile)
  		objData = objData[0].data

  		skyData = fits.open(skyFile)
  		skyData = skyData[ext].data

  		objData = np.array(objData[y1:y2,x1:x2])
  		skyData = np.array(skyData[y1:y2,x1:x2])

  		###############FIRST MASK OFF THE PIXELS WITH 0 VALUE FROM BOTH OBJECT AND SKY###########
  		objDataMedian = np.nanmedian(objData)
  		skyDataMedian = np.nanmedian(skyData)	
		skyDataStd = np.nanstd(skyData)
  		objDataStd = np.nanstd(objData)

  		###############MASKING THE HIGH SIGMA PIXELS FOR BETTER RHO##########################
  		#Let's try masking the pixels which are bigger than 1000 counts and less than 50

 		#NOTE FOR FUTURE - THIS IS NOT A TOTALLY SECURE WAY OF COMPUTING THE CROSS CORR 
 		#I'VE HAD VERY SUCCESSFUL RUNS OF THE ALGORITHM HARD-CODING IN THESE LIMITS 
 		#BUT FOUND THAT THE PERCENTILES GIVES ROUGHLY THE SAME NUMBERS, 
 		#BUT THESE CHANGE FROM DETECTOR TO DETECTOR.
 		
		objData[objData < 250] = np.nan
		objData[objData > 1500] = np.nan
		skyData[skyData < 250] = np.nan
		skyData[skyData > 1500] = np.nan		
		newobjDataMedian = np.nanmedian(objData)				
		newskyDataMedian = np.nanmedian(skyData)	

  		firstPart = np.sqrt(np.nansum((objData - newobjDataMedian)**2))
  		secondPart = np.sqrt(np.nansum((skyData - newskyDataMedian)**2))
  		denom = firstPart * secondPart
  		#print denom
  		numer = np.nansum((objData - newobjDataMedian)*(skyData - newskyDataMedian))
  		rho = numer / denom
  		print rho
  		return rho		

  	def shiftImage(self, ext, infile, skyfile, badpmap, interp_type, stepsize, xmin, xmax, ymin, ymax):

  		"""
  		Def:
  		Compute the correlation coefficient for a grid of pixel shift values and 
  		decide which one is best (if better than the original) and apply this to 
  		the object image to align with the sky. First because we use only the 
  		first extension because of the way the shiftImageSegments function is 
  		defined. This function now applies the bad pixel map both before cross 
  		correlating and before interpolation - need to ignore bad pixels.

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		infile - Input object file to shift 
		skyFile - sky image to compare objFile with
		interp_type - type of interpolation function for the shift. 

			-'nearest': nearest neighbour
			-'linear':bilinear x,y, interpolation
			-'poly3':third order interior polynomial
			-'poly5':fifth order interior polynomial
			-'spline3':third order spline3

		stepsize - value to increment grid by each time, increasing 
		this increases the time taken for the computation 
		xmin, xmax, ymin, ymax - Grid extremes for brute force shift 

		"""

  		#We first want to apply the bad pixel map 
  		objTable = fits.open(infile)
  		objData = objTable[ext].data
  		skyTable = fits.open(skyfile)
  		skyData = skyTable[ext].data
  		badpTable = fits.open(badpmap)
  		badpData = badpTable[ext].data

		#Find the headers of the primary HDU and chosen extension 
		objPrimHeader = objTable[0].header
		objExtHeader = objTable[ext].header
		skyPrimHeader = skyTable[0].header
		skyExtHeader = skyTable[ext].header		
		badpPrimHeader = badpTable[0].header
		badpExtHeader = badpTable[ext].header

		print (objPrimHeader)
		print (objExtHeader)
		print (skyPrimHeader)
		print (skyExtHeader)		
		print (badpPrimHeader)
		print (badpExtHeader) 

		#Find the coordinates of the bad pixels and the slitlets 
		bad_pixel_coords = np.where(badpData == 0)

		#Loop around the bad pixel locations and mask off on the manObjData and manSkyData
		for i in range(len(bad_pixel_coords[0])):
			#Because of the way np.where works, need to define the x and y coords in this way
			xcoord = bad_pixel_coords[0][i]
			ycoord = bad_pixel_coords[1][i]
			#Now set all positions where there is a dead pixel to np.nan in the object and sky
			objData[xcoord][ycoord] = np.nan
			skyData[xcoord][ycoord] = np.nan
		#fits.writeto('pom.fits', data=objData, clobber=True)	

		#Define the minimum and maximum ranges for the correlation 
		xMinCorr = (1 * len(objData[0]))/4
		xMaxCorr = (3 * len(objData[0]))/4
		yMinCorr = (1 * len(objData))/4
		yMaxCorr = (3 * len(objData))/4


		#Write out to new temporary fits files - annoyingly need to have 
		#the data in fits files to be able to use pyraf functions
		#######OBJECT##########
		objhdu = fits.PrimaryHDU(header=objPrimHeader)
		objhdu.writeto('maskedObj.fits', clobber=True)
		fits.append('maskedObj.fits', data=objData, header=objExtHeader)

		#######Sky##########
		skyhdu = fits.PrimaryHDU(header=skyPrimHeader)
		skyhdu.writeto('maskedSky.fits', clobber=True)
		fits.append('maskedSky.fits', data=skyData, header=skyExtHeader)		

  		#First compute the correlation coefficient with just the newly saved fits file 
  		rhoArray = []

  		rhoArray.append(self.crossCorrFirst('maskedObj.fits', 'maskedSky.fits', yMinCorr, yMaxCorr, xMinCorr, xMaxCorr))

  		print rhoArray

  		#Working. Now create grid of fractional shift values. 
  		xArray = np.arange(xmin, xmax, stepsize)
  		xArray = np.around(xArray, decimals = 4)
  		yArray = np.arange(ymin, ymax, stepsize)
  		yArray = np.around(yArray, decimals = 4)

  		#Set up mesh grid of rho values for contour plot 
  		rhoGrid = np.zeros(shape=(len(xArray), len(yArray)))

  		#Before attempting the interpolation, we want to mask the bad pixel values, 
  		#and save to a fresh temporary fits file. matty was here :) 

  		#Loop over all values in the grid, shift the image by this 
  		#amount each time and compute the correlation coefficient
  		successDict = {}
  		for i in range(len(xArray)):
  			for j in range(len(yArray)):
  				#Perform the shift 
  				infileName = 'maskedObj.fits[1]'
  				pyraf.iraf.imshift(input=infileName, output='temp_shift.fits', \
  					xshift=xArray[i], yshift=yArray[j], interp_type=interp_type)
  				#re-open the shifted file and compute rho

  				rho = self.crossCorrZeroth('temp_shift.fits', 'maskedSky.fits',\
  				  yMinCorr, yMaxCorr, xMinCorr, xMaxCorr)
  				rhoGrid[i][j] = rho	
  				#If the correlation coefficient improves, append to new array
  				if rho > rhoArray[0]:
  					print 'SUCCESS, made improvement!'
  					entryName = str(xArray[i]) + ' and ' + str(yArray[j])
  					entryValue = [round(xArray[i], 3), round(yArray[j], 3)]
  					successDict[str(round(rho, 4))] = entryValue
  				rhoArray.append(rho)
  				#Clean up by deleting the created temporary fits file
  				os.system('rm temp_shift.fits')
  				#Go back through loop, append next value of rho
  				print 'Finished shift: %s %s, rho = %s ' % (xArray[i], yArray[j], rho)
  				#sys.stdout.flush()
  		os.system('rm maskedObj.fits')
  		os.system('rm maskedSky.fits')
  		#print round(max(rhoArray), 4)
  		#print rhoArray[0]		
  		#print successDict
  		#Now we want to choose the best shift value and actually apply this 
  		#Need to find the x and y shift values which correspond to the maximum rho
  		#Only do this if the success dictionary is not empty, if it is empty return 0.0,0.0

  		print rhoGrid

		plt.contour(xArray, yArray, rhoGrid, levels = [(np.max(rhoGrid) - (0.25 * np.std(rhoGrid))), \
		 (np.max(rhoGrid) - (1*np.std(rhoGrid))), (np.max(rhoGrid) - (1.5 * np.std(rhoGrid)))])
		plt.xlabel('$\Delta x$')
		plt.ylabel('$\Delta y$')
		plt.title('Correlation Coefficient Grid')
		plt.savefig('Correlation_coefficient.png')
		plt.close('all')

		print 'Made plot Successfully'

  		if successDict: 
  			print 'Finding Best Shift Value...'
  			rhoMax = str(round(max(rhoArray), 4))
  			#print rhoMax
  			shiftVector = successDict[rhoMax]
  			return shiftVector
  		
  		else:
  			print 'No Shift Value Found'
  			shiftVector = [0.0, 0.0]
  			return shiftVector

  		#We can also save a contour plot of the results to show where the best shift location is 
  		#This only works properly right now for a shift segment of 1, otherwise will 
  		#overwrite the filename each time. Could easily fix this.


  	def shiftImageFirst(self, ext, infile, skyfile, badpmap, interp_type, stepsize, xmin, xmax, ymin, ymax):

  		"""
  		Def:
  		Compute the correlation coefficient for a grid of pixel shift values and 
  		decide which one is best (if better than the original) and apply this to 
  		the object image to align with the sky. First because we use only the 
  		first extension because of the way the shiftImageSegments function is 
  		defined. This function now applies the bad pixel map both before cross 
  		correlating and before interpolation - need to ignore bad pixels.

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		infile - Input object file to shift 
		skyFile - sky image to compare objFile with
		interp_type - type of interpolation function for the shift. 

			-'nearest': nearest neighbour
			-'linear':bilinear x,y, interpolation
			-'poly3':third order interior polynomial
			-'poly5':fifth order interior polynomial
			-'spline3':third order spline3

		stepsize - value to increment grid by each time, increasing 
		this increases the time taken for the computation 
		xmin, xmax, ymin, ymax - Grid extremes for brute force shift 

  		"""
  		#We first want to apply the bad pixel map 
  		objTable = fits.open(infile)
  		objData = objTable[1].data
  		skyTable = fits.open(skyfile)
  		skyData = skyTable[1].data
  		badpTable = fits.open(badpmap)
  		badpData = badpTable[1].data

		#Find the headers of the primary HDU and chosen extension 
		objPrimHeader = objTable[0].header
		objExtHeader = objTable[1].header
		skyPrimHeader = skyTable[0].header
		skyExtHeader = skyTable[1].header		
		badpPrimHeader = badpTable[0].header
		badpExtHeader = badpTable[1].header

		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')
		print (objPrimHeader)
		print (objExtHeader)
		print (skyPrimHeader)
		print (skyExtHeader)
		print (badpPrimHeader)
		print (badpExtHeader)
		sys.stdout.close()
		sys.stdout = temp
		

		badpData[badpData == 0] = np.nan
		objData = objData * badpData
		skyData = skyData * badpData
		#fits.writeto('pom.fits', data=objData, clobber=True)	

		#Define the minimum and maximum ranges for the correlation 
		xMinCorr = (1 * len(objData[0]))/4
		xMaxCorr = (3 * len(objData[0]))/4
		yMinCorr = (1 * len(objData))/4
		yMaxCorr = (3 * len(objData))/4

		#Write out to new temporary fits files - annoyingly need to have 
		#the data in fits files to be able to use pyraf functions
		#######OBJECT##########
		objhdu = fits.PrimaryHDU(header=objPrimHeader)
		objhdu.writeto('maskedObj.fits', clobber=True)
		fits.append('maskedObj.fits', data=objData, header=objExtHeader)

		#######Sky##########
		skyhdu = fits.PrimaryHDU(header=skyPrimHeader)
		skyhdu.writeto('maskedSky.fits', clobber=True)
		fits.append('maskedSky.fits', data=skyData, header=skyExtHeader)		

  		#First compute the correlation coefficient with just the newly saved fits file 
  		rhoArray = []

  		rhoArray.append(self.crossCorrFirst('maskedObj.fits', 'maskedSky.fits', yMinCorr, yMaxCorr, xMinCorr, xMaxCorr))

  		print rhoArray

  		#Working. Now create grid of fractional shift values. 
  		xArray = np.arange(xmin, xmax, stepsize)
  		xArray = np.around(xArray, decimals = 4)
  		yArray = np.arange(ymin, ymax, stepsize)
  		yArray = np.around(yArray, decimals = 4)
  		thArray = np.arange(-0.05, 0.05, 0.01)
  		thArray = np.around(thArray, decimals =4)

  		x, y = np.meshgrid(xArray, yArray)

  		#Set up mesh grid of rho values for contour plot 
  		rhoGrid = np.zeros(shape=(len(x), len(x[0])))

  		#Before attempting the interpolation, we want to mask the bad pixel values, 
  		#and save to a fresh temporary fits file. 

  		#Loop over all values in the grid, shift the image by this 
  		#amount each time and compute the correlation coefficient
  		successDict = {}
  		for i in range(len(xArray)):
  			for j in range(len(yArray)):
  				#Perform the shift 
  				infileName = 'maskedObj.fits[1]'
  				pyraf.iraf.imshift(input=infileName, output='temp_shift.fits', \
  					xshift=xArray[i], yshift=yArray[j], interp_type=interp_type)
  				#re-open the shifted file and compute rho

  				rho = self.crossCorrZeroth('temp_shift.fits', 'maskedSky.fits',\
  				  yMinCorr, yMaxCorr, xMinCorr, xMaxCorr)
  				rhoGrid[j][i] = rho	
  				#If the correlation coefficient improves, append to new array
  				if rho > rhoArray[0]:
  					print 'SUCCESS, made improvement!'
  					entryName = str(xArray[i]) + ' and ' + str(yArray[j])
  					entryValue = [round(xArray[i], 3), round(yArray[j], 3)]
  					successDict[str(round(rho, 4))] = entryValue
  				rhoArray.append(rho)
  				#Clean up by deleting the created temporary fits file
  				os.system('rm temp_shift.fits')
  				#Go back through loop, append next value of rho
  				print 'Finished shift: %s %s, rho = %s ' % (xArray[i], yArray[j], rho)
  				#sys.stdout.flush()
  		os.system('rm maskedObj.fits')
  		os.system('rm maskedSky.fits')
  		#print round(max(rhoArray), 4)
  		#print rhoArray[0]		
  		#print successDict
  		#Now we want to choose the best shift value and actually apply this 
  		#Need to find the x and y shift values which correspond to the maximum rho
  		#Only do this if the success dictionary is not empty, if it is empty return 0.0,0.0

  		#print rhoGrid

		plt.contour(x, y, rhoGrid, levels = [(np.max(rhoGrid) - (0.25 * np.std(rhoGrid))), \
		 (np.max(rhoGrid) - (1*np.std(rhoGrid))), (np.max(rhoGrid) - (1.5 * np.std(rhoGrid)))])
		plt.xlabel('$\Delta x$')
		plt.ylabel('$\Delta y$')
		plt.title('Correlation Coefficient Grid')
		plotName = infile[:-5]  + '_'  + str(ext)  + '_' + interp_type + '_CorrelationGraph.png'
		plt.savefig(plotName)
		plt.close('all')

		print 'Made plot Successfully'

  		if successDict: 
  			print 'Finding Best Shift Value...'
  			rhoMax = str(round(max(rhoArray), 4))
  			#print rhoMax
  			shiftVector = successDict[rhoMax]
  			return shiftVector
  		
  		else:
  			print 'No Shift Value Found'
  			shiftVector = [0.0, 0.0]
  			return shiftVector

  		#We can also save a contour plot of the results to show where the best shift location is 
  		#This only works properly right now for a shift segment of 1, otherwise will 
  		#overwrite the filename each time. Could easily fix this.






  	def rotateImage(self, ext, infile, skyfile, interp_type, minAngle, maxAngle, stepsize):

  		"""
  		Def:
  		Compute the correlation coefficient for a line of rotation angles and 
  		decide which one is best (if better than the original) and apply this to 
  		the object image to align with the sky.

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		infile - Input object file to shift 
		skyFile - sky image to compare objFile with
		interp_type - type of interpolation function for the shift. 

			-'nearest': nearest neighbour
			-'linear':bilinear x,y, interpolation
			-'poly3':third order interior polynomial
			-'poly5':fifth order interior polynomial
			-'spline3':third order spline3

		stepsize - value to increment grid by each time, increasing 
		this increases the time taken for the computation 
		xmin, xmax, ymin, ymax - Grid extremes for brute force shift 

  		"""
  		#First compute the correlation coefficient with just infile 
  		rhoArray = []
  		rhoArray.append(self.crossCorr(ext, infile, skyfile, 1000, 1200, 1000, 1200))
  		print rhoArray

  		#Working. Now create grid of fractional shift values. 
  		rotArray = np.arange(minAngle, maxAngle, stepsize)
  		rotArray = np.around(rotArray, decimals = 4)


  		#Loop over all values in the grid, shift the image by this 
  		#amount each time and compute the correlation coefficient
  		successDict = {}

		for number in rotArray:
			#Perform the shift 
			infileName = infile + '[' + str(ext) + ']'
			pyraf.iraf.rotate(input=infileName, output='temp_rot.fits', \
				rotation=number, interpolant=interp_type)
			#re-open the shifted file and compute rho
			rho = self.crossCorrOne(ext,'temp_rot.fits', skyfile,\
			 1000, 1200, 1000, 1200)
			#If the correlation coefficient improves, append to new array
			if rho > rhoArray[0]:
				print 'SUCCESS, made improvement!'
				entryName = str(number)
				entryValue = [number, rho]
				successDict[entryName] = entryValue
			rhoArray.append(rho)
			#Clean up by deleting the created temporary fits file
			os.system('rm temp_rot.fits')
			#Go back through loop, append next value of rho
			print 'Finished shift: %s, rho = %s ' % (number, rho)
			#sys.stdout.flush()

  		print max(rhoArray)
  		print rhoArray[0]		
  		print successDict 		


  	def imSplit(self, ext, infile, vertSegments, horSegments):

  		"""
  		Def: 
  		Take an input image file and split up into a series of squares 
  		Main purpose is for use in the shiftImageSegments functino

  		Input: 
  		infile - file to be divided
  		vertSegments - number of vertical segments (2048 must be divisible by) 
  		horSegments - number of horizontal segments (2048 must be divisible by)
  		ext - extension number

  		Output: 
  		segmentArray - 1D array containing 2D square array segments

  		"""
  		#Read in the data file at the given extension
  		data = fits.open(infile)
  		data = data[ext].data

  		#Initialise the empty array
  		segmentArray = []

  		#We can't do this if 2048 isn't divisible by the segments 
  		#write an error function to check that this is the case 
  		#And exit if it isn't 
  		if ((2048 % vertSegments != 0) or  (2048 % horSegments != 0)):
  			raise ValueError('Please ensure that 2048 is divisible by your segment choice')

		#Counters for the horizontal slicing
		hor1 = 0
		hor2 = (2048 / horSegments)

		for j in range(horSegments):
			

			#Counters for the vertical slicing
			x = 0
			y = (2048 / vertSegments)

			for i in range(vertSegments):

			   
			   #Slice the data according to user selection
			   segmentArray.append(data[hor1:hor2,x:y])
			   x += (2048 / vertSegments)
			   y += (2048 / vertSegments)

			hor1 += (2048 / horSegments)
			hor2 += (2048 / horSegments)   

		#print segmentArray	
		return segmentArray	






  	def shiftImageSegments(self, ext, infile, skyfile, badpmap,\
  	 vertSegments, horSegments, interp_type, stepsize, xmin, xmax, ymin, ymax):

  		"""
  		Def: 
  		Lots of arguments because of using lots of different functions. 
  		This is taking an object and a sky image, splitting them into a specified 
  		number of segments, performing shifts to each of the segments and then 
  		computing the cross-correlation function to see if we can improve the 
  		alignment at all. Should give better results than a global shift 

  		Inputs: 
  		infile - file to be divided
		skyFile - sky image to compare objFile with
  		vertSegments - number of vertical segments (2048 must be divisible by) 
  		horSegments - number of horizontal segments (2048 must be divisible by)
  		ext - extension number
		interp_type - type of interpolation function for the shift. 

			-'nearest': nearest neighbour
			-'linear':bilinear x,y, interpolation
			-'poly3':third order interior polynomial
			-'poly5':fifth order interior polynomial
			-'spline3':third order spline3

		stepsize - value to increment grid by each time, increasing 
		this increases the time taken for the computation 
		xmin, xmax, ymin, ymax - Grid extremes for brute force shift 

		"""
		########################TO ADD###############################
		#Make sure that 2048 is divisible by the segment numbers 



		#Create arrays of the split files using the imSplit function 
		objArray = self.imSplit(ext, infile, vertSegments, horSegments)
		skyArray = self.imSplit(ext, skyfile, vertSegments, horSegments)
		badpArray = self.imSplit(ext, badpmap, vertSegments, horSegments)

		#Find the headers of the primary HDU and chosen extension 
		objTable = fits.open(infile)
		objPrimHeader = objTable[0].header
		objExtHeader = objTable[ext].header
		skyTable = fits.open(skyfile)
		skyPrimHeader = skyTable[0].header
		skyExtHeader = skyTable[ext].header
		badpTable = fits.open(badpmap)
		badpPrimHeader = badpTable[0].header
		badpExtHeader = badpTable[ext].header

		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')
		print (objPrimHeader)
		print (objExtHeader)
		print (skyPrimHeader)
		print (skyExtHeader)
		print (badpPrimHeader)
		print (badpExtHeader)
		sys.stdout.close()
		sys.stdout = temp


		shiftArray = []
		#Should now have two 1D arrays of 2D arrays of equal size
		for i in range(len(objArray)):

			tempObjName = infile[:-5] + str(vertSegments) + str(horSegments) + str(i) + '_temp.fits'
			#Write out to new temporary fits files - annoyingly need to have 
			#the data in fits files to be able to use pyraf functions
			#######OBJECT##########
			objhdu = fits.PrimaryHDU(header=objPrimHeader)
			objhdu.writeto(tempObjName, clobber=True)
			fits.append(tempObjName, data=objArray[i], header=objExtHeader)

			#######SKY#############
			skyhdu = fits.PrimaryHDU(header=skyPrimHeader)
			skyhdu.writeto('tempSky.fits', clobber=True)
			fits.append('tempSky.fits', data=skyArray[i], header=skyExtHeader)

			#######BADPIXEL#############
			badphdu = fits.PrimaryHDU(header=badpPrimHeader)
			badphdu.writeto('tempbadp.fits', clobber=True)
			fits.append('tempbadp.fits', data=badpArray[i], header=badpExtHeader)

			#NOTES FOR RESUMING - I now have temporary files containing the object 
			#and sky segments to be shifted and cross correlated. The shiftImage function 
			#can be applied to each of these directly within the for loop!! - remember to 
			# a) clean up the fits files after each loop 
			# b) Find a way to look at the cross correlation results for each segment independently
			# probably by returning the cross correlation arrays into a new array  
			# c) find a way to search specifically for shift success and apply to each segment 
			# d) find a way to recombine all segments together after the shift has happened 
			# e) make sure to use the crossCorrFirst function here, otherwise it will break in 
			# certain situations (i.e. when not using extension one	)

			#Now need to apply the shiftImageFirst function, which compares the chosen 
			#extension shifted object and sky files. 
			#Create an array to hold the shift coordinates for each segment. This is defined 
			#Outside the for loop so that I am not initialising it every time.
			

			print 'This is shift: %s' % i
			shiftArray.append(self.shiftImageFirst(ext, tempObjName, 'tempSky.fits', 'tempbadp.fits', \
			 interp_type, stepsize, xmin, xmax, ymin, ymax))



			#Clean up the temporary fits files during each part of the loop 
			os.system('rm %s' % tempObjName)
			os.system('rm tempSky.fits')
			os.system('rm tempbadp.fits')

			#That should work then for each segment in turn. Does work. 
			#vStackArray.append(np.hstack(objArray))		
			#	hor1 += 128
			#	hor2 += 128	

			#Now just need to vstack all of these arrays and will have a 2048x2048 corrected array
			#newObjData = np.vstack(vStackArray)	
		print shiftArray
		#Now the clever part - to actually apply the shifts to the unmasked infile 
		#imshift can be used with a list of infile names, outfile names and shift coordinates
		#If I create these lists I can imshift all at once, read in the data and then recombine
		#First get the x and y vectors for the shift coordinates
		xArray = []
		yArray = []
		for item in shiftArray:
			xArray.append(item[0])
			yArray.append(item[1])
		xArray = np.array(np.around(xArray, 3))
		yArray = np.array(np.around(yArray, 3))	
		#Create the .txt file with the two columns specifying the shift coordinates
		np.savetxt('coords.txt', np.c_[xArray, yArray], fmt=('%5.3f', '%5.3f'))

		#Coordinates list sorted. Now need list of input files and input file names 
		#To do this need to go back to the imSplit method with the unmasked file 
		#Write to a list of temporary fits files, which will become temporary 
		#Output fits files, which will be read back in as data before recombining 
		#Must clean everything up at the end by removing with os.system()
		#Create arrays of the split files using the imSplit function 

		#Need to apply the shifts to a masked object file. Open the bad pixel map 
		#and the object file and mask the pixels and save to temporary file 
		#Find the coordinates of the bad pixels and the slitlets 
		objData = objTable[ext].data
		badpData = badpTable[ext].data
		badpData[badpData == 0] = np.nan
		objData = objData * badpData

		#Write out to new file which will then be read in to split up the data
		objhdu = fits.PrimaryHDU(header=objPrimHeader)
		objhdu.writeto('temp_masked.fits', clobber=True)
		fits.append('temp_masked.fits', data=objData, header=objExtHeader)


		objArray = self.imSplit(1, 'temp_masked.fits', vertSegments, horSegments)
		inFileArray = []
		outFileArray = []
		shiftedDataArray = []
		vstackArray = []
		hstackArray = []


		#Should now have two 1D arrays of 2D arrays of equal size
		for i in range(len(objArray)):

			inFileName = 'tempObjin'+str(i)+'.fits'
			outFileName = 'tempObjout'+str(i)+'.fits'
			inFileArray.append(inFileName)
			outFileArray.append(outFileName)

			#Write out to new temporary fits files - annoyingly need to have 
			#the data in fits files to be able to use pyraf functions
			#######OBJECT##########
			objhdu = fits.PrimaryHDU(header=objPrimHeader)
			objhdu.writeto(inFileName, clobber=True)
			fits.append(inFileName, data=objArray[i], header=objExtHeader)

			inFileName = inFileName + '[1]'

  			#Now apply imshift with all the parameters 
  			pyraf.iraf.imshift(input = inFileName, output=outFileName, \
  				xshift=xArray[i], yshift=yArray[i] ,interp_type=interp_type)

  			#We want a 1D array of 2D arrays again, read the data files back in 
  			data = fits.open(outFileName)
  			data = data[0].data
  			shiftedDataArray.append(data)
  			#Go back to the top of the loop and grab the next file

  		#The final problem is that we have a 1D arrays of 2D arrays that needs 
  		#to be recombined into the original 2048 x 2048 which created it 
  		#Ordering depends on the number of vertical and horizontal segments 
 
  		x = len(shiftedDataArray) / horSegments	
  		a = 0 

  		while x <= len(shiftedDataArray):
  			for i in range(a, x): 
  				print i
  				hstackArray.append(shiftedDataArray[i])

  			vstackArray.append(np.hstack(hstackArray))
  			hstackArray = []
  			x += len(shiftedDataArray) / horSegments
  			a += len(shiftedDataArray) / horSegments
  			

  		#for item in vstackArray:
  			#print item.shape	
  		#Reconstruct by vstacking the final array	
  		reconstructedData = np.vstack(vstackArray)

  		#Clean up by getting rid of uneeded files
		for item in inFileArray:
			os.system('rm %s' % item)
		for item in outFileArray:
			os.system('rm %s' % item)
		os.system('rm coords.txt')	
		os.system('rm temp_masked.fits')
		os.system('rm log.txt')

		return reconstructedData



	def shiftAllExtensions(self, infile, skyfile, badpmap,\
  	 vertSegments, horSegments, interp_type, stepsize, xmin, xmax, ymin, ymax ):

		"""
		Def: Uses the shiftImageSegments method for each extensions and then combines
		all of these together into a single shifted fits file 

		"""

		#Prepare the headers for writing out the fits file
		objTable = fits.open(infile)
		objPrimHeader = objTable[0].header
		objExtHeader1 = objTable[1].header
		objExtHeader2 = objTable[2].header
		objExtHeader3 = objTable[3].header

		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')
		print (objPrimHeader)
		print (objExtHeader1)
		print (objExtHeader2)
		print (objExtHeader3)						
		sys.stdout.close()
		sys.stdout = temp

		#Set up the array 
		reconstructedDataArray = []

		#Use the shifted image segment function 
		for i in range(1, 4):
			print 'Shifting Extension: %s' % i
			reconstructedDataArray.append(self.shiftImageSegments(i, infile, skyfile, badpmap,\
  	 vertSegments, horSegments, interp_type, stepsize, xmin, xmax, ymin, ymax))


		#Name the shifted data file 
  		shiftedName = infile[:-5] + '_' + str(vertSegments) + str(horSegments) + '_' + interp_type + '_Shifted.fits' 
  		print 'Saving %s' % shiftedName

  		objhdu = fits.PrimaryHDU(header=objPrimHeader)
		objhdu.writeto(shiftedName, clobber=True)
		fits.append(shiftedName, data=reconstructedDataArray[0], header=objExtHeader1)
		fits.append(shiftedName, data=reconstructedDataArray[1], header=objExtHeader2)
		fits.append(shiftedName, data=reconstructedDataArray[2], header=objExtHeader3)


	def applyShiftAllExtensions(self, fileList, badpmap,\
  	 vertSegments, horSegments, interp_type, stepsize, xmin, xmax, ymin, ymax):
		#Read in the data from the fileList
		data = np.genfromtxt(fileList, dtype='str')
		#Save the names and types as lists 
		names = data[0:,0]
		types = data[0:,1]
		#Loop round all names and apply the computeOffsetSegments method
		for i in range(1, len(names)):
			if types[i] == 'O':
				objFile = names[i]
				if i == 1:
					skyFile = names[i + 1]
				elif types[i - 1] == 'S':
					skyFile = names[i - 1]
				else:
					skyFile = names[i + 1]

				print 'Shifting file: %s : %s' % (i, objFile)	
				#Now use the method defined within this class 
				self.shiftAllExtensions(objFile, skyFile, badpmap,\
  	 vertSegments, horSegments, interp_type, stepsize, xmin, xmax, ymin, ymax )
				#Which will loop through all and save the corrected object file 
				#as objectFile_Corrected.fits. These are then fed through the pipeline. 

##########################################################################################################################
##########################################################################################################################
#                          ALTERNATE SET OF FUNCTIONS USING MINIMISATION INSTEAD OF GRIDSEARCH                           #
##########################################################################################################################
##########################################################################################################################

  	def shiftImageFirstMin(self, xArray, infile, skyfile, badpmap, interp_type):

  		"""
  		Def:
  		Compute the correlation coefficient for a grid of pixel shift values and 
  		decide which one is best (if better than the original) and apply this to 
  		the object image to align with the sky. First because we use only the 
  		first extension because of the way the shiftImageSegments function is 
  		defined. This function now applies the bad pixel map both before cross 
  		correlating and before interpolation - need to ignore bad pixels.

		Inputs: 
		ext - detector extension, must be either 1, 2, 3
		infile - Input object file to shift 
		skyFile - sky image to compare objFile with
		interp_type - type of interpolation function for the shift. 

			-'nearest': nearest neighbour
			-'linear':bilinear x,y, interpolation
			-'poly3':third order interior polynomial
			-'poly5':fifth order interior polynomial
			-'spline3':third order spline3

		stepsize - value to increment grid by each time, increasing 
		this increases the time taken for the computation 
		xmin, xmax, ymin, ymax - Grid extremes for brute force shift 

  		"""
  		
  		#We first want to apply the bad pixel map 
  		objTable = fits.open(infile)
  		objData = objTable[1].data
  		skyTable = fits.open(skyfile)
  		skyData = skyTable[1].data
  		badpTable = fits.open(badpmap)
  		badpData = badpTable[1].data

		#Find the headers of the primary HDU and chosen extension 
		objPrimHeader = objTable[0].header
		objExtHeader = objTable[1].header
		skyPrimHeader = skyTable[0].header
		skyExtHeader = skyTable[1].header		
		badpPrimHeader = badpTable[0].header
		badpExtHeader = badpTable[1].header

		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')
		print (objPrimHeader)
		print (objExtHeader)
		print (skyPrimHeader)
		print (skyExtHeader)
		print (badpPrimHeader)
		print (badpExtHeader)
		sys.stdout.close()
		sys.stdout = temp
		

		#Find the coordinates of the bad pixels and the slitlets 
		#Instead of looping, do much faster multiplication
		#Want to avoid loops at all costs
		badpData[badpData == 0] = np.nan
		objData = objData * badpData
		skyData = skyData * badpData

		#Define the minimum and maximum ranges for the correlation 
		xMinCorr = (1 * len(objData[0]))*0.0625
		xMaxCorr = (15 * len(objData[0]))*0.0625
		yMinCorr = (1 * len(objData))*0.0625
		yMaxCorr = (15 * len(objData))*0.0625

		#Write out to new temporary fits files - annoyingly need to have 
		#the data in fits files to be able to use pyraf functions
		#######OBJECT##########
		objhdu = fits.PrimaryHDU(header=objPrimHeader)
		objhdu.writeto('maskedObj.fits', clobber=True)
		fits.append('maskedObj.fits', data=objData, header=objExtHeader)

		#######Sky##########
		skyhdu = fits.PrimaryHDU(header=skyPrimHeader)
		skyhdu.writeto('maskedSky.fits', clobber=True)
		fits.append('maskedSky.fits', data=skyData, header=skyExtHeader)		


  		#Before attempting the interpolation, we want to mask the bad pixel values, 
  		#and save to a fresh temporary fits file. 

  		#Loop over all values in the grid, shift the image by this 

  		infileName = 'maskedObj.fits[1]'

  		print 'Shifting: %s %s' % (xArray[0], xArray[1])

  		pyraf.iraf.imshift(input=infileName, output='temp_shift.fits', \
  			xshift=xArray[0], yshift=xArray[1], interp_type=interp_type)
  				#re-open the shifted file and compute rho

		rho = self.crossCorrZeroth('temp_shift.fits', 'maskedSky.fits',\
		  yMinCorr, yMaxCorr, xMinCorr, xMaxCorr)
		#Tidy up
		os.system('rm temp_shift.fits')
  		os.system('rm maskedObj.fits')
  		os.system('rm maskedSky.fits')

  		#Return the correlation coefficient
  		print (1.0 / rho)
  		return (1.0 / rho)
  		


  	def minimiseRho(self, infile, skyfile, badpmap, interp_type):
  		#print 'Hello'
  		#Minimise the function recipShift with respect to x and y
  		#Define the shift starting points
  		x0 = [1.0, 1.0]	
  		minimizer_kwargs = {'method':'Nelder-Mead' ,'args': (infile, skyfile, badpmap, interp_type,)}

  		#First Method - using simple downhill simplex 
  		res = minimize(self.shiftImageFirstMin, x0, args=(infile, skyfile, badpmap, interp_type,), \
  		 method = 'Nelder-Mead', tol=0.005, options={'disp': True})

  		#Second method - more suited to a global minimization procedure. Takes longer.
		#res = basinhopping(self.shiftImageFirstMin, x0, niter=5, stepsize = 0.005, minimizer_kwargs = minimizer_kwargs, disp=True)
  			


  		#Return the shift array which minimises the inverse correlation coefficient
  		print res.x
  		return res.x


  	def shiftImageSegmentsMin(self, ext, infile, skyfile, badpmap,\
  	 vertSegments, horSegments, interp_type):

  		"""
  		Def: 
  		Lots of arguments because of using lots of different functions. 
  		This is taking an object and a sky image, splitting them into a specified 
  		number of segments, performing shifts to each of the segments and then 
  		computing the cross-correlation function to see if we can improve the 
  		alignment at all. Should give better results than a global shift 

  		Inputs: 
  		infile - file to be divided
		skyFile - sky image to compare objFile with
  		vertSegments - number of vertical segments (2048 must be divisible by) 
  		horSegments - number of horizontal segments (2048 must be divisible by)
  		ext - extension number
		interp_type - type of interpolation function for the shift. 

			-'nearest': nearest neighbour
			-'linear':bilinear x,y, interpolation
			-'poly3':third order interior polynomial
			-'poly5':fifth order interior polynomial
			-'spline3':third order spline3

		stepsize - value to increment grid by each time, increasing 
		this increases the time taken for the computation 
		xmin, xmax, ymin, ymax - Grid extremes for brute force shift 

		"""
		########################TO ADD###############################
		#Make sure that 2048 is divisible by the segment numbers 



		#Create arrays of the split files using the imSplit function 
		objArray = self.imSplit(ext, infile, vertSegments, horSegments)
		skyArray = self.imSplit(ext, skyfile, vertSegments, horSegments)
		badpArray = self.imSplit(ext, badpmap, vertSegments, horSegments)

		#Find the headers of the primary HDU and chosen extension 
		objTable = fits.open(infile)
		objPrimHeader = objTable[0].header
		objExtHeader = objTable[ext].header
		skyTable = fits.open(skyfile)
		skyPrimHeader = skyTable[0].header
		skyExtHeader = skyTable[ext].header
		badpTable = fits.open(badpmap)
		badpPrimHeader = badpTable[0].header
		badpExtHeader = badpTable[ext].header

		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')
		print (objPrimHeader)
		print (objExtHeader)
		print (skyPrimHeader)
		print (skyExtHeader)
		print (badpPrimHeader)
		print (badpExtHeader)
		sys.stdout.close()
		sys.stdout = temp


		shiftArray = []
		#Should now have two 1D arrays of 2D arrays of equal size
		for i in range(len(objArray)):

			tempObjName = infile[:-5] + str(vertSegments) + str(horSegments) + str(i) + '_temp.fits'
			#Write out to new temporary fits files - annoyingly need to have 
			#the data in fits files to be able to use pyraf functions
			#######OBJECT##########
			objhdu = fits.PrimaryHDU(header=objPrimHeader)
			objhdu.writeto(tempObjName, clobber=True)
			fits.append(tempObjName, data=objArray[i], header=objExtHeader)

			#######SKY#############
			skyhdu = fits.PrimaryHDU(header=skyPrimHeader)
			skyhdu.writeto('tempSky.fits', clobber=True)
			fits.append('tempSky.fits', data=skyArray[i], header=skyExtHeader)

			#######BADPIXEL#############
			badphdu = fits.PrimaryHDU(header=badpPrimHeader)
			badphdu.writeto('tempbadp.fits', clobber=True)
			fits.append('tempbadp.fits', data=badpArray[i], header=badpExtHeader)

			#Now need to apply the shiftImageFirst function, which compares the chosen 
			#extension shifted object and sky files. 
			#Create an array to hold the shift coordinates for each segment. This is defined 
			#Outside the for loop so that I am not initialising it every time.

			xMinCorr = (1 * len(objArray[0]))*0.0625
			xMaxCorr = (15 * len(objArray[0]))*0.0625
			yMinCorr = (1 * len(objArray[0]))*0.0625
			yMaxCorr = (15 * len(objArray[0]))*0.0625


  			print 'Before shifting, the correlation is: %s ' % \
  			(1.0 / self.crossCorrFirst(tempObjName, 'tempSky.fits', yMinCorr, yMaxCorr, xMinCorr, xMaxCorr))

			print 'This is shift: %s' % i
			shiftArray.append(self.minimiseRho(tempObjName, 'tempSky.fits', 'tempbadp.fits', interp_type))



			#Clean up the temporary fits files during each part of the loop 
			os.system('rm %s' % tempObjName)
			os.system('rm tempSky.fits')
			os.system('rm tempbadp.fits')

			#Now just need to vstack all of these arrays and will have a 2048x2048 corrected array
		print shiftArray
		#apply the shifts to the unmasked infile 
		#imshift can be used with a list of infile names, outfile names and shift coordinates
		#create these lists and imshift all at once, read in the data and then recombine
		#First get the x and y vectors for the shift coordinates
		xArray = []
		yArray = []
		for item in shiftArray:
			xArray.append(item[0])
			yArray.append(item[1])
		xArray = np.array(np.around(xArray, 3))
		yArray = np.array(np.around(yArray, 3))	
		#Create the .txt file with the two columns specifying the shift coordinates
		np.savetxt('coords.txt', np.c_[xArray, yArray], fmt=('%5.3f', '%5.3f'))

		#Need to apply the shifts to a masked object file. Open the bad pixel map 
		#and the object file and mask the pixels and save to temporary file 
		#Find the coordinates of the bad pixels and the slitlets 
		objData = objTable[ext].data
		badpData = badpTable[ext].data
		badpData[badpData == 0] = np.nan
		objData = objData * badpData

		#Write out to new file which will then be read in to split up the data
		objhdu = fits.PrimaryHDU(header=objPrimHeader)
		objhdu.writeto('temp_masked.fits', clobber=True)
		fits.append('temp_masked.fits', data=objData, header=objExtHeader)


		objArray = self.imSplit(1, 'temp_masked.fits', vertSegments, horSegments)
		inFileArray = []
		outFileArray = []
		shiftedDataArray = []
		vstackArray = []
		hstackArray = []


		#Should now have two 1D arrays of 2D arrays of equal size
		for i in range(len(objArray)):

			inFileName = 'tempObjin'+str(i)+'.fits'
			outFileName = 'tempObjout'+str(i)+'.fits'
			inFileArray.append(inFileName)
			outFileArray.append(outFileName)

			#Write out to new temporary fits files - annoyingly need to have 
			#the data in fits files to be able to use pyraf functions
			#######OBJECT##########
			objhdu = fits.PrimaryHDU(header=objPrimHeader)
			objhdu.writeto(inFileName, clobber=True)
			fits.append(inFileName, data=objArray[i], header=objExtHeader)

			inFileName = inFileName + '[1]'


  			#Now apply imshift with all the parameters 
  			pyraf.iraf.imshift(input = inFileName, output=outFileName, \
  				xshift=xArray[i], yshift=yArray[i] ,interp_type=interp_type)


  			#We want a 1D array of 2D arrays again, read the data files back in 
  			data = fits.open(outFileName)
  			data = data[0].data
  			shiftedDataArray.append(data)
  			#Go back to the top of the loop and grab the next file

  		#The final problem is that we have a 1D arrays of 2D arrays that needs 
  		#to be recombined into the original 2048 x 2048 which created it 
  		#Ordering depends on the number of vertical and horizontal segments 
 
  		x = len(shiftedDataArray) / horSegments	
  		a = 0 

  		while x <= len(shiftedDataArray):
  			for i in range(a, x): 
  				print i
  				hstackArray.append(shiftedDataArray[i])

  			vstackArray.append(np.hstack(hstackArray))
  			hstackArray = []
  			x += len(shiftedDataArray) / horSegments
  			a += len(shiftedDataArray) / horSegments
  			

  		#for item in vstackArray:
  			#print item.shape	
  		#Reconstruct by vstacking the final array	
  		reconstructedData = np.vstack(vstackArray)

  		#Clean up by getting rid of uneeded files
		for item in inFileArray:
			os.system('rm %s' % item)
		for item in outFileArray:
			os.system('rm %s' % item)
		os.system('rm coords.txt')	
		os.system('rm temp_masked.fits')
		os.system('rm log.txt')

		return reconstructedData, shiftArray



	def shiftAllExtensionsMin(self, infile, skyfile, badpmap,\
  	 vertSegments, horSegments, interp_type):

		"""
		Def: Uses the shiftImageSegments method for each extensions and then combines
		all of these together into a single shifted fits file 

		"""

		#Prepare the headers for writing out the fits file
		objTable = fits.open(infile)
		objPrimHeader = objTable[0].header
		objExtHeader1 = objTable[1].header
		objExtHeader2 = objTable[2].header
		objExtHeader3 = objTable[3].header

		temp = sys.stdout
		sys.stdout = open('log.txt', 'w')
		print (objPrimHeader)
		print (objExtHeader1)
		print (objExtHeader2)
		print (objExtHeader3)						
		sys.stdout.close()
		sys.stdout = temp

		#Set up the array 
		reconstructedDataArray = []
		ShiftArrayList = []

		#Use the shifted image segment function 
		for i in range(1, 4):
			print 'Shifting Extension: %s' % i
			reconstructedData, shiftArray = self.shiftImageSegmentsMin(i, infile, skyfile, badpmap,\
  	 vertSegments, horSegments, interp_type)
			reconstructedDataArray.append(reconstructedData)
			ShiftArrayList.append(shiftArray)
		

		#Name the shifted data file 
  		shiftedName = infile[:-5] + '_' + str(vertSegments) + str(horSegments) + '_' + interp_type + '_Shifted.fits' 
  		print 'Saving %s' % shiftedName

  		objhdu = fits.PrimaryHDU(header=objPrimHeader)
		objhdu.writeto(shiftedName, clobber=True)
		fits.append(shiftedName, data=reconstructedDataArray[0], header=objExtHeader1)
		fits.append(shiftedName, data=reconstructedDataArray[1], header=objExtHeader2)
		fits.append(shiftedName, data=reconstructedDataArray[2], header=objExtHeader3)

		return ShiftArrayList


	def applyShiftAllExtensionsMin(self, fileList, badpmap,\
  	 vertSegments, horSegments, interp_type):
		#Read in the data from the fileList
		data = np.genfromtxt(fileList, dtype='str')
		#Save the names and types as lists 
		names = data[0:,0]
		types = data[0:,1]
		shiftList = []
		#Loop round all names and apply the computeOffsetSegments method
		for i in range(1, len(names)):
			if types[i] == 'O':
				objFile = names[i]
				if i == 1:
					skyFile = names[i + 1]
				elif types[i - 1] == 'S':
					skyFile = names[i - 1]
				else:
					skyFile = names[i + 1]

				print 'Shifting file: %s : %s' % (i, objFile)	
				#Now use the method defined within this class 
				shiftList.append(self.shiftAllExtensionsMin(objFile, skyFile, badpmap,\
  	 					vertSegments, horSegments, interp_type))
				#Which will loop through all and save the corrected object file 
				#as objectFile_Corrected.fits. These are then fed through the pipeline.
		saveName = fileList + '_Coords.txt' 
		print shiftList
		g = []
		for entry in shiftList:
			g.append(np.hstack(entry))
		h = np.vstack(g)	
		np.savetxt(saveName, h, fmt='%10.5f')


##########################################################################################################################
##########################################################################################################################
#                          ALTERNATE SET OF FUNCTIONS USING MINIMISATION INSTEAD OF GRIDSEARCH                           #
##########################################################################################################################
##########################################################################################################################


	#Apply the 
	def applySubtraction(self, fileList):
		"""
		Def: 
		Apply the subframes method to a list of files

		Input: fileList - list of files, where the top line is 
		the name and type, the two columns and the name of the file 
		and either O for object and S for sky. 

		Output: List of subtracted files saved in the object directory 
		"""
		#Read in the data from the fileList
		data = np.genfromtxt(fileList, dtype='str')
		#Save the names and types as lists 
		names = data[0:,0]
		print names
		types = data[0:,1]
		#Loop round all names and apply the computeOffsetSegments method
		for i in range(1, len(names)):
			if types[i] == 'O':
				objFile = names[i]
				if i == 1:
					skyFile = names[i + 1]
				elif types[i - 1] == 'S':
					skyFile = names[i - 1]
				else:
					skyFile = names[i + 1]

				print 'Subbing file: %s : ' %  objFile	
				#Now use the method defined within this class 
				self.subFrames(objFile, skyFile)
				#Which will loop through all and save the corrected object file 
				#as objectFile_Corrected.fits. These are then fed through the pipeline.


	def compareSky(self, skyCube, combNames):
		"""
		Def: 
		From an input sky cube and set of sci_combined file names, 
		work out the median difference between the bright sky lines  
		and the corresponding object pixels 

		Input: skyCube - Any reconstructed skyCube only 
			   combNames - List of the sci_combined names 

		Ouptut: medianVal - median difference between the sky and object values 

		"""

		#Create instance from the skycube 
		sky_cube = cubeOps(skyCube)

		#Extract the sky flux
		flux = sky_cube.centralSpec()
		#Check for where the flux exceeds a certain number of counts 
		indices = np.where(flux > 500)
		#Find the sky values at these pixels
		values = flux[indices] 

		#The combNames should be generated from within the Next routine
		namesOfFiles = np.genfromtxt(combNames, dtype='str')
		#Initialise an empty dictionary 
		medVals = {}
		#Loop round each of the cubes in combNames, create a cube 
		#and store the median 

		for fileName in namesOfFiles:
			tempCube = cubeOps(fileName)
			tempFlux = tempCube.specPlot(3)
			tempValues = tempFlux[indices]
			#Either use the values themselves or the difference
			medVals[tempCube.IFUNR] = np.median(tempValues)
		medVector = np.mean(medVals.values())
		print medVector
		return np.array(medVals.values())

	def gaussFit(self, combNames):

		"""
		Def: 
		Uses the psfMask function from cubeClass to 
		loop round the list of combNames and return a 
		list of FWHM values and a list of mask profiles 

		Input: combNames - list of sci_combined files produced 
		by the pipeline 
		"""

		namesOfFiles = np.genfromtxt(combNames, dtype='str')

		fwhmArray = []
		psfArray = []
		try:

			#Loop round and create an instance of the class each time
			for fileName in namesOfFiles:
				print 'Gaussian fitting science frame: %s' % fileName
				tempCube = cubeOps(fileName)
				params, psfProfile, fwhm, offList = tempCube.psfMask()
				fwhmArray.append(fwhm)
				psfArray.append(psfProfile)
			return fwhmArray, psfArray, offList

		except TypeError:
			#Only one file in the list of files 
			print namesOfFiles
			cubeName = str(namesOfFiles)
			print 'Gaussian fitting science frame: %s' % namesOfFiles
			tempCube = cubeOps(cubeName)
			params, psfProfile, fwhm, offList = tempCube.psfMask()
			return fwhm, psfProfile, offList


		#Return the arrays	
		

	def combFrames(self, frame_array):
		"""
		Def:

		Helper Function to combine a list of object files 
		categorised by the frameCheck method. First appends 
		sci_reconstructed_ to each of the names and then 
		combines these together using the KMOS pipeline 
		recipe kmo_combine 
		Input: frame_array - list of object names as specified 
		by the frameCheck method

		"""
		#Remove .sof file if this exists
		if os.path.isfile('sci_combine.sof'):
			os.system('rm sci_combine.sof')
		if len(frame_array) == 0:
			print 'Empty Array'
		else:

			#Create new list of arrays with prepended names 
			new_names = []
			with open('sci_combine.sof', 'a') as f:
				for entry in frame_array:

					#If the entry doesn't contain a backslash, the entry 
					#is the object name and can prepend directly 
					if entry.find("/") == -1:
						name = 'sci_reconstructed_' + entry
						f.write('%s\n' % name)
					#Otherwise the directory structure is included and have to 
					#search for the backslash and omit up to the last one 
					else:
						objName = entry[len(entry) - entry[::-1].find("/"):]
						name = 'sci_reconstructed_' + objName
						f.write('%s\n' % name)
			#Now execute the recipe 
			os.system('esorex kmo_combine --edge_nan=TRUE sci_combine.sof')
			
	def frameCheck(self, skyCube, frameNames):
		"""
		Def:
		Loop round a list of frameNames of a given type and apply the science 
		reduction to each pair. Return the vectors which contain the sky tweak 
		performance for each pair and for each IFU in each pair, and the vector 
		containing the fwhm of the tracked star. The tracked star is defined in 
		the multiExtract method. Also bin objects based on their fwhm and return 
		a dictionary containing the different bins. 
		Input: skyCube - a given reconstructed skyCube. Note this assumed that the 
		sky values will not vary significantly in wavelength from frame to frame. 
		Need to check this. 
			   frameNames - file containing a list of object and sky frames 
				in the standard format, with column 1 the file name and column 2 the 
				file type 
		Output: IFUValVec - mean for each IFU of skytweak performance 
				frameValVec - mean for each frame of skytweak performance 
				fwhmValVec - tracked star fwhm for each frame 
				fwhmDict - keys of 'Best', 'Good', 'Okay', 'Bad' corresponding
				to the fwhm of the tracked star, and values being the names of 
				the files which fall into each of these bins 
		Uses: self.compareSky
			  self.gaussFit 
			  cubeOps  

		"""
		#First read in the names and the types from frameNames
		data = np.genfromtxt(frameNames, dtype='str')
		#Save the names and types as lists 
		names = data[0:,0]
		types = data[0:,1]
		#Set the sci_comb names defined in the cubeclass  
		if types[1] == 'O':
			combNames = cubeOps(names[1]).combNames
			rec_combNames = cubeOps(names[1]).rec_combNames
		elif types[2] == 'O':
			combNames = cubeOps(names[2]).combNames
			rec_combNames = cubeOps(names[2]).rec_combNames
		elif types[3] == 'O':
			combNames = cubeOps(names[3]).combNames
			rec_combNames = cubeOps(names[3]).rec_combNames
		else:
			print 'Having difficulty setting sci_comb names'

		#Define the tracked star ID
		#Writing out a temporary file containing the tracked star name
		##########################################################################
		###########################HARDWIRED######################################
		##########################################################################
		#Can probably in the future get the name of the IFU tracking a standard 
		#Star straight from the fits header. Will hardwire it in a-priori now 
		track_name = 'n55_19'
		#Loop round the list of combNames until the track_name appears 
		for entry in combNames:
			if entry.find(track_name) != -1:
				tracked_star = entry
		#Initialise loop counter and variables 
		counter = 0
		frameValVec = []
		IFUValVec = []
		namesVec = []
		fwhmValVec = []
		#Set up the different bins for fwhm of tracked star
		a_fwhm_names = []
		b_fwhm_names = []
		c_fwhm_names = []
		d_fwhm_names = []

		#Loop around each name, assign sky pair and populate 
		#the sky tweak performance and fwhm variables 
		for i in range(1, len(names)):
			if types[i] == 'O':
				counter += 1
				objFile = names[i]
				if i == 1:
					skyFile = names[i + 1]
				elif types[i - 1] == 'S':
					skyFile = names[i - 1]
				else:
					skyFile = names[i + 1]

				print 'reducing file: %s : ' %  objFile
				namesVec.append(objFile)
				#Create the new .sof file at each stage for just the object sky pair
				#the .sof file must be in the directory, and must have all the other 
				#files required already specified.

				#Create a copy of the sci_reduc.sof in a new temporary file
				with open('sci_reduc.sof') as f:
				    with open('sci_reduc_temp.sof', 'w') as f1:
				        for line in f:
				                f1.write(line)

				#Append the current object and skyfile names to the newly created .sof file
				with open('sci_reduc_temp.sof', 'a') as f:
					f.write('\n%s\tSCIENCE' % objFile)
					f.write('\n%s\tSCIENCE' % skyFile)
				#Now just execute the esorex recipe for this new file 
				os.system('esorex kmo_sci_red --sky_tweak=TRUE --pix_scale=0.2 --edge_nan=TRUE sci_reduc_temp.sof')

				#We have all the science products now execute the above method for each 
				#of the pairs. Should think of a better way to create the combNames file
				print 'Checking IFU sky tweak performance'
				#This is the array of IFU values for each frame 
				medVals = self.compareSky(skyCube, combNames)
				print combNames
				#Append the full vector for a more detailed plot 
				IFUValVec.append(medVals)
				#Append the mean value for the average plot
				frameValVec.append(np.mean(medVals))

				#Move onto FWHM analysis of chosen tracked star 	
				print 'Checking PSF of tracked star'
				fwhm, psfProfile, offList = self.gaussFit('tracked.txt')
				fwhmValVec.append(fwhm)
				#remove the temporary .sof file and go back to the start of the loop
				os.system('rm sci_reduc_temp.sof') 	
				

				#########################################################
				#Here place the objects into different bins based on the#
				#computed FWHM of each frame                            #
				#########################################################
				#Change FWHM to arcsecond scale, by using the pixel scale
				#recover the pixel scale by creating a cubeClass instance
				pixel_scale = float(cubeOps(tracked_star).pix_scale)
				arc_fwhm =  fwhm * pixel_scale


				#Conditional binning - HARDWIRED VALUES 
				#Could look at percentiles of FWHM distribution? 
				if (arc_fwhm < 0.6):
					print 'Placing object in best bin' #wagwanplaya
					a_fwhm_names.append(objFile)
				elif (arc_fwhm > 0.6 and arc_fwhm < 1.0):
					print 'Placing object in good bin'
					b_fwhm_names.append(objFile)
				elif (arc_fwhm > 1.0 and arc_fwhm < 1.5):
					print 'Placing object in okay bin'
					c_fwhm_names.append(objFile)
				else:
					print 'Placing object in bad bin'
					d_fwhm_names.append(objFile)		

		#Should now have populated the frameValVec, IFUValVec and incremented counter
		IFUValVec = np.array(IFUValVec)
		fwhmValVec = np.array(fwhmValVec)
		offList = np.array(offList)
		#Convert the FWHM to arcseconds instead of pixels
		fwhmValVec = pixel_scale * np.array(fwhmValVec)
		ID = np.arange(0.0, counter, 1.0)

		#Make the dictionary of fwhm values 
		fwhmDict = {'Best':a_fwhm_names,'Good':b_fwhm_names,'Okay':c_fwhm_names,'Bad':d_fwhm_names}
		#Return all of these values 
		return ID, offList, namesVec, IFUValVec, frameValVec, fwhmValVec, fwhmDict

	def meanIFUPlot(self, offList, namesVec, IFUValVec):
		"""
		Def:
		Takes the output from frameCheck and plots a line graph 
		of skytweak performance against IFUID for each input frame. 
		Each of the IFUs which are not operational are plotted as 
		np.nan
		Input - ID: Vector from 1 - len(number of frames)
				offList: List of the IFUs which are not operational
				namesVec: The names of the input object files 
				IFUValVec: Vector of means for each IFU
		Output - Plot of performance against IFU, recognising the 
				IFUs which are not illuminated
		"""
		#Construct ID array of length 24
		IFUID = np.arange(1.0, 25, 1.0)
		
		#Insert np.nan at the locations where the IFU is off
		#Initialise the counter for the frame naming
		val = 0
		colors_plot = cycle(cm.rainbow(np.linspace(0, 1, len(IFUValVec))))
		colors_scatter = cycle(cm.rainbow(np.linspace(0, 1, len(IFUValVec))))
		#Collape all the information at the IFU level onto a single plot
		fig, ax = plt.subplots(1, 1, figsize=(14.0, 14.0))
		for entry in IFUValVec:
			#Extend the value array to match the IFUID array
			for value in offList:
				entry = np.insert(entry, value, np.nan)

			ax.plot(IFUID, entry, label=namesVec[val], color=next(colors_plot))
			ax.scatter(IFUID, entry, color=next(colors_scatter))
			val += 1
		ax.set_title('Sky Tweak Performance vs. IFU ID')
		ax.set_xlabel('IFU ID')
		ax.set_xlim(1,24)
		ax.set_xticks((np.arange(min(IFUID), max(IFUID)+1, 1.0)))
		ax.grid(b=True, which='both', linestyle='--')
		plt.legend(prop={'size':10})
		fig.savefig('IFU_by_Frame.png')
		plt.show()
		plt.close('all')

	def indIFUPlot(self, offList, ID, IFUValVec):
		"""
		Def: Uses the output of frameCheck to 
		Plot for each individual IFU the performance of skytweak 
		against frame ID. More detail as to how well skytweak is 
		performing. 
		Input - Offlist: List of IFU numbers which aren't illuminated
			  - ID: np.arange between 0 and total number of operational IFUs 
			  - IFUValVec: 2D array of median sky tweak performance values 
		Output - subplot array showing how well the sky has been subtracted 
		in each individual IFU
		"""
		#Make a plot for each IFU in a subplot array
		fig, axArray = plt.subplots(3, 8, figsize=(20.0, 15.0))
		IFUCount = 0
		dataCount = 0
		#Have the data now - populate the subplots 
		for col in range(3):
			for row in range(8):
				#Only plot if the IFU is functioning 
				if IFUCount not in offList:
					frameVec = IFUValVec[:, dataCount]
					axArray[col][row].plot(ID, frameVec)
					axArray[col][row].scatter(ID, frameVec)
					axArray[col][row].set_title('IFU %s' % (IFUCount + 1))
					axArray[col][row].set_xlabel('Frame ID')
					axArray[col][row].set_xticks((np.arange(min(ID), max(ID)+1, 1.0)))
					axArray[col][row].grid(b=True, which='both', linestyle='--')
					axArray[col][row].set_ylim(0, 70)
					dataCount += 1
				#Increment the IFUCount number
				IFUCount += 1
		#Subplots populated, save the overall figure 
		fig.savefig('IFU_subplots.png')
		plt.show()
		plt.close('all')

	def meanFramePlot(self, ID, frameValVec):

		"""
		Def:
		Uses the output from frameCheck to make a simple 
		plot of the mean sky tweak performance against frame 
		Input - ID: np.arange between 0 and count of number of frames 
			  - frameValVec: 1D array of mean sky tweak performance
		"""
		#Make the overall mean plot of performance for the frames  
		#Create a figure and plot the results  
		fig, ax = plt.subplots(1, 1, figsize=(14.0,14.0))
		ax.plot(ID, frameValVec)
		ax.scatter(ID, frameValVec)
		ax.set_title('Sky Tweak Performance vs. Frame ID')
		ax.set_xlabel('Frame ID')
		ax.set_xticks((np.arange(min(ID), max(ID)+1, 1.0)))
		ax.grid(b=True, which='both', linestyle='--')
		fig.savefig('frame_performance.png')
		plt.show()
		plt.close('all')

	def meanFWHMPlot(self, ID, fwhmValVec):

		"""
		Def:
		Uses the output from frameCheck to make a simple 
		plot of the tracked star FWHM against frame ID 
		Input - ID: np.arange between 0 and count of number of frames 
			  - fwhmValVec: 1D array of tracked star fwhm in each frame
		"""
		fig, ax = plt.subplots(1, 1, figsize=(14.0,14.0))
		ax.plot(ID, fwhmValVec)
		ax.scatter(ID, fwhmValVec)
		ax.set_title('Average fwhm vs. Frame ID')
		ax.set_xlabel('Frame ID')
		ax.set_xticks((np.arange(min(ID), max(ID)+1, 1.0)))
		ax.grid(b=True, which='both', linestyle='--')
		fig.savefig('frame_fwhm.png')
		plt.show()
		plt.close('all')

	def extractSpec(self, fwhmDict, combNames, rec_combNames):
		"""
		Def:
		Takes the grouped tracked star fwhm dictionary, combines 
		the objects in each bin using the ESO pipeline and then 
		extracts the optimal spectrum from each IFU in each bin 
		with appended dictionary name, i.e. 'Good_sci_combined*'
		"""
		#Writing out a temporary file containing the tracked star name
		##########################################################################
		###########################HARDWIRED######################################
		##########################################################################
		#Can probably in the future get the name of the IFU tracking a standard 
		#Star straight from the fits header. Will hardwire it in a-priori now 
		track_name = 'n55_19'
		#Loop round the list of combNames until the track_name appears 
		for entry in combNames:
			if entry.find(track_name) != -1:
				tracked_star = entry

		#First check the directory for the rec_combNames and delete if they exist 
		for entry in rec_combNames:
			if os.path.isfile(entry):
				os.system('rm %s' % entry)

			#Set the rec_combName of the standard star in the same way as above 
			if entry.find(track_name) != -1:
				rec_tracked_star = entry

		#retrieve the dictionary combining the science names and IFU numbers 
		combDict = cubeOps(tracked_star).combDict
		#Remove the current sci_combined*.fits prior to this analysis
		os.system('rm sci_combined*.fits')

		#Initialise dictionary for final fwhm values
		fwhm_values = {}
		#Loop around each of the keys in the FWHM dictionary
		for group in fwhmDict.keys():

			print 'Extracting Spectra for the %s group' % group

			if fwhmDict[group]:
				
				#The case with only 1 entry in the group (complex) 
				if len(fwhmDict[group]) == 1:
					print 'Only 1 %s PSF frame: Selecting %s PSF Cubes' % (group, group)

					#Construct the reconstructed file name 
					#If the entry doesn't contain a backslash, the entry 
					#is the object name and can prepend directly 
					if fwhmDict[group][0].find("/") == -1:
						rec_frame = 'sci_reconstructed_' + fwhmDict[group][0]
					#Otherwise the directory structure is included and have to 
					#search for the backslash and omit up to the last character 
					else:
						objName = fwhmDict[group][0][len(fwhmDict[group][0]) - fwhmDict[group][0][::-1].find("/"):]
						rec_frame = 'sci_reconstructed_' + objName
					#Now have the correct name of the reconstructed file 
					#Need to select the correct extension for the tracked_star
					ext = combDict[track_name]	
					#Open the reconstructed frame and assign the data for the correct IFU:
					Table = fits.open(rec_frame)
					primHeader = Table[0].header 	
					dataHeader = Table[ext].header
					cube_data = Table[ext].data

					#Fix the issue with the fits header
					temp = sys.stdout
					sys.stdout = open('log.txt', 'w')
					print (primHeader)
					print (dataHeader)						
					sys.stdout.close()
					sys.stdout = temp
					os.system('rm log.txt')

					#Write out to a new fits file
			  		objhdu = fits.PrimaryHDU(header=primHeader)
					objhdu.writeto(tracked_star, clobber=True)
					fits.append(tracked_star, data=cube_data, header=dataHeader)

					#Now on the same footing as below 
					tracked_cube = cubeOps(tracked_star)
					#Extract the PSFProfile from the stacked, tracked star
					params, psfProfile, FWHM, offList = tracked_cube.psfMask()
					#Add best stacked FWHM to the dictionary just to check afterwards 
					fwhm_values[group] = FWHM * float(tracked_cube.pix_scale)
					#Set the rec_combNames as an iterable for specifying the file name
					iterCombNames_one = cycle(combNames)
					iterCombNames_two = cycle(combNames)
					#Now get all the operational IFUs and do the same
					for name in combDict.keys():
						ext = combDict[name]
						#Open the reconstructed frame:
						Table = fits.open(rec_frame)
						primHeader = Table[0].header 	
						dataHeader = Table[ext].header
						cube_data = Table[ext].data

						temp = sys.stdout
						sys.stdout = open('log.txt', 'w')
						print (primHeader)
						print (dataHeader)						
						sys.stdout.close()
						sys.stdout = temp
						os.system('rm log.txt')


						#Write out to a new fits file
				  		objhdu = fits.PrimaryHDU(header=primHeader)
						objhdu.writeto(next(iterCombNames_one), clobber=True)
						fits.append(next(iterCombNames_two), data=cube_data, header=dataHeader)
					new_name_vec = []
					#Append best to all the rec_combNames 
					for entry in combNames:
						group_name = group + '_' + entry
						new_name_vec.append(group_name)
						os.system('mv %s %s' % (entry, group_name))


				#The Case where there is more than one entry in the group (normal)
				elif len(fwhmDict[group]) > 1:
					print 'Combining Best PSF Cubes'
					self.combFrames(fwhmDict[group])
					#The output from this is the combined_sci file names contained in rec_combNames
					#These are all stacked data cubes in bins of seeing 
					tracked_cube = cubeOps(rec_tracked_star)
					#Extract the PSFProfile from the stacked, tracked star
					params, psfProfile, FWHM, offList = tracked_cube.psfMask()
					#Add best stacked FWHM to the dictionary just to check afterwards 
					fwhm_values[group] = FWHM * float(tracked_cube.pix_scale)

					new_name_vec = []
					#Append best to all the rec_combNames 
					for entry in zip(rec_combNames, combNames):
						current_name = entry[0]
						group_name = group + '_' + entry[1]
						new_name_vec.append(group_name)
						os.system('mv %s %s' % (current_name, group_name))

				#Should now have the working directory filled with objects 
				#That have the same name, regardless of whether they passed through 
				#the single object in a bin route or the multi-route, with these names 
				#stored in the new_name_vec array 

				print 'Fitting Gaussian to each of the stacked %s objects' % group
				#Record the centre of the tracked star 
				tracked_centre = [params[2], params[1]]
				tracked_profile = copy(psfProfile)
				tracked_fwhm = copy(FWHM)
				#First Fit a gaussian to each of the objects to determine the center! 
				for name in new_name_vec:
					cube = cubeOps(name)
					#Find the central value of the object flux by 
					#fitting a gaussian to the image
					params, objProfile, FWHM, offList = cube.psfMask()
					obj_centre = [params[2], params[1]]

					print 'The standard star centre is: %s' % tracked_centre
					print 'The Object centre is: %s' % obj_centre
					#Find the difference between the tracked centre and obj centre
					x_shift = obj_centre[0] - tracked_centre[0]
					y_shift = obj_centre[1] - tracked_centre[1]
					x_shift = int(np.round(x_shift))
					y_shift = int(np.round(y_shift))
					print 'Shifting Profile by: %s %s' % (x_shift, y_shift)
					#Use numpy.roll to shift the psfMask to the location of the object 
					new_mask = np.roll(tracked_profile, y_shift, axis=0)
					#For the x_shift need to loop round the elements of the new_mask 
					final_new_mask = []
					for arr in new_mask:
						final_new_mask.append(np.roll(arr, x_shift))
					final_new_mask = np.array(final_new_mask)

					#Check to see that the gaussian and shifted profile align
					colFig, colAx = plt.subplots(1,1, figsize=(14.0,14.0))
					colCax = colAx.imshow(final_new_mask, interpolation='bicubic')
					colFig.colorbar(colCax)
					#plt.show()
					#Extract each optimal spectrum 
					optimal_spec = cube.optimalSpecFromProfile(final_new_mask, tracked_fwhm, params[2], params[1])
					#Save the optimal spectrum for each object 
					#Need to create a new fits table for this 
					tbhdu = fits.new_table(fits.ColDefs(\
						[fits.Column(name='Wavelength', format='E', array=cube.wave_array),\
					     fits.Column(name='Flux', format='E', array=optimal_spec)]))
					prihdu = fits.PrimaryHDU(header=cube.primHeader)
					thdulist = fits.HDUList([prihdu, tbhdu])
					thdulist.writeto(name[:-5] + '_spectrum.fits', clobber=True)
					#This should save the optimal spectrum for each object

		print fwhm_values

	def multiExtractSpec(self, skyCube, frameNames):

		"""
		Def: 
		Executes the kmo_sci_red recipe with skytweak 
		for each of the individual object sky pairs and then 
		applies compareSky to the science products. Plots a graph 
		for the identification of bad science frames. Also fits a gaussian 
		function to the collapsed data for each IFU, for each object sky pair.  

		Input: 	reconstructed object cube from the current set
				skyCube - any reconstructed sky cube 
				frameNames - list of object/sky pairs with 
				the names in the first column and type in second 

		Outpt: Plot of frame performance against ID
		"""
		#Need the sci_reduc.sof file in the directory 
		if ((not (os.path.isfile('sci_reduc.sof')))):
			raise ValueError("Missing reduction .sof file")

		#remove the temporary sof file if it exists
		if os.path.isfile('sci_reduc_temp.sof'):
			os.system('rm sci_reduc_temp.sof')
		if os.path.isfile('tracked.txt'):
			os.system('rm tracked.txt')
		#Read in the data from the frameNames
		data = np.genfromtxt(frameNames, dtype='str')
		#Save the names and types as lists 
		names = data[0:,0]
		types = data[0:,1]
		#Set the sci_comb names defined in the cubeclass  
		if types[1] == 'O':
			combNames = cubeOps(names[1]).combNames
			rec_combNames = cubeOps(names[1]).rec_combNames
		elif types[2] == 'O':
			combNames = cubeOps(names[2]).combNames
			rec_combNames = cubeOps(names[2]).rec_combNames
		elif types[3] == 'O':
			combNames = cubeOps(names[3]).combNames
			rec_combNames = cubeOps(names[3]).rec_combNames
		else:
			print 'Having difficulty setting sci_comb names'

		#Star straight from the fits header. Will hardwire it in a-priori now 
		track_name = 'n55_19'
		#Loop round the list of combNames until the track_name appears 
		for entry in combNames:
			if entry.find(track_name) != -1:
				tracked_star = entry

		#Remove tracked.txt if it already exists
		if os.path.isfile('tracked.txt'):
			os.system('rm tracked.txt')
		with open('tracked.txt', 'a') as f:
			#Hard-Wired in right now - would need to change for different data set 
			#And different object names. How can the brightest star be detected 
			#automatically? Is there a S/N parameter in the header?
			f.write(tracked_star)

		ID, offList, namesVec, IFUValVec, frameValVec, fwhmValVec, fwhmDict = self.frameCheck(skyCube, frameNames)
		########################################################################################################
		#PLOTTING
		########################################################################################################
		#There are the IFU sky tweak performance plots 
		#The mean sky tweak performance across each IFU
		print 'Plotting frame performance against IFU number'
		self.meanIFUPlot(offList, namesVec, IFUValVec)
		#The performance of skytweak in each IFU
		print 'Plotting Individual IFU performance with frame'
		self.indIFUPlot(offList, ID, IFUValVec)
		#Mean skytweak as a function of frame
		print 'Plotting mean sky subtraction performance'
		self.meanFramePlot(ID, frameValVec)

		#FWHM PLOT - monitoring seeing across the frames
		print 'Plotting evolution of tracked star FWHM' 
		self.meanFWHMPlot(ID, fwhmValVec)

###########################################################################################
###########################################################################################
#PLOTS CREATED - NOW TO ANALYSE THE BINS OF DIFFERENT FWHM
###########################################################################################
###########################################################################################
#Takes into account the name of each fwhm bin and appends to the names of the final 
#combined products, e.g. 'best_sci_combined_n55_19__skytweak.fits'


		#Now combine the different PSF bins 
		#Start with best - check to see if this bin is empty or not 
		print 'These are the best names: %s ' % fwhmDict['Best']
		print 'These are the Good names: %s ' % fwhmDict['Good']
		print 'These are the Okay names: %s ' % fwhmDict['Okay']
		print 'These are the Bad names: %s ' % fwhmDict['Bad']

		#Extract the spectra in each of the fwhm bins and save
		self.extractSpec(fwhmDict, combNames, rec_combNames)

	def saveSpec(self, cubeName):
		"""
		Def:
		Extract a spectrum from a given cube optimally 
		and save the spectrum to a fits file in the same 
		format as the frameCheck method 
		Input - cube: Any reconstructed cube, object or sky 
		"""

		#Create an instance of the cube class 
		cube = cubeOps(cubeName)
		#extract the properties 
		wave_arr = cube.wave_array
		spec = cube.optimalSpec()
		#Save to fits file
		tbhdu = fits.new_table(fits.ColDefs(\
			[fits.Column(name='Wavelength', format='E', array=wave_arr),\
		     fits.Column(name='Flux', format='E', array=spec)]))
		prihdu = fits.PrimaryHDU(header=cube.primHeader)
		thdulist = fits.HDUList([prihdu, tbhdu])
		thdulist.writeto(cubeName[:-5] + '_spectrum.fits', clobber=True)
		 



	def plotSpecs(self, objSpec, skySpec, n):
		import matplotlib.gridspec as gridspec
		from matplotlib.ticker import MaxNLocator

		"""
		Def: 
		Takes the object and sky spectra, bins according to n 
		which must be a factor of the wavelength array and plots both 
		on the same axes 
		Input - objSpec: Input spectrum, must be in the fits format specified 
		in the frameCheck recipe i.e. Table data with one column that has header 
		FLUX and one with header WAVELENGTH
			  - skySpec: Input sky spectrum in the same format 
			  - n: Binning order, must be an integer
		"""

		if type(n) != int:
			raise TypeError('n must be an integer')
		if n <= 0:
			raise ValueError("You have specified a value less than or equal 0 for n")

		#Read in the files 
		objTable = fits.open(objSpec)
		obj_spec = objTable[1].data['FLUX']
		obj_wave = objTable[1].data['WAVELENGTH']

		#skyTable 
		skyTable = fits.open(skySpec)
		sky_spec = skyTable[1].data['FLUX']
		sky_wave = skyTable[1].data['WAVELENGTH']

		if n == 1:
			new_obj_spec = copy(obj_spec)
			new_obj_wave = copy(obj_wave)
			new_sky_spec = copy(sky_spec)
			new_sky_wave = copy(sky_wave)

		elif n > 1:

			#Variables to house the new binned spectra
			new_obj_spec = []
			new_obj_wave = []
			new_sky_spec = []
			new_sky_wave = [] 

			#Counters for the binning 
			lower = 0 
			upper = copy(n)

			#Bin the data 
			for i in range(len(obj_wave) / n):
				#The binned spectra are the sum over the ranges
				new_obj_spec.append(sum(obj_spec[lower:upper]))
				new_sky_spec.append(sum(sky_spec[lower:upper]))
				#The binned wavelengths are the median over the ranges
				new_obj_wave.append(np.median(obj_wave[lower:upper]))
				new_sky_wave.append(np.median(sky_wave[lower:upper]))	
				lower += n 
				upper += n 
			#Print to make sure they are the same length 
			print len(new_obj_spec), len(new_sky_spec)
			new_obj_spec = np.array(new_obj_spec)
			new_obj_wave = np.array(new_obj_wave)
			new_sky_spec = np.array(new_sky_spec)
			new_sky_wave = np.array(new_sky_wave)

		#Plot the results 
		#Now make the plots for both nights, want the same x-axis for all three layers
		f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(18.0, 10.0))
		ax1.plot(new_obj_wave, new_obj_spec, color='b')
		ax1.set_title('Object and Sky Comparison', fontsize=24)
		ax1.set_ylim(0, max(new_obj_spec))
		ax1.tick_params(axis='y', which='major', labelsize=15)
		ax2.plot(new_sky_wave, new_sky_spec, color='g')
		ax2.set_xlabel(r'Wavelength ($\AA$)', fontsize=24)
		ax2.tick_params(axis='both', which='major', labelsize=15)
		ax2.set_ylim(0, max(new_sky_spec))
		nbins = len(ax1.get_xticklabels())
		ax2.yaxis.set_major_locator(MaxNLocator(nbins=nbins, prune='upper'))
		ax2.set_xlim(1.15,1.20)
		f.subplots_adjust(hspace=0.001)
		plt.show()
		f.savefig('spec_compare.png')
	


